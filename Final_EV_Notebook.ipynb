{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P_c5H1-2pQZs"
   },
   "source": [
    "# **Electric Vehicle Charging Station Analysis: Impact of Policy Interventions and Predictive Modeling for Urban Infrastructure Optimization**\n",
    "\n",
    "### Core Technologies\n",
    "[![Python](https://img.shields.io/badge/Python-3.x-blue.svg)](https://www.python.org/)\n",
    "[![Jupyter](https://img.shields.io/badge/Jupyter-Notebook-orange.svg)](https://jupyter.org/)\n",
    "[![Pandas](https://img.shields.io/badge/Pandas-Data_Processing-150458.svg)](https://pandas.pydata.org/)\n",
    "[![NumPy](https://img.shields.io/badge/NumPy-Scientific_Computing-013243.svg)](https://numpy.org/)\n",
    "\n",
    "### Machine Learning & Deep Learning\n",
    "[![Scikit-learn](https://img.shields.io/badge/Scikit--learn-Machine_Learning-F7931E.svg)](https://scikit-learn.org/)\n",
    "[![PyTorch](https://img.shields.io/badge/PyTorch-Deep_Learning-EE4C2C.svg)](https://pytorch.org/)\n",
    "[![XGBoost](https://img.shields.io/badge/XGBoost-Gradient_Boosting-003153.svg)](https://xgboost.readthedocs.io/)\n",
    "[![Prophet](https://img.shields.io/badge/Prophet-Time_Series-00A4EF.svg)](https://facebook.github.io/prophet/)\n",
    "[![Optuna](https://img.shields.io/badge/Optuna-Hyperparameter_Optimization-2F6792.svg)](https://optuna.org/)\n",
    "\n",
    "### Data Visualization\n",
    "[![Matplotlib](https://img.shields.io/badge/Matplotlib-Plotting-11557C.svg)](https://matplotlib.org/)\n",
    "[![Seaborn](https://img.shields.io/badge/Seaborn-Statistical_Viz-4EABD5.svg)](https://seaborn.pydata.org/)\n",
    "[![Folium](https://img.shields.io/badge/Folium-Interactive_Maps-77B829.svg)](https://python-visualization.github.io/folium/)\n",
    "[![KeplerGL](https://img.shields.io/badge/KeplerGL-Geospatial_Viz-00B4FF.svg)](https://kepler.gl/)\n",
    "\n",
    "### Analysis Types\n",
    "[![Time Series](https://img.shields.io/badge/Time_Series-Analysis-yellow.svg)](https://www.statsmodels.org/)\n",
    "[![Geospatial](https://img.shields.io/badge/Geospatial-Analysis-darkgreen.svg)](https://geopandas.org/)\n",
    "[![Statistical](https://img.shields.io/badge/Statistical-Analysis-purple.svg)](https://www.scipy.org/)\n",
    "[![Policy Analysis](https://img.shields.io/badge/Policy-Analysis-brown.svg)](#)\n",
    "\n",
    "### Project Focus\n",
    "[![EV Infrastructure](https://img.shields.io/badge/EV-Infrastructure-success.svg)](#)\n",
    "[![Urban Planning](https://img.shields.io/badge/Urban-Planning-informational.svg)](#)\n",
    "[![Sustainability](https://img.shields.io/badge/Sustainability-Research-brightgreen.svg)](#)\n",
    "[![Smart Cities](https://img.shields.io/badge/Smart-Cities-blueviolet.svg)](#)\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iaXr8k9ja3BG"
   },
   "source": [
    "# **1. Project Introduction**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VXSg9kwCbSN_"
   },
   "source": [
    "## **1.1. Purpose of this Research Project**\n",
    "\n",
    "This analysis investigates electric vehicle (EV) charging station usage patterns in urban environments. The project aims to understand the complex dynamics of EV charging infrastructure utilization while evaluating the effectiveness of policy interventions, specifically focusing on fee policies. Additionally, it develops predictive models for both energy consumption and port availability to support data-driven decision-making in urban EV infrastructure planning.\n",
    "\n",
    "### Key Objectives:\n",
    "1. Analyze temporal and spatial patterns in EV charging station usage\n",
    "2. Evaluate the impact of fee policy implementation on station utilization\n",
    "3. Identify key station characteristics influencing usage patterns\n",
    "4. Develop predictive models for:\n",
    "   - Energy consumption forecasting\n",
    "   - Port availability prediction\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q7Ap-3dwbSmE"
   },
   "source": [
    "## **1.2. Motivation**\n",
    "\n",
    "The rapid adoption of electric vehicles represents a crucial step toward sustainable urban transportation. However, this transition presents significant challenges in infrastructure planning and management. This research is motivated by several key factors:\n",
    "\n",
    "### Environmental Impact\n",
    "- Rising concerns about climate change and urban air quality\n",
    "- Need to support and accelerate the transition to sustainable transportation\n",
    "- Potential for reducing greenhouse gas emissions through optimized EV infrastructure\n",
    "\n",
    "### Infrastructure Optimization\n",
    "- Growing demand for EV charging stations in urban areas\n",
    "- Challenge of balancing station availability with utilization efficiency\n",
    "- Need for data-driven approaches to infrastructure planning\n",
    "\n",
    "### Policy Effectiveness\n",
    "- Understanding the impact of pricing strategies on user behavior\n",
    "- Evaluating the effectiveness of policy interventions\n",
    "- Supporting evidence-based policy-making for sustainable urban mobility\n",
    "\n",
    "### Smart City Integration\n",
    "- Contributing to smart city initiatives through data-driven planning\n",
    "- Enhancing urban mobility through predictive analytics\n",
    "- Supporting the development of resilient and sustainable urban infrastructure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JEaj36iKSG_G"
   },
   "source": [
    "## **1.3. Research Questions**\n",
    "\n",
    "This research project addresses several key questions that guide our analysis and methodology:\n",
    "\n",
    "### Usage Patterns\n",
    "1. What are the temporal patterns in EV charging station usage?\n",
    "   - Daily, weekly, and seasonal variations\n",
    "   - Peak usage times and duration patterns\n",
    "   - Geographic distribution of usage\n",
    "\n",
    "### Policy Impact\n",
    "2. How does the implementation of fee policies affect charging station utilization?\n",
    "   - Changes in usage patterns pre- and post-policy implementation\n",
    "   - Impact on charging duration and energy consumption\n",
    "   - User behavior adaptation to pricing changes\n",
    "\n",
    "### Station Characteristics\n",
    "3. Which station characteristics significantly influence utilization rates?\n",
    "   - Location and LandUse factors\n",
    "   - Station Behavioral factors\n",
    "   - Socioeconomic characteristics\n",
    "\n",
    "### Predictive Modeling\n",
    "4. Can we accurately forecast:\n",
    "   - Energy consumption patterns?\n",
    "   - Port availability at different times?\n",
    "   - Impact of various factors on station utilization?\n",
    "\n",
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T3opDOmibnwg"
   },
   "source": [
    "## **1.4. Methodology**\n",
    "\n",
    "This research employs a comprehensive, multi-stage approach to address the research questions:\n",
    "\n",
    "### Stage 1: Data Preparation and Preprocessing\n",
    "- Data acquisition from EV charging station records\n",
    "- Cleaning and handling missing values\n",
    "- Feature engineering and temporal data processing\n",
    "- Spatial data preprocessing and validation\n",
    "\n",
    "### Stage 2: Exploratory Data Analysis (EDA)\n",
    "- Temporal pattern analysis\n",
    "- Spatial distribution visualization\n",
    "- Statistical analysis of usage patterns\n",
    "- Correlation analysis between variables\n",
    "\n",
    "### Stage 3: Policy Impact Analysis\n",
    "- Pre-post fee policy implementation comparison\n",
    "- Statistical significance testing\n",
    "- User behavior pattern analysis\n",
    "- Impact assessment on various metrics\n",
    "\n",
    "### Stage 4: Predictive Modeling\n",
    "- Development of time-series forecasting models\n",
    "- Implementation of multi-output prediction models\n",
    "- Model validation and performance assessment\n",
    "- Feature importance analysis\n",
    "\n",
    "### Tools and Technologies\n",
    "- **Programming Language:** Python 3.x\n",
    "- **Key Libraries:**\n",
    "  - Data Processing: Pandas, NumPy\n",
    "  - Machine Learning: Scikit-learn, XGBoost\n",
    "  - Deep Learning and Transformers: PyTorch, Informer\n",
    "  - Visualization: Matplotlib, Seaborn, Folium\n",
    "  - Time Series Analysis: Prophet, StatsModels\n",
    "- **Development Environment:** Jupyter Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LH6MOSZAsv1"
   },
   "source": [
    "# **2. Data Acquisition, Cleaning, and Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wwzr63AHbD69"
   },
   "source": [
    "## **2.1. Data Collection and Library Setup**\n",
    "\n",
    "This section details the process of acquiring the EV charging station dataset and setting up the necessary analytical environment. The data is sourced from the City of Palo Alto's open data portal, covering EV charging station usage from July 2011 to December 2020.\n",
    "\n",
    "### Data Source\n",
    "- **Provider:** City of Palo Alto Open Data Portal\n",
    "- **Time Period:** July 2011 - December 2020\n",
    "- **Format:** CSV file\n",
    "- **Access Method:** Direct URL download\n",
    "\n",
    "### Required Libraries\n",
    "Our analysis utilizes a comprehensive set of Python libraries:\n",
    "\n",
    "#### Data Processing\n",
    "- `pandas`: Data manipulation and analysis\n",
    "- `numpy`: Numerical computing\n",
    "- `missingno`: Missing data visualization\n",
    "- `json`: JSON data handling\n",
    "\n",
    "#### Visualization\n",
    "- `matplotlib`: Basic plotting library\n",
    "- `seaborn`: Statistical data visualization\n",
    "- `folium`: Interactive maps\n",
    "- `keplergl`: Geospatial visualization\n",
    "\n",
    "#### Machine Learning & Statistical Analysis\n",
    "- `scikit-learn`: Traditional machine learning algorithms\n",
    "- `xgboost`: Gradient boosting\n",
    "- `catboost`: Gradient boosting\n",
    "- `prophet`: Time series forecasting\n",
    "- `torch`: Deep learning implementations\n",
    "\n",
    "#### Geospatial Analysis\n",
    "- `pgeocode`: Geocoding operations\n",
    "\n",
    "#### Time Series Processing\n",
    "- `datetime`: Date and time handling\n",
    "- `holidays`: Holiday calendar integration\n",
    "\n",
    "The following code cells will import these libraries and load the dataset for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZoageaBUfkGM"
   },
   "outputs": [],
   "source": [
    "# Check the python version of this notebook\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bGe8s-5TfndB"
   },
   "outputs": [],
   "source": [
    "# Install and Import necessary libraries\n",
    "# Data processing tasks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Colab settings\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "# Visualization tasks\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import MarkerCluster\n",
    "from branca.colormap import linear\n",
    "!pip install keplergl\n",
    "from keplergl import KeplerGl\n",
    "\n",
    "# Geocoding tasks\n",
    "!pip install pgeocode\n",
    "import pgeocode\n",
    "\n",
    "# Time-series tasks\n",
    "import datetime as dt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from prophet import Prophet\n",
    "!pip install pandas holidays\n",
    "import holidays\n",
    "import calendar\n",
    "\n",
    "\n",
    "# Machine learning tasks\n",
    "!pip install catboost\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "# Deep learning tasks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "!pip install optuna\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7g-4nZBQppHi"
   },
   "outputs": [],
   "source": [
    "# Reading the Electric Vehicle (EV) charging station dataset from a remote CSV file\n",
    "# The 'low_memory' parameter is set to False to ensure efficient memory usage during loading\n",
    "EV=pd.read_csv(\"https://data.cityofpaloalto.org/datasets/194693-electric-vehicle-charging-station-usage-july-2011-dec-2020.download/\", low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EkwSSX_yA3di"
   },
   "source": [
    "## **2.2. Initial Data Exploration**\n",
    "\n",
    "In this section, we perform an initial exploration of the EV charging station dataset to understand its structure, content, and potential challenges that need to be addressed during preprocessing.\n",
    "\n",
    "### Dataset Overview\n",
    "- Examination of the first few records\n",
    "- Analysis of data types and column descriptions\n",
    "- Assessment of dataset dimensions and completeness\n",
    "\n",
    "### Key Data Elements\n",
    "1. **Station Information**\n",
    "   - Station identification and location details\n",
    "   - Geographical coordinates\n",
    "   - Postal code information\n",
    "\n",
    "2. **Usage Metrics**\n",
    "   - Energy consumption (kWh)\n",
    "   - Charging duration\n",
    "   - Total session duration\n",
    "   - Associated fees\n",
    "\n",
    "3. **Temporal Information**\n",
    "   - Start and end timestamps\n",
    "   - Time zone data\n",
    "   - Session duration metrics\n",
    "\n",
    "4. **User Information**\n",
    "   - Anonymous user identifiers\n",
    "   - Driver location data (postal codes)\n",
    "\n",
    "The following cells will display key information about our dataset structure and content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bo6jyixYlYz7"
   },
   "outputs": [],
   "source": [
    "# Display first rows of the data\n",
    "EV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mJ1LQdgUADzr"
   },
   "outputs": [],
   "source": [
    "# Display column-wise information\n",
    "EV.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rszB4jTbnxqu"
   },
   "source": [
    "With 259,415 rows and 33 columns, this dataset is quite extensive. However, not all of these columns are pertinent to the analysis, and streamlining the dataset is crucial. Specific categories of columns will be retained to simplify the data for analysis:\n",
    "\n",
    "**1.   Station-wise Information:**\n",
    "\n",
    "  These columns provide essential details about the charging station, such as its name and location. These are the relevant columns:\n",
    "\n",
    "      Station Name\n",
    "      Postal Code\n",
    "      Latitude\n",
    "      Longitude\n",
    "\n",
    "**2.   User-wise Information:**\n",
    "\n",
    "  This category encompasses columns related to user data, including user IDs and driver postal codes. These are the specific columns:\n",
    "\n",
    "      Driver Postal Code\n",
    "      User ID\n",
    "\n",
    "**3.   Time-related Information:**\n",
    "\n",
    " Columns associated with time, encompassing start and end dates and their related time zones. These are the relevant data fields:\n",
    "  \n",
    "      Start Date\n",
    "      Start Time Zone\n",
    "      End Date\n",
    "      End Time Zone\n",
    "\n",
    "**4.   Event-related Information:**\n",
    "\n",
    "This category includes columns that directly pertain to the charging event, such as duration of event, charging time, energy consumption and transaction details.These are the specific columns:\n",
    "\n",
    "      Total Duration\n",
    "      Charging Time\n",
    "      Energy (kWh)\n",
    "      Fee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-TbYigSfzWbQ"
   },
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "EV.drop(EV[[\"MAC Address\", \"Org Name\", \"Transaction Date (Pacific Time)\", \"GHG Savings (kg)\", \"Gasoline Savings (gallons)\", \"Address 1\", \"City\", \"EVSE ID\", \"Port Type\", \"Port Number\", \"Plug Type\", \"Plug In Event Id\", \"State/Province\", \"Country\", \"Currency\", \"County\", \"System S/N\", \"Ended By\", \"Model Number\"]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kvb0jlCcvdm_"
   },
   "outputs": [],
   "source": [
    "duplicates = EV.duplicated()\n",
    "\n",
    "num_duplicates = duplicates.sum()\n",
    "print(\"Number of duplicate rows:\", num_duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oby1aGHSmM0B"
   },
   "source": [
    "Here are a few notable observations from column-wise information:\n",
    "*  Some columns, such as \"Driver Postal Code\" and \"User ID\" have missing values. These missing values may need to be addressed during data preprocessing.\n",
    "*  Time-related columns, such as \"Start and End Date and Time Zone\", \"Total Duration\" and \"Charging Time\" may require to be treated as datetime columns considering timezone information.\n",
    "*  Columns containing textual information, such as \"Station Name\" may require further encoding.\n",
    "*   Numeric columns like \"Energy (kWh)\", \"GHG Savings (kg)\", \"Gasoline Savings (gallons)\" and \"Fee\" appear to be correctly typed as float64 for numerical calculations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL0Jag82NkDj"
   },
   "source": [
    "## **2.3. Data Cleaning and Preprocessing**\n",
    "\n",
    "This section focuses on preparing the raw EV charging station data for analysis through a series of systematic cleaning and preprocessing steps. Our goal is to ensure data quality and consistency while preserving the integrity of the information.\n",
    "\n",
    "### Preprocessing Pipeline\n",
    "\n",
    "#### 1. Duplicate Handling\n",
    "- Identification of duplicate records\n",
    "- Analysis of duplication patterns\n",
    "- Removal of redundant entries while preserving data integrity\n",
    "\n",
    "#### 2. Temporal Data Processing\n",
    "- Conversion of timestamp columns to appropriate datetime format\n",
    "- Standardization of time zones\n",
    "- Creation of derived temporal features\n",
    "- Handling of duration calculations\n",
    "\n",
    "#### 3. Missing Value Treatment\n",
    "- Assessment of missing data patterns\n",
    "- Implementation of appropriate imputation strategies\n",
    "- Validation of imputed values\n",
    "\n",
    "#### 4. Feature Engineering\n",
    "- Creation of meaningful derived variables\n",
    "- Temporal feature extraction\n",
    "- Geographical feature enhancement\n",
    "- Usage pattern metrics calculation\n",
    "\n",
    "#### 5. Data Validation\n",
    "- Range checking for numerical variables\n",
    "- Consistency verification for categorical variables\n",
    "- Temporal sequence validation\n",
    "- Geographical coordinate verification\n",
    "\n",
    "The following cells implement these preprocessing steps systematically to prepare our dataset for analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2snHjDXM-1pN"
   },
   "source": [
    "In this section, we focus on preparing and cleaning the raw electric vehicle (EV) charging data to make it suitable for analysis. The primary data preprocessing tasks include converting date and time columns to appropriate data types, handling missing values, addressing time zone discrepancies, and consolidating station identifiers. Let's dive into each of these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnjVL4Gf6eKp"
   },
   "source": [
    "### **2.3.1. Droping Duplicates**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sauqJblTiciQ"
   },
   "outputs": [],
   "source": [
    "# Check for duplicate rows in the dataset\n",
    "duplicates = EV.duplicated()\n",
    "\n",
    "# Calculate the number of duplicate rows\n",
    "num_duplicates = duplicates.sum()\n",
    "print(\"Number of duplicate rows:\", num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e-xztxQVyUvL"
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows from the dataset\n",
    "EV = EV.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "78MENGL_yAoQ"
   },
   "source": [
    "### **2.3.2. Converting Start Date and End Date to Datetime**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vIrUdDcOyAoa"
   },
   "outputs": [],
   "source": [
    "# Convert 'Start Date' and 'End Date' columns to Datetime\n",
    "EV[[\"Start Date\", \"End Date\"]]= EV[[\"Start Date\", \"End Date\"]].apply(pd.to_datetime, format='%m/%d/%Y %H:%M', errors='coerce') # Invalid formats would become NaT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I5U1zxfh6tf6"
   },
   "source": [
    "### **2.3.3. Converting Total Duration and Charging Time to Timedelta and Extracting total minutes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a4DJEmjNdJt5"
   },
   "outputs": [],
   "source": [
    "# Convert 'Total Duration (hh:mm:ss)' and 'Charging Time (hh:mm:ss)' columns to Timedelta\n",
    "EV[[\"Total Duration (hh:mm:ss)\", \"Charging Time (hh:mm:ss)\"]]= EV[[\"Total Duration (hh:mm:ss)\", \"Charging Time (hh:mm:ss)\"]].apply(pd.to_timedelta, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NUuyQ6Fmnf9h"
   },
   "outputs": [],
   "source": [
    "# Creating Charging_min Column\n",
    "EV[\"Charging_min\"] = EV[\"Charging Time (hh:mm:ss)\"].dt.total_seconds() / 60\n",
    "\n",
    "# Creating Duration_min Column\n",
    "EV[\"Duration_min\"] = EV[\"Total Duration (hh:mm:ss)\"].dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XTzGDAVC7OTb"
   },
   "source": [
    "### **2.3.4. Handling missing values**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4csCRRLZfOKX"
   },
   "outputs": [],
   "source": [
    "#Visualize Missing Values\n",
    "msno.matrix(EV)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uz5q3IhA8Tk-"
   },
   "outputs": [],
   "source": [
    "# Calculate the number of missing values in columns\n",
    "EV.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WZsWMl8EiftQ"
   },
   "outputs": [],
   "source": [
    "# Calculate missing \"End Date\" values based on \"Start Date\" and \"Total Duration (hh:mm:ss)\" when \"End Date\" is NaT\n",
    "EV['End Date'] = EV.apply(lambda row: row['Start Date'] + row['Total Duration (hh:mm:ss)'] if pd.isnull(row['End Date']) else row['End Date'], axis=1)\n",
    "\n",
    "# Fill missing values in the \"Driver Postal Code\" and \"User ID\" columns with \"Unknown\" as a placeholder\n",
    "EV = EV.fillna({\"Driver Postal Code\": \"Unknown\", \"User ID\": \"Unknown\"})\n",
    "\n",
    "# Verify the effectiveness of missing value handling by checking the remaining missing values in the dataset\n",
    "EV.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iWDA9stAyIeU"
   },
   "outputs": [],
   "source": [
    "# Remove the \"Start Time Zone\" and \"End Time Zone\" columns as they are no longer needed in the dataset\n",
    "EV.drop(EV[[\"Total Duration (hh:mm:ss)\", \"Charging Time (hh:mm:ss)\"]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Er-cBu4TEK7_"
   },
   "source": [
    "### **2.3.5. Dealing with UTC Time Zone**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z1KwFXz4w5S8"
   },
   "outputs": [],
   "source": [
    "# Calculate and display the count of unique combinations of \"Start Time Zone\" and \"End Time Zone\"\n",
    "EV[[\"Start Time Zone\", \"End Time Zone\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gg5canNCMexN"
   },
   "outputs": [],
   "source": [
    "# Define a time delta representing the difference between UTC and Pacific Standard Time (PST)\n",
    "PST= pd.to_timedelta(\"0 days 08:00:00\")\n",
    "# Define a time delta representing the difference between UTC and Pacific Daylight Time (PDT)\n",
    "PDT= pd.to_timedelta(\"0 days 07:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SqQc4wfxQkoM"
   },
   "outputs": [],
   "source": [
    "# Adjust the \"Start Date\" for events where the \"Start Time Zone\" is UTC and the \"End Time Zone\" is PST\n",
    "EV.loc[(EV[\"Start Time Zone\"]== \"UTC\") & (EV[\"End Time Zone\"]== \"PST\"), \"Start Date\"] = EV.loc[(EV[\"Start Time Zone\"]== \"UTC\") & (EV[\"End Time Zone\"]== \"PST\"), \"Start Date\"] - PST\n",
    "\n",
    "# Adjust the \"Start Date\" for events where the \"Start Time Zone\" is UTC and the \"End Time Zone\" is PDT\n",
    "EV.loc[(EV[\"Start Time Zone\"]== \"UTC\") & (EV[\"End Time Zone\"]== \"PDT\"), \"Start Date\"] = EV.loc[(EV[\"Start Time Zone\"]== \"UTC\") & (EV[\"End Time Zone\"]== \"PDT\"), \"Start Date\"] - PDT\n",
    "\n",
    "# Adjust the \"End Date\" for events where the \"End Time Zone\" is UTC by subtracting the PDT time delta\n",
    "EV.loc[EV[\"End Time Zone\"]== \"UTC\", \"End Date\"] = EV[EV[\"End Time Zone\"]== \"UTC\"][\"End Date\"] - PDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-fckwQrrMVR4"
   },
   "outputs": [],
   "source": [
    "# Remove the \"Start Time Zone\" and \"End Time Zone\" columns as they are no longer needed in the dataset\n",
    "EV.drop(EV[[\"Start Time Zone\",\t\"End Time Zone\"]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5zI2DJryMvo-"
   },
   "source": [
    "### **2.2.6. Consolidating Station Identifiers**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8ttLI_oFJFFP"
   },
   "outputs": [],
   "source": [
    "# Group the 'EV' dataset by the \"Station Name\" column and count the occurrences of each station name\n",
    "EV.groupby(\"Station Name\")[\"Station Name\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kNvA1gR9-lai"
   },
   "outputs": [],
   "source": [
    "# Create a copy of the original dataset 'EV' to preserve the plug identification data\n",
    "EV_s = EV.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UHIhAuMC-G--"
   },
   "outputs": [],
   "source": [
    "# Define a list of values to be removed or modified from the \"Station Name\" column\n",
    "dropping_values= ['PALO ALTO CA / ', '#', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "# Loop through each value in 'dropping_values' and apply string operations to clean 'Station Name'\n",
    "for value in dropping_values:\n",
    "  EV[\"Station Name\"]= EV[\"Station Name\"].str.replace(value, '').str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3rjkV8-4J5l"
   },
   "outputs": [],
   "source": [
    "# Group the 'EV' dataset by the cleaned \"Station Name\" column and calculate the mean latitude and longitude values\n",
    "EV[[\"Latitude\", \"Longitude\"]]= EV.groupby(\"Station Name\")[[\"Latitude\", \"Longitude\"]].transform('mean')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3PuV7dNUToOb"
   },
   "source": [
    "### **2.2.7. User Segmentation based on events count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lDQpbRU03QnW"
   },
   "outputs": [],
   "source": [
    "# Count charging events by User ID\n",
    "users= EV[\"User ID\"].value_counts()\n",
    "users"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bFOiO2QKVV1b"
   },
   "source": [
    "To achieve this segmentation, users will be categorized into five distinct segments based on their charging event counts:\n",
    "\n",
    "* Passing Users:\n",
    "\n",
    "  These are users who\n",
    "have charged their vehicles only once. We refer to them as passing users since they are likely individuals passing through Palo Alto and utilizing the charging infrastructure for a one-time event.\n",
    "\n",
    "* Infrequent Users:\n",
    "\n",
    "  This group includes users who have charged their vehicles fewer than ten times, possibly corresponding to an annual usage pattern.\n",
    "\n",
    "* Occasional Users:\n",
    "\n",
    "  Users in this category have charged their vehicles fewer than 100 times, indicating a monthly or less frequent charging pattern.\n",
    "\n",
    "* Frequent Users:\n",
    "\n",
    "  These are users who have charged their vehicles more than 100 times, suggesting a usage frequency of more than once a month.\n",
    "\n",
    "* Unknown Users:\n",
    "\n",
    "  This category encompasses users with unknown User IDs, totaling 7,677 events. These events lack associated user information.\n",
    "\n",
    "Let's proceed with the code to perform this segmentation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C43c6qTKVUtQ"
   },
   "outputs": [],
   "source": [
    "# Define bins and labels for user segmentation\n",
    "bins = [0, 2, 10, 100, 2000, float('inf')]\n",
    "labels = ['Passing', 'Infrequent', 'Occasional', 'Frequent', 'Unknown']\n",
    "\n",
    "# Perform user segmentation based on event counts\n",
    "user_segments = pd.cut(users, bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count the number of users in each segment\n",
    "segment_counts = user_segments.value_counts()\n",
    "\n",
    "# Display the user segmentation results\n",
    "print(\"User Segmentation:\")\n",
    "print(segment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uxQ_ont_X81P"
   },
   "outputs": [],
   "source": [
    "# Create a mapping dictionary from the user_segments Series\n",
    "user_segments_mapping = user_segments.to_dict()\n",
    "\n",
    "# Add a new 'User Segments' column to the EV DataFrame by mapping user_segments\n",
    "EV['User Segment'] = EV['User ID'].map(user_segments_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwvkOS4PaR3n"
   },
   "source": [
    "### **2.2.8. A closer look at numeric columns, outliers detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HZx5soJVbscK"
   },
   "outputs": [],
   "source": [
    "EV.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nrrJRqzTeOIe"
   },
   "source": [
    "The data types seem appropriate. However, the columns associated with drivers' Postal codes and User ID originally held numerical values, but due to the inclusion of \"unknown\" values for missing data, they have been converted to object types. This poses no issue as these numerical values essentially represent characters.\n",
    "\n",
    "In this section, the focus for analysis centers on the following numeric columns:\n",
    "\n",
    "     Energy (kWh)\n",
    "     Fee\n",
    "     Charging_min\n",
    "     Duration_min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7d1zckr8FZvo"
   },
   "outputs": [],
   "source": [
    "EV.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pPZ2RIqif4BY"
   },
   "source": [
    "The results obtained from the .describe function offer valuable insights into the dataset.\n",
    "\n",
    "The \"Energy (kWh)\" column ranges from a minimum of 0.01 kWh to a maximum of 97.36 kWh, with an average of 8.54 kWh and a standard deviation of 7.19 kWh.\n",
    "\n",
    "The \"Postal Code\", \"Latitude\" and \"Longitude\" column provide information about the location of stations, with no evident statistical summary.\n",
    "\n",
    "The \"Fee\" column ranges from 0 to 84.56, with a mean value of approximately 1.10 and a standard deviation of 1.93, indicating variability in the fee amounts. It's important to note that the median value for this column is zero, warranting a closer examination as part of the data analysis process.\n",
    "\n",
    "Finally, in the columns \"Duration_min\" and \"Charging_min\", we observe significant variations. In the former, the data ranges from a minimum of 1 minute to an impressive maximum of 4 days and 18 hours, highlighting the potential for extended vehicle stays at charging stations. Conversely, the latter column exhibits a narrower range, spanning from 6 seconds to approximately 23 hours, underscoring the fact that vehicles often remain parked at charging stations for longer durations than their actual charging times. This phenomenon may be attributed to the dual function of many public charging stations as parking areas. On average, the \"Total Duration\" column records an approximate duration of 2 hours and 30 minutes, while the \"Charging Time\" column averages around 2 hours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8fHxvIbLj8-S"
   },
   "outputs": [],
   "source": [
    "# Create a figure with a (2x2) grid of subplots and set the figure size\n",
    "fig, axes = plt.subplots(nrows=2, ncols=2, figsize=(12, 12))\n",
    "\n",
    "# Create a histogram of 'Energy (kWh)' in the top-left subplot\n",
    "sns.histplot(data=EV, x='Energy (kWh)', ax=axes[0, 0])\n",
    "axes[0, 0].set_xlabel('Energy (kWh)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Histogram of Energy (kWh)')\n",
    "\n",
    "# Create a histogram of 'Fee' in the top-right subplot\n",
    "sns.histplot(data=EV, x='Fee', ax=axes[0, 1], bins=100)\n",
    "axes[0, 1].set_xlabel('Fee')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Histogram of Fee')\n",
    "\n",
    "# Create a histogram of 'Charging_min' in the bottom-left subplot\n",
    "sns.histplot(data=EV, x='Charging_min', ax=axes[1, 0])\n",
    "axes[1, 0].set_xlabel('Charging Duration (Minute)')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Histogram of Charging Duration (Minute)')\n",
    "\n",
    "# Create a histogram of 'Duration_min' in the bottom-right subplot\n",
    "sns.histplot(data=EV, x='Duration_min', ax=axes[1, 1])\n",
    "axes[1, 1].set_xlabel('Total Duration (Minute)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].set_title('Histogram of Total Duration (Minute)')\n",
    "\n",
    "# Adjust the layout to prevent subplot overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot to visualize the histograms\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TxS_UWWRk7N0"
   },
   "source": [
    "The histograms of variables exhibited a significant left-skewed distribution. This skewness indicates that the majority of charging events tend to have lower values for these variables, such as lower energy consumption, shorter charging and total durations. This observation suggests that a substantial portion of users opt for shorter charging sessions. The presence of a long tail towards higher values implies that there is also a segment of users with higher energy needs and longer charging durations, which is probably driven by the land-use of these stations as parking lots. Further exploration and segmentation of the data, especially outliers can help us better understand the factors driving this skewness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PxV4cZfjdpJA"
   },
   "source": [
    "The 'Fee' column's distribution highlights that over 50% of its values are zero. Given our analysis in the EDA section primarily focuses on Energy Consumption and Time Duration, rather than financial aspects, it would be beneficial to treat this column as Free and Paid Charging events. This transformation will enable us to better explore user behavior in both free and paid charging scenarios, aligning with analytical objectives."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "awT668TnHeba"
   },
   "outputs": [],
   "source": [
    "# Create a figure with a (3, 1) grid of subplots with a size of 12x12 inches\n",
    "fig, axes = plt.subplots(nrows=3, ncols=1, figsize=(6, 12))\n",
    "\n",
    "# Create a boxplot for the 'Energy (kWh)' column\n",
    "sns.boxplot(data=EV, x='Energy (kWh)', ax=axes[0])\n",
    "axes[0].set_xlabel('Energy (kWh)')\n",
    "axes[0].set_ylabel('Frequency')\n",
    "axes[0].set_title('Boxplot of Energy (kWh)')\n",
    "\n",
    "# Create a boxplot for the 'Charging_min' column\n",
    "sns.boxplot(data=EV, x='Charging_min', ax=axes[1])\n",
    "axes[1].set_xlabel('Charging Duration (Minute)')\n",
    "axes[1].set_ylabel('Frequency')\n",
    "axes[1].set_title('Boxplot of Charging Duration (Minute)')\n",
    "\n",
    "# Create a boxplot for the 'Duration_min' column\n",
    "sns.boxplot(data=EV, x='Duration_min', ax=axes[2])\n",
    "axes[2].set_xlabel('Total Duration (Minute)')\n",
    "axes[2].set_ylabel('Frequency')\n",
    "axes[2].set_title('Boxplot of Total Duration (Minute)')\n",
    "\n",
    "# Adjust the layout for a better appearance\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DtsO1G1ckRii"
   },
   "source": [
    "The presence of longer tail and numerous outliers on the right side of the boxplots is consistent with earlier observation of a left-skewed histogram. Outliers in this context are data points that deviate significantly from the central tendency of the distribution.\n",
    "\n",
    "Detecting and observation of these outliers is crucial as they often represent unusual or extreme observations within the dataset.\n",
    "\n",
    "Understanding the nature and potential causes of these outliers can provide valuable insights into the data's characteristics, uncover anomalies, and inform decision-making processes, such as whether they are legitimate data points or require further investigation and potential data cleansing or transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0FN4PGFNl7pI"
   },
   "source": [
    "Let's inspect the outliers from Total Duration column which is more effected by a few extreme data points with values more than 3000 minutes. These events will be sliced to further decide about them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xKSljj17l7Bi"
   },
   "outputs": [],
   "source": [
    "dur_extreme_outliers= EV[EV[\"Duration_min\"] > 3000]\n",
    "\n",
    "dur_extreme_outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cLgsU8bXnVh4"
   },
   "source": [
    "There are five events in the dataset that exhibit extreme outliers in their Total Duration values, exceeding four days. Among these events, four are associated with a specific station, Cambridge. These four events have charging durations of less than two hours but total durations exceeding four days. Notably, they share similar Start Dates, End Dates, and even end times, suggesting a potential system error. Therefore imputing the Duration values for this data points would not be a good choice since their End Date values look abnormal too.\n",
    "\n",
    "The fifth event is associated with RincondaLib and occurred in 2020, during the Covid-19 crisis and lockdown period, which could explain the longer vehicle stay duration. However, determining the accountability of these events may require further investigation by the respective stations.\n",
    "\n",
    "However, Various techniques can be performed for outliers imputation, but considering dropping only these 5 extreme datapoints from dataset in order to align Duration_min values and outliers with other two columns, Energy (kWh) and Charging_min, with performing .drop method, can be more reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kl-TgXe0E5RX"
   },
   "outputs": [],
   "source": [
    "# Drop extreme outliers from Duration_min column\n",
    "EV.drop(dur_extreme_outliers.index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2SoKlWotLnGw"
   },
   "source": [
    "\n",
    "\n",
    "> Regarding to other outliers in the dataset, again, while there's a lot of options like dropping, transforming, or imputating them, even using machine learning techniques like KNN for adjusting the values and making our work easier, I rather to keep them in our dataset as they are.\n",
    "\n",
    "Even though these unusual observation are influential, they accurately reflect the potential surprises and anomalies inherent in the Electric Vehicles charging stations system. Removing or imputing them, would make analysis and prediction model make the process appear more predictable than it actually is. Itâ€™s bad practice to remove or impute data points simply to produce a better fitting model or statistically significant results.\n",
    "\n",
    "Instead, trying to use more robust techniques to oultliers in the analysis, like using median instead of mean, also performing some data engineering techniques before modeling in order to make the project less sensitive to outliers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wEA6GIUOSlVs"
   },
   "source": [
    "Now let's take a final look to the cleaned dataset before starting Exploratory Data Analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3cFaXpKH3Knm"
   },
   "outputs": [],
   "source": [
    "# Look at first five rows of our cleaned data\n",
    "EV.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eXiN-GhFuDlc"
   },
   "outputs": [],
   "source": [
    "# shape of cleaned data\n",
    "EV.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zbz4Ir9lhmGg"
   },
   "source": [
    "# **3. Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sTupCN3Qfqqf"
   },
   "source": [
    "The Exploratory Data Analysis (EDA) phase is a pivotal stage allowing to unlock a deeper understanding of the dataset, paving the way for meaningful insights. The EDA unfolds across five critical domains of analysis:\n",
    "\n",
    "* Station-wise Analysis\n",
    "* User-Wise Analysis\n",
    "* Origin-Destination Analysis\n",
    "* Long Term Trends and Insights\n",
    "* Behavioral Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dWAi-Qf-Tfo6"
   },
   "source": [
    "## **3.1. Station-wise Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y4JKC8gx3s0S"
   },
   "source": [
    "### **3.1.1. Charging Stations Geographical Distribution and Events Count**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjyn8plYpooU"
   },
   "outputs": [],
   "source": [
    "# Get distinct coordinates of stations\n",
    "final_coordinates= EV[[\"Station Name\", \"Latitude\", \"Longitude\"]].drop_duplicates()\n",
    "\n",
    "# Create a folium map centered on Palo Alto\n",
    "palo_alto= folium.Map(location=[37.4419, -122.143936], zoom_start= 13)\n",
    "\n",
    "# Create a feature group for the markers\n",
    "marker_group= folium.FeatureGroup(name= 'Markers')\n",
    "\n",
    "# Add markers for charging stations\n",
    "for index, row in final_coordinates.iterrows():\n",
    "    # Define a custom icon for charging stations\n",
    "    icon = folium.DivIcon(\n",
    "        html=f'<i class=\"fa-solid fa-charging-station fa-xl\" style=\"color: #1e00ff;\"></i>')  # Using icons from FontAwesome\n",
    "    # Iterate through rows to add icon and popups to the marker_group\n",
    "    folium.Marker([row[\"Latitude\"], row[\"Longitude\"]], icon= icon, popup= row[\"Station Name\"]).add_to(marker_group)\n",
    "\n",
    "# Add the feature group to the map\n",
    "marker_group.add_to(palo_alto)\n",
    "\n",
    "# Display the map\n",
    "palo_alto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N_7CGPEIngyi"
   },
   "source": [
    "The map reveals a distinct spatial arrangement of charging stations, categorized into four prominent areas:\n",
    "\n",
    " High, Hamilton, Bryant, and Webster. Notably, these clusters are strategically positioned in proximity to financial and commercial districts, as well as in close vicinity to Stanford University and Palo Alto station. This spatial alignment strongly suggests that a substantial portion of users frequenting these stations are individuals commuting to and from these bustling centers.\n",
    "\n",
    " Another area is characterized by a cluster comprising Cambridge, Ted Tompson, and Sherman stations. These stations are strategically located near various points of interest, indicating potential user influence from nearby attractions and commercial establishments.\n",
    "\n",
    " Two additional stations stand apart from the rest. Firstly, Rinconda Lib, nestled within the Rinconda Library's parking area, is primarily frequented by library patrons. However, given its location within a predominantly residential land-use zone, it's likely that local residents also utilize this station\n",
    "\n",
    " Lastly, we have MPL, in proximity to Mitchel Park, which hints at a higher proportion of recreational users. The presence of community centers, diverse land-uses, and residential areas in the vicinity further suggests that this station's usage patterns are influenced by a combination of factors, including recreational activities and local residential needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g5J1pMTmyXDe"
   },
   "outputs": [],
   "source": [
    "# Calculate the count of charging events per station\n",
    "station_counts= EV.groupby(\"Station Name\")[\"Station Name\"].count()\n",
    "\n",
    "# Create a bar plot to visualize the distribution\n",
    "plt.figure(figsize=(10, 6))  # Adjust figure size as needed\n",
    "ax = station_counts.plot(kind='bar', color='dodgerblue')\n",
    "\n",
    "# Add data labels above the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(str(p.get_height()), (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Distribution of Stations\")\n",
    "plt.xlabel(\"Station Name\")\n",
    "plt.ylabel(\"Count\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "# Rotate x-axis labels for readability\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "plt.tight_layout()  # Improving spacing\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UiRzUdgStDgB"
   },
   "source": [
    "The plot clearly illustrates that the Bryant station boasts the highest number of events within the dataset, closely followed by High, Hamilton, and Webster stations. Interestingly, these four stations align with the earlier observation that they are strategically situated near bustling commercial centers.\n",
    "\n",
    "In stark contrast, Sherman station records the lowest event count, indicating significantly lower user activity at this location.\n",
    "\n",
    "To gain a more comprehensive understanding of the distribution of charging station events over time among these stations, we will conduct a heatmap analysis. This approach promises to unveil valuable insights into how events are distributed and evolve among these stations throughout the observed timeframe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iy5BKXQevVD0"
   },
   "source": [
    "> To generate this heat-map, we will need to engage in some additional data wrangling. Our process unfolds as follows:\n",
    "1. We initiate the procedure by grouping the events based on the station name and the start date column, which is obtained using the .dt.date method. This grouping allows us to tally the event counts for each station on each specific date, with the results being accumulated into a new data frame.\n",
    "2. Subsequently, we proceed to construct a pivot table from the newly created data frame. This pivot table takes on the structure of a matrix, with station names serving as row indices and start dates as column headers. The values within this matrix correspond to the cumulative event counts derived from the initial groupby operation. Crucially, we employ the aggfunc parameter set to 'sum' and ensure any unassigned values are filled with zeros. This meticulous transformation safeguards against any potential missing data arising from this process.\n",
    "3. With the pivot table meticulously prepared, it is now poised to be employed as the core input for our sns.heatmap function, ultimately culminating in the creation of the heatmap visualization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ucJVBkZnQMRU"
   },
   "outputs": [],
   "source": [
    "# Group by station and day for start events\n",
    "daily_start_ev = EV.groupby(['Station Name', EV['Start Date'].dt.date])['User ID'].count().to_frame()\n",
    "\n",
    "# Pivot the data to create a matrix of station-day usage count\n",
    "pivot_table = daily_start_ev.pivot_table(index='Station Name', columns='Start Date', values='User ID', aggfunc='sum', fill_value=0)\n",
    "\n",
    "# Creating a diverging colormap\n",
    "cmap = sns.diverging_palette(220, 20, as_cmap=True)\n",
    "\n",
    "# Creating a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.heatmap(pivot_table, cmap=cmap, cbar=True, center=0)\n",
    "\n",
    "plt.title('Charging Events Heatmap for Stations Over Time')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Charging Stations')\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wadBY_QIeHEi"
   },
   "source": [
    "The heatmap presented here offers a revealing snapshot of our dataset's dynamics during its last stages. It notably underscores the significance of the four most frequently used stations, which, intriguingly, also happen to be the oldest stations in our dataset, with their operations commencing as far back as 2011. Among these stations, Hamilton stands as the oldest, closely followed by High and Bryant. While Webster, a relatively new addition since mid-2015, boasts a remarkably high frequency of events, indicative of its robust functionality and popularity.\n",
    "\n",
    "Conversely, Sherman emerges as the newest station among its counterparts, having been introduced in the late 2020s. Given the chronological boundaries of our dataset, it is unsurprising that Sherman records a comparatively lower number of events, likely spanning only a few days within the dataset's timeframe.\n",
    "\n",
    "A discerning examination of the heatmap unveils several noteworthy insights:\n",
    "\n",
    "* A notable surge in station openings occurred between mid-2014 and the first month of 2016, with five out of nine stations inaugurated during this period.\n",
    "\n",
    "* A subsequent dip in event counts, particularly evident for Bryant, High, and Webster, around mid-2017 warrants further investigation to unearth the underlying causes.\n",
    "\n",
    "* A significant decline in event counts after March 2020 is striking and can be attributed to the unmistakable influence of the COVID-19 pandemic and the ensuing lockdown measures.\n",
    "\n",
    "With these preliminary findings in hand, we are well-prepared to dive deeper into our dataset. Our next steps involve uncovering hidden patterns and conducting a comprehensive comparative analysis of the stations, aiming to elucidate their individual characteristics compared to each other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_jjJQjOH_Jiz"
   },
   "source": [
    "### **3.1.2. Charging Stations Comparative Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SOWaPxFo5f4s"
   },
   "source": [
    "In this subsection, we will dive deeper into the comparative analysis of charging stations, focusing on energy consumption and charging duration per event."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TaQoCm_65kqK"
   },
   "source": [
    "To analyze energy consumption per event at different charging stations, we calculate the median energy consumption across all events and visualize the average energy consumption for each station. Additionally, we add a horizontal line to represent the median energy consumption across all events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-KL9GWlgDyuA"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate Median Energy Consumption over all events\n",
    "median_energy = EV[\"Energy (kWh)\"].median()\n",
    "\n",
    "# Calculate Median Energy Consumption per Station\n",
    "avg_station_energy = EV.groupby(\"Station Name\")[[\"Energy (kWh)\"]].median().reset_index()\n",
    "\n",
    "# Create a figure and axes\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=avg_station_energy, x=\"Station Name\", y=\"Energy (kWh)\", palette=\"Set2\")\n",
    "\n",
    "# Add horizontal line for median energy consumption\n",
    "ax.axhline(median_energy, color='r', linestyle='--', label='Median Energy Consumption')\n",
    "\n",
    "# Add data labels\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.1f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Station Name\")\n",
    "plt.ylabel(\"Energy (kWh)\")\n",
    "plt.title(\"Energy Consumption per Event by Station\")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPoCD3tV5_fG"
   },
   "source": [
    "The bar plot above displays the energy consumption per event for different charging stations. Sherman, Ted Tompson, and Webster use more energy than the median energy consumption which are highlighted. This analysis provides insights into variations in energy consumption across stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bK-RWLE06bxG"
   },
   "source": [
    "Next, we examine the charging duration per event at different charging stations. We calculate the median charging duration across all events and visualize the average charging duration for each station. Similar to the previous analysis, we include a horizontal line to represent the median charging duration across all events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Cn33yEtQq6E"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate Median Charging Duration over all events\n",
    "median_duration = EV[\"Charging_min\"].median()\n",
    "\n",
    "# Calculate Average Charging Duration per Station\n",
    "avg_station_duration = EV.groupby(\"Station Name\")[[\"Charging_min\"]].median().reset_index()\n",
    "\n",
    "# Create a figure and axes\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=avg_station_duration, x=\"Station Name\", y=\"Charging_min\", palette=\"Set2\")\n",
    "\n",
    "# Add horizontal line for median Charging Duration\n",
    "ax.axhline(median_duration, color='r', linestyle='--', label='Median Duration (Minutes)')\n",
    "\n",
    "# Add data labels\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.1f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Station Name\")\n",
    "plt.ylabel(\"Charging Duration (minutes)\")\n",
    "plt.title(\"Charging Duration per Event by Station\")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rptxjbm563gE"
   },
   "source": [
    "The bar plot above reveals that these three stations also exhibit longer charging durations. Interestingly, in the case of Sherman Station, the events have a comparatively shorter duration. This observation could be attributed to the station's novelty, suggesting that newer charging infrastructure may have a tendency to consume more energy while requiring less time for vehicle charging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lI56KFnzSjWf"
   },
   "source": [
    "### **3.1.3. Charging Stations Spatial Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l9KiT90EjR8f"
   },
   "source": [
    "In this section, we will conduct a spatial analysis of charging stations, specifically emphasizing the duration of non-charging events at various stations. Additionally, we will examine the average daily charging events for each station. These analyses will be visually presented on maps, allowing us to gain deeper insights into the relationship between station locations and these key metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TQQNfPdZyjrF"
   },
   "source": [
    "The term \"non-charging duration\" refers to the period when a vehicle is at the charging station, connected to the plug, but the actual charging process is not in progress. This situation often arises due to vehicles remaining parked at the charging station even after their charging has been completed, thereby occupying the charging plug.\n",
    "\n",
    "To initiate our analysis, we compute the average non-charging duration by aggregating data from all events. Subsequently, we visually represent the average non-charging duration for each charging station. Additionally, we include a horizontal line in the visualization to denote the overall average wait time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kYqe8LyxRgQH"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create non_charge_min Column\n",
    "EV[\"non_charge_min\"] = EV[\"Duration_min\"] - EV[\"Charging_min\"]\n",
    "\n",
    "# Calculate Average non_charge_min\n",
    "avg_non_charge = EV[\"non_charge_min\"].mean()\n",
    "\n",
    "# Calculate Average non_charge_min duration per Station\n",
    "non_charge = EV.groupby(\"Station Name\")[\"non_charge_min\"].mean().reset_index()\n",
    "\n",
    "# Create a figure and axes\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=non_charge, x=\"Station Name\", y=\"non_charge_min\", palette=\"Set2\")\n",
    "\n",
    "# Add horizontal line for average wait time\n",
    "ax.axhline(avg_non_charge, color='r', linestyle='--', label='Average Line')\n",
    "\n",
    "# Add data labels\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.1f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Station Name\")\n",
    "plt.ylabel(\"Minutes\")\n",
    "plt.title(\"Average Time Spent Without Charging at Stations\")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"upper left\")\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgtPGDI21aZy"
   },
   "source": [
    "In contrast to our previous calculations, we have opted for the mean instead of the median to compute these durations. The rationale behind this choice is to better capture the impact of outliers in Total Durations, particularly in specific stations.\n",
    "\n",
    "As depicted in the plot, three stations â€” MPL, Webster, and Rinconda Lib â€” exhibit non-charging durations surpassing the average line. This observation suggests that vehicles tend to linger at these stations beyond the expected charging time.\n",
    "\n",
    "To gain a more comprehensive understanding, let's proceed to visualize these values spatially."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "duS1YqED8lIh"
   },
   "outputs": [],
   "source": [
    "# Group the data to calculate the mean non_charge_min for each station\n",
    "non_charge_map = EV.groupby([\"Station Name\", \"Latitude\", \"Longitude\"])[\"non_charge_min\"].mean().reset_index()\n",
    "\n",
    "# Define a colormap for marker colors\n",
    "colormap = linear.YlOrRd_09.scale(non_charge_map['non_charge_min'].min(), non_charge_map['non_charge_min'].max())\n",
    "\n",
    "# Create a base map centered at Palo Alto with a different tileset\n",
    "base_map = folium.Map(location=[37.4419, -122.143936], zoom_start=13, tiles=\"CartoDB positron\")\n",
    "\n",
    "# Iterate over stations and add markers\n",
    "for index, row in non_charge_map.iterrows():\n",
    "    station_name = row['Station Name']\n",
    "    non_charge = row['non_charge_min']\n",
    "    latitude = row['Latitude']\n",
    "    longitude = row['Longitude']\n",
    "\n",
    "    # Get the corresponding marker color from the colormap\n",
    "    marker_color = colormap(non_charge)\n",
    "\n",
    "    # Create a custom HTML icon as an HTML string with inline style for color\n",
    "    icon_html = f'<i class=\"fa-solid fa-charging-station fa-xl\" style=\"color: {marker_color};\"></i>'\n",
    "\n",
    "    # Create the marker and set its icon using the custom HTML\n",
    "    marker = folium.Marker(\n",
    "        location=[latitude, longitude],\n",
    "        icon=folium.DivIcon(html=icon_html),\n",
    "        popup=folium.Popup(f\"Station: {station_name}, Non-charge Duration: {non_charge:.2f} minutes\", parse_html=True)\n",
    "    )\n",
    "\n",
    "    # Add the marker to the map\n",
    "    marker.add_to(base_map)\n",
    "\n",
    "# Add the colormap legend\n",
    "colormap.add_to(base_map)\n",
    "\n",
    "# Display the map\n",
    "base_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WG95R_zrhTxd"
   },
   "source": [
    "On the map, we observe that stations that are relatively distant from other charging stations and commercial points of interest tend to exhibit longer non-charge durations. This pattern suggests that these stations are likely used more by residential users, as they are in closer proximity to residential areas. The extended non-charge durations may indicate that vehicles tend to overstay, possibly for overnight charging events. Notably, MPL, Rinconda Lib, and Webster stations exemplify this scenario."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XPzCk1T0AJjk"
   },
   "source": [
    "In our next step, we will explore the daily average charging events that occur at different charging stations.\n",
    "\n",
    "To begin, we calculate the daily average charging events for each station and visualize the average event count per day for each station. Additionally, we add a horizontal line to represent the overall average event count per day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g6LZSHzkh7HY"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Group by 'Station Name' and 'Start Date', then counting the number of events\n",
    "charging_events_per_day = EV.groupby([\"Station Name\", \"Latitude\",\t\"Longitude\", EV[\"Start Date\"].dt.date]).size().reset_index(name='Event Count')\n",
    "\n",
    "# Calculate Average Event Count per day\n",
    "avg_event = charging_events_per_day[\"Event Count\"].mean()\n",
    "\n",
    "# Group by 'Station Name' and calculate the average charging events per day\n",
    "average_events_per_day = charging_events_per_day.groupby([\"Station Name\", \"Latitude\",\t\"Longitude\"])[\"Event Count\"].mean().reset_index(name='Average Events')\n",
    "\n",
    "# Create a figure and axes\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=average_events_per_day, x=\"Station Name\", y=\"Average Events\", palette=\"Set2\")\n",
    "\n",
    "# Add horizontal line for average Event Count\n",
    "ax.axhline(avg_event, color='r', linestyle='--', label='Average Line')\n",
    "\n",
    "# Add data labels\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.1f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Station Name\")\n",
    "plt.ylabel(\"Events per Day\")\n",
    "plt.title(\"Average Charging Events per Day in Stations\")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"upper center\")\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rPskro0-Avpj"
   },
   "source": [
    "By examining the bar plot, it becomes evident that Webster and Cambridge, followed by Bryant, stand out as the most popular charging stations. These stations exhibit daily event counts that surpass the average, indicating a higher level of utilization. To gain a deeper understanding of this trend, let's proceed to visualize these values spatially, providing further insights into the popularity of these stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5YAsei72Chz6"
   },
   "outputs": [],
   "source": [
    "# Define a colormap for marker colors\n",
    "colormap = linear.YlOrRd_09.scale(average_events_per_day['Average Events'].min(), average_events_per_day['Average Events'].max())\n",
    "\n",
    "# Create a base map centered at Palo Alto with a different tileset\n",
    "event_map = folium.Map(location=[37.4419, -122.143936], zoom_start=13, tiles=\"CartoDB positron\")\n",
    "\n",
    "# Iterate over stations and add markers\n",
    "for index, row in average_events_per_day.iterrows():\n",
    "    station_name = row['Station Name']\n",
    "    average_events = row['Average Events']\n",
    "    latitude = row['Latitude']\n",
    "    longitude = row['Longitude']\n",
    "\n",
    "    # Get the corresponding marker color from the colormap\n",
    "    marker_color = colormap(average_events)\n",
    "\n",
    "    # Create a custom HTML icon as an HTML string with inline style for color\n",
    "    icon_html = f'<i class=\"fa-solid fa-charging-station fa-xl\" style=\"color: {marker_color};\"></i>'\n",
    "\n",
    "    # Create the marker and set its icon using the custom HTML\n",
    "    marker = folium.Marker(\n",
    "        location=[latitude, longitude],\n",
    "        icon=folium.DivIcon(html=icon_html),\n",
    "        popup=folium.Popup(f\"Station: {station_name}, Average Events: {average_events:.2f} per Day\", parse_html=True)\n",
    "    )\n",
    "\n",
    "    # Add the marker to the map\n",
    "    marker.add_to(event_map)\n",
    "\n",
    "# Add the colormap legend\n",
    "colormap.caption = 'Average Events'\n",
    "event_map.add_child(colormap)\n",
    "\n",
    "# Display the map\n",
    "event_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cX0wJ3JywNfH"
   },
   "source": [
    "The map above provides a spatial representation of daily average charging events at different charging stations. Stations with higher daily average event counts are shown in darker shades, indicating greater popularity. Notably, stations like Webster and Cambridge appear to be the most popular, possibly due to specific land uses or points of interest in their vicinity.\n",
    "\n",
    "By analyzing daily average charging events and their spatial distribution, we can identify stations that are frequently used and investigate potential reasons behind their popularity, such as location and accessibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QkGjZK8e0ton"
   },
   "source": [
    "## **3.2. User-wise Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "COelZzPjNtd9"
   },
   "source": [
    "In this section, we will conduct a user-wise analysis, delving into the behavior of different user segments. These user segments were defined based on User IDs' event counts during the data preprocessing stage. A comprehensive explanation of the user segmentations and their underlying logic can be found in the Data Preprocessing section of our second segment. Before diving deep into the analysis, it's essential to have a brief recap of how these users have performed in our dataset.\n",
    "\n",
    "During the data preprocessing stage, we identified a total of 21,442 User IDs, one of these IDs are Unknown class with 7,677 events attributed to these unknown IDs. Among these 21,442 User IDs, 532 were categorized as Frequent users with events count for more than once a month, making up 2.5% of known User IDs. Occasional users comprised 3,075 distinct User IDs, accounting for 14.3% of known User IDs. Infrequent and Passing user IDs numbered 8,464 and 9,370 distinct User IDs, respectively, constituting 39.4% and 43.7% of the known User IDs. The majority of User IDs (83.1%) fall into the Infrequent and Passing segments. However, to gain a comprehensive understanding of their impact and significance within our dataset, we need to analyze the events associated with these segments.\n",
    "\n",
    "Let's begin the in-depth exploration of patterns within these user segments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nAQJp9-TZC-R"
   },
   "source": [
    "### **3.2.1. Average Charging Events per day by User Segment**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nn2eWukAOQ1V"
   },
   "source": [
    "In this section, we will examine the average number of charging events per day for each user segment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wJ6B8Ah4Fb-D"
   },
   "outputs": [],
   "source": [
    "# Grouping by 'User Segments' and 'Start Date', then counting the number of events\n",
    "charging_events_per_day_user = EV.groupby([\"User Segment\", EV[\"Start Date\"].dt.date]).size().reset_index(name='Event Count')\n",
    "\n",
    "# Calculating Average Event Count per day for each user segment\n",
    "avg_event_per_day_user = charging_events_per_day_user.groupby(\"User Segment\")[\"Event Count\"].mean().reset_index()\n",
    "\n",
    "# Calculate Average Event Count over\n",
    "avg_event = charging_events_per_day_user[\"Event Count\"].mean()\n",
    "\n",
    "# Setting a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Creating a bar plot\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=avg_event_per_day_user, x=\"User Segment\", y=\"Event Count\", palette=\"Set2\")\n",
    "\n",
    "# Add horizontal line for median Charging Duration\n",
    "ax.axhline(avg_event, color='r', linestyle='--', label='Median Events count')\n",
    "\n",
    "# Adding data labels above the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.2f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "# Adding title and labels\n",
    "plt.xlabel(\"User Segment\")\n",
    "plt.ylabel(\"Average Events per Day\")\n",
    "plt.title(\"Average Charging Events per Day by User Segment\")\n",
    "\n",
    "# Rotating x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Adding grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"upper right\")\n",
    "\n",
    "# Improving spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-nOLuDrOU0i"
   },
   "source": [
    "The bar plot above illustrates the average daily charging events for different user segments. Despite Frequent and Occasional users comprising only 2.5% and 14.3% of User IDs, respectively, they exhibit the highest daily impact, with an average of 35 events per day for Frequent users and 26 events per day for Occasional users. In contrast, the lowest event counts are associated with Passing and Unknown users, averaging nearly 3-4 events per day.\n",
    "\n",
    "> The insights gained from this analysis not only provide valuable information about the daily charging behavior of various user segments but also serve as a reminder of the Pareto principle, often referred to as the \"80-20 rule.\" This principle suggests that a significant proportion of effects come from a small percentage of causes. In our case, it becomes evident that a relatively small percentage of users, specifically Frequent and Occasional users, contribute to the majority of daily charging events. Understanding such dynamics can be crucial in optimizing services and resources to cater to the most active user segments efficiently."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C_ehsEUHVHB6"
   },
   "source": [
    "### **3.2.2. Charging Duration per Event by User Type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AWEefQVlRULx"
   },
   "source": [
    "In this section, we explore the charging duration per event for different user segments and gain insights into their charging behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "syRLNovGGRxn"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate Median Charging Duration\n",
    "median_duration = EV[\"Charging_min\"].median()\n",
    "\n",
    "# Calculate Average Charging Duration per User Segment\n",
    "avg_segment_duration = EV.groupby(\"User Segment\")[[\"Charging_min\"]].median().reset_index()\n",
    "\n",
    "# Create a figure and axes\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=avg_segment_duration, x=\"User Segment\", y=\"Charging_min\", palette=\"Set2\")\n",
    "\n",
    "# Add horizontal line for median Charging Duration\n",
    "ax.axhline(median_duration, color='r', linestyle='--', label='Median Duration (Minutes)')\n",
    "\n",
    "# Add data labels\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.1f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"User Segment\")\n",
    "plt.ylabel(\"Charging Duration (minutes)\")\n",
    "plt.title(\"Charging Duration per Event by User Type\")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend()\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lphXqP_kLzxz"
   },
   "source": [
    "The bar plot above illustrates the charging duration per event for different user segments. Notably, Frequent users exhibit the longest charging duration per event for more than 2 hours duration, followed by the Unknown segment and then Occasional users. This suggests that Unknown users behave similarly to Frequent and Occasional users in terms of charging duration, rather than resembling Infrequent and Passing users. Moreover, Passing users have slightly longer charging durations per event compared to Infrequent users. Given that energy consumption is a function of charging duration, we can infer that these proportions hold true for energy consumption as well."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "w5c01a7gSVTf"
   },
   "source": [
    "### **3.2.3. Non-Charging Duration per Event by User Type**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zz3HYWR_SRHF"
   },
   "source": [
    "Much like our Station-wise analysis, we will also conduct a Non-Charging Duration analysis for our user segments. This analysis aims to provide insights into which user types have a tendency to leave their vehicles plugged in and parked at the charging station without actively charging, consequently occupying the charging plug."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qWMAlZmtSRHH"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate Average non_charge_min\n",
    "avg_non_charge = EV[\"non_charge_min\"].mean()\n",
    "\n",
    "# Calculate Average non_charge_min duration per User Segment\n",
    "non_charge = EV.groupby(\"User Segment\")[\"non_charge_min\"].mean().reset_index()\n",
    "\n",
    "# Create a figure and axes\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(data=non_charge, x=\"User Segment\", y=\"non_charge_min\", palette=\"Set2\")\n",
    "\n",
    "# Add horizontal line for average wait time\n",
    "ax.axhline(avg_non_charge, color='r', linestyle='--', label='Average Line')\n",
    "\n",
    "# Add data labels\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f\"{p.get_height():.1f}\", (p.get_x() + p.get_width() / 2., p.get_height()),\n",
    "                ha='center', va='center', xytext=(0, 5), textcoords='offset points', fontsize=10)\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"User Segment\")\n",
    "plt.ylabel(\"Minutes\")\n",
    "plt.title(\"Average Time Spent Without Charging by User Type\")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(loc=\"upper center\")\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d-m6ywVXSRHH"
   },
   "source": [
    "This plot indicates that both Frequent users and Unknown users tend to spend more time at the charging stations than the expected charging duration. This analysis reinforces the notion that these unknown User IDs exhibit behavior similar to that of Frequent users, suggesting that they may belong to this user segment.\n",
    "\n",
    "Furthermore, we can infer that our Frequent users are likely comprised mainly of residents or commuters who engage in overnight charging events or frequent usage of these stations, which may also serve as parking lots.\n",
    "\n",
    "Additionally, we observe that shorter durations are associated with our Passing and Infrequent users. This observation suggests that they primarily utilize these stations solely for charging their vehicles and do not tend to linger or occupy the charging plugs for extended periods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9QXvhqLjL0qU"
   },
   "source": [
    "### **3.2.4. Distribution of User Segments per Station**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OiM6zm2KV2PD"
   },
   "source": [
    "In this section, we explore the distribution of user segments across charging stations, providing insights into the prevalence of various user types at each station."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L63xPva5WnO_"
   },
   "source": [
    "The following code involves grouping and counting user segments for each station, followed by normalizing the counts to represent proportions. The resulting stacked bar plot visually illustrates how various user segments are distributed at different charging stations, offering insights into the user demographic landscape within the charging network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tMxMZnuHXtgo"
   },
   "outputs": [],
   "source": [
    "# Group by 'Station Name' and 'User Segment' and count\n",
    "segment_num = EV.groupby([\"Station Name\", \"User Segment\"]).size().unstack(fill_value=0)\n",
    "\n",
    "# Normalize the counts for each station\n",
    "segment_num_normalized = segment_num.div(segment_num.sum(axis=1), axis=0)\n",
    "\n",
    "# Create a stacked bar plot using Pandas and Matplotlib\n",
    "ax = segment_num_normalized.plot(kind=\"bar\", stacked=True, figsize=(12, 6), cmap=\"tab20\")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel(\"Station Name\")\n",
    "plt.ylabel(\"Proportion\")\n",
    "plt.title(\"Normalized Distribution of User Types per Station\")\n",
    "\n",
    "# Rotate x-axis labels for better readability\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Add legend\n",
    "plt.legend(title=\"User Segments\", bbox_to_anchor=(1, 1))\n",
    "\n",
    "# Add grid lines\n",
    "plt.grid(axis=\"y\", linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add proportion numbers in the center of the stacked bars\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.2f', label_type='center', fontsize=10, color='black', weight='bold')\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FlEuKGZzLzhL"
   },
   "source": [
    "The stacked barplot reveals intriguing insights into user segment distribution across various charging stations. Notably, Rinconda Lib, Webster, and Ted Thompson stations emerge as highly favored destinations for frequent users, collectively accounting for over 50% of their usage. Conversely, other stations maintain a 44% share of frequent users. Interestingly, the newly opened Sherman station exhibits a different pattern, with only a 21% share of frequent users, suggesting a distinct preference for their habitual stations.\n",
    "\n",
    "Among occasional users, the percentages hover around 30-40% across stations. Ted Thompson stands out with a slightly larger share at 41%, while Hamilton lags slightly behind at 28%.\n",
    "\n",
    "Infrequent users represent around 10-17% of users at most stations, except for Ted Thompson and Rinconda Lib, which have shares below 10%.\n",
    "\n",
    "For passing users, the share ranges from 3-6%, with Ted Thompson and Rinconda Lib being less popular among this group, each garnering less than 3%. This trend suggests that these stations may not be as accessible to users who prefer more accessible charging locations. Remarkably, the newly opened Sherman station enjoys significant popularity among passing users, with an approximately 8% share.\n",
    "\n",
    "In the case of unknown users, their distribution varies from their charging duration habits, which was similar to Frequent users. Sherman station claims an 18% share, in contrast to other stations with shares ranging from 2-4%. Rinconda Lib, however, boasts a smaller share at 1%. These trends provide valuable insights into the station preferences of unknown users, highlighting Sherman station's distinct appeal within this user segment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B-3gUzR7bjfT"
   },
   "source": [
    "\n",
    "## **3.3. Origin-Destination Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CrPSOwFOhIji"
   },
   "source": [
    "With a solid grasp of both station-specific and user-specific data, we're ready to dive into a blended approach. In this section, we'll undertake an Origin-Destination Analysis that bridges the gap between users and stations. This analytical fusion is key to unveiling the spatial dimensions hidden within our dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wSklLnINbjfY"
   },
   "source": [
    "### **3.3.1. Geocoding Driver Postal Code**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YCxVShp6bIoJ"
   },
   "source": [
    "To kickstart our Origin-Destination Analysis, our first task is to compile a comprehensive list of distinct Driver Postal Codes from our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ua3vsT7qu7NT"
   },
   "outputs": [],
   "source": [
    "# Making a list of Driver Postal Codes\n",
    "postal_codes_list= EV[EV[\"Driver Postal Code\"] != \"Unknown\"][\"Driver Postal Code\"].astype(int)\n",
    "postal_codes_list= postal_codes_list.drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sUKTAEh8iBUa"
   },
   "source": [
    "With this list in hand, we can proceed to geocode these postal codes to obtain their corresponding geographical coordinates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gB07z9jsiJjO"
   },
   "source": [
    "To geocode the postal codes, we'll employ the pgeocode library. This library allows us to query the coordinates of postal codes and store the information in a dictionary list named \"postal_code_data.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKKyhEePubuN"
   },
   "outputs": [],
   "source": [
    "# Create a geocoder instance for the United States\n",
    "geolocator = pgeocode.Nominatim('us')\n",
    "\n",
    "postal_code_data = []\n",
    "\n",
    "# Iterate through the list of postal codes and fetch their coordinates\n",
    "for postal_code in postal_codes_list:\n",
    "    location = geolocator.query_postal_code(postal_code)\n",
    "\n",
    "    if not location.empty:\n",
    "        data = {\n",
    "            \"postal_code\": postal_code,\n",
    "            \"latitude\": location.latitude,\n",
    "            \"longitude\": location.longitude\n",
    "        }\n",
    "        postal_code_data.append(data)\n",
    "    else:\n",
    "        print(f\"No location found for postal code: {postal_code}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zAKe4uUdBUt"
   },
   "source": [
    "Now that we have gathered the geographical coordinates for each postal code, we can proceed to merge this data with our EV dataset, creating a new DataFrame named \"ev_zip_merged\". This merged dataset will contain the geographical coordinates (latitude and longitude) corresponding to each Driver Postal Code and it will be pivotal for our subsequent spatial analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gyaoWtDB2Ww7"
   },
   "outputs": [],
   "source": [
    "# Create a postal code DataFrame from the postal code data list\n",
    "df_postal = pd.DataFrame(postal_code_data)\n",
    "df_postal = df_postal.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "# Create a new DataFrame from the EV dataset, excluding records with \"Unknown\" postal codes\n",
    "zip_codes = EV[EV[\"Driver Postal Code\"] != \"Unknown\"].copy()  # Use .copy() to create a copy of the DataFrame\n",
    "# Changing postal code data type to integer for merging\n",
    "zip_codes[\"Driver Postal Code\"] = zip_codes[\"Driver Postal Code\"].astype(int)\n",
    "\n",
    "# Merge the postal code information with the original DataFrame\n",
    "ev_zip_merged = pd.merge(zip_codes, df_postal, left_on=\"Driver Postal Code\", right_on=\"postal_code\")\n",
    "\n",
    "# Print the tail of the merged DataFrame to inspect the result\n",
    "ev_zip_merged.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2OfcZ3Gjjq6T"
   },
   "source": [
    "With the \"ev_zip_merged\" DataFrame ready, we are now equipped to perform Origin-Destination Analysis and gain deeper insights into the spatial aspects of our data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l5fD8klQd1n9"
   },
   "source": [
    "### **3.3.2. Heatmap of Driver Postal Codes based on Average Events per Day**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J4kXv_nNkTj9"
   },
   "source": [
    "In this section, we visualize a heatmap representing Driver Postal Codes based on their average daily events. This visualization is instrumental in understanding the distribution of events across geographical locations.\n",
    "\n",
    "To create the heatmap, we follow these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6YL0wpdFlrvv"
   },
   "source": [
    "**1. Data Preparation:** We start by grouping our dataset by Driver Postal Codes and their respective coordinates. The aim is to calculate the total events for each postal code. Since our dataset spans over nine years, analyzing the raw event counts might not provide meaningful insights. Therefore, we normalize the data by dividing the event counts by the total days covered in the dataset, giving us the events per day for each postal code.\n",
    "\n",
    "To enhance the map's clarity, I implemented a filtering criterion to exclude postal codes with very low event counts. Specifically, I chose to display only Frequent postal codes with an average event rate exceeding once a month. Given that a typical month contains approximately 30 days, this filtering threshold was set at 0.03 Events per Day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UAP9_Gbn-KYc"
   },
   "outputs": [],
   "source": [
    "# Calculat3 Total Days over the Dataset\n",
    "total_days = len(EV[\"Start Date\"].dt.date.unique())\n",
    "\n",
    "# Group by 'Station Name' and calculate the average charging events per day\n",
    "pc_total_events = ev_zip_merged.groupby([\"postal_code\", \"latitude\",\t\"longitude\"]).size().reset_index(name='Total Events')\n",
    "\n",
    "# Calculate Events per Day by Postal Code\n",
    "pc_total_events[\"Event_p_Day\"]= pc_total_events[\"Total Events\"]/ total_days\n",
    "\n",
    "# Filter the Frequent Postal codes with average event more than once a month\n",
    "pc_total_events= pc_total_events[pc_total_events[\"Event_p_Day\"]> 0.033]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kVjVOPkrlh1Y"
   },
   "source": [
    "**2. Creating the Heatmap:** We use the Folium library to generate the heatmap. To enhance the map's readability, we add another layer representing Postal Code Markers. These markers are made nearly invisible but contain pop-up information with the postal code name and its corresponding events per day for better comprehension."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EjGjpENBTLq4"
   },
   "outputs": [],
   "source": [
    "# Create a base map centered at Palo Alto\n",
    "pcmap = folium.Map(location=[37.4419, -122.143936], zoom_start=10)\n",
    "\n",
    "# Convert the data into a list of (latitude, longitude, value) tuples\n",
    "heatmap_data = [\n",
    "    (row['latitude'], row['longitude'], row['Event_p_Day']) for index, row in pc_total_events.iterrows()\n",
    "]\n",
    "\n",
    "# Determine the range of values for the colorbar\n",
    "min_value = min(heatmap_data, key=lambda x: x[2])[2]\n",
    "max_value = max(heatmap_data, key=lambda x: x[2])[2]\n",
    "\n",
    "# Create a heatmap layer with default colormap\n",
    "heatmap_layer = HeatMap(heatmap_data, radius=20, blur=15)\n",
    "heatmap_layer.add_to(pcmap)\n",
    "heatmap_layer.layer_name = 'Heatmap of Frequent Events'\n",
    "\n",
    "# Add colorbar legend\n",
    "colormap_caption = 'Events per Day'\n",
    "colormap = folium.LinearColormap(colors=['steelblue', 'lime', 'yellow', 'orange', 'red'], vmin=min_value, vmax=max_value)\n",
    "colormap.caption = colormap_caption\n",
    "pcmap.add_child(colormap)\n",
    "\n",
    "# Create a feature group for the markers\n",
    "marker_group = folium.FeatureGroup(name='Postal Code Markers')\n",
    "\n",
    "# Iterate over postal codes and add markers with pop-up information\n",
    "for index, row in pc_total_events.iterrows():\n",
    "    postal_code = row['postal_code']\n",
    "    events = row['Event_p_Day']\n",
    "    latitude = row['latitude']\n",
    "    longitude = row['longitude']\n",
    "    color = colormap(events)\n",
    "    # Create the marker\n",
    "    marker = folium.CircleMarker(\n",
    "        [latitude, longitude], radius=6, color=color, fill=True, fill_color=color,opacity=0.001, fill_opacity=0.001,\n",
    "        popup=f'Postal Code: {postal_code}, Avg Events per Day: {events}').add_to(marker_group)\n",
    "\n",
    "# Add the marker group to the map, but keep it initially hidden\n",
    "marker_group.add_to(pcmap)\n",
    "marker_group.layer_name = 'Postal Code Popup'\n",
    "\n",
    "# Add a layer control to toggle visibility of the layers\n",
    "layer_control = folium.LayerControl(collapsed=False).add_to(pcmap)\n",
    "\n",
    "# Show map\n",
    "pcmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bJ8PzriVr2Mr"
   },
   "source": [
    "The heatmap reveals insights into event distribution. Notably, a concentration of events originates from the city itself, Palo Alto. However, nearby cities like San Francisco (SF), Santa Clara, San Jose, and Oakland also contribute, albeit with event rates mostly less than one per day. Zooming out further, the map displays points from distant locations such as Los Angeles (LA) and even outside of California, including cities like Portland, and Denver. Interestingly, there is a point from Green Bay, Wisconsin, indicating a possible long trip or an extended stay in Palo Alto.\n",
    "This heatmap provides a visual understanding of event patterns across different postal codes, enabling further exploration and analysis of the dataset's spatial aspects.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "coLXwXbKQtWK"
   },
   "source": [
    "### **3.3.3. Origin-Destination Flowmap Based On Hour of the day**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1TTSaAmlwatj"
   },
   "source": [
    "In this section, we conduct an Origin-Destination (OD) analysis to explore the flow of events from user coordinates to station coordinates. However, we add a temporal dimension by analyzing these flows for each hour of the day, offering insights into hourly usage patterns.\n",
    "\n",
    "To create this dynamic flow map, we follow these steps:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBGmttWCwi1v"
   },
   "source": [
    "**1. Data Preparation:** We start by extracting the hour component from the start dates in our previously prepared dataset, ev_zip_merged. This allows us to categorize events by the hour they occurred. We then group the data by 'Hour,' 'postal_code,' 'Station Name,' and their corresponding coordinates. Within each group, we calculate the event counts. These event counts are stored in a new dataset named hourly_OD. Subsequently, we normalize the counts by dividing them by the total number of days in our dataset, obtaining 'Events per Day' values. To enhance map clarity, we filter the dataset to retain only routes with an average event rate exceeding 0.033, indicating routes with more than one event per month for that particular hour.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gToicn53Q15z"
   },
   "outputs": [],
   "source": [
    "# Calculate Total Days over the Dataset\n",
    "total_days = len(EV[\"Start Date\"].dt.date.unique())\n",
    "\n",
    "# Add Hour column to ev_zip_merged\n",
    "ev_zip_merged[\"Hour\"]= ev_zip_merged[\"Start Date\"].dt.hour\n",
    "\n",
    "# Group by 'Hour', 'postal_code', 'Station Name' and their related coordinates, then calculate the Event Counts\n",
    "hourly_OD = ev_zip_merged.groupby([\"Hour\", \"postal_code\", \"latitude\",\t\"longitude\", \"Station Name\", \"Latitude\", \"Longitude\"]).size().reset_index(name='Event Count')\n",
    "\n",
    "# Calculate Events per Day\n",
    "hourly_OD[\"Event_p_Day\"]= hourly_OD[\"Event Count\"] / total_days\n",
    "\n",
    "# Filter the Frequent Routes with average event more than once a month for that perticular hour\n",
    "OD_freq= hourly_OD[hourly_OD[\"Event_p_Day\"]> 0.033]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uh2GxaXasMEg"
   },
   "source": [
    "**2. Creating the Flowmap:** To visualize the hourly OD flows, we define the data, a colormap, and a function to update the map based on the selected hour."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "etz34RTBe5WQ"
   },
   "outputs": [],
   "source": [
    "# Load origin-destination dataset\n",
    "data = OD_freq\n",
    "\n",
    "# Define a list of colors from yellow to red\n",
    "colors = ['#FFFF00', '#FF7700', '#FF0000']\n",
    "\n",
    "# Create a custom colormap from the list of colors\n",
    "cmap = LinearSegmentedColormap.from_list('custom', colors, N=256)\n",
    "\n",
    "# Normalize the colormap based on the range of your events values\n",
    "norm = mcolors.Normalize(vmin=data['Event_p_Day'].min(), vmax=data['Event_p_Day'].max())\n",
    "\n",
    "# Define a function to map events to colors\n",
    "def map_events_to_color(events):\n",
    "    color = cmap(norm(events))\n",
    "    color_hex = mcolors.rgb2hex(color)\n",
    "    return color_hex\n",
    "\n",
    "# Define a function to update the flow map based on the selected hour\n",
    "def update_flow_map(hour):\n",
    "    # Create a map centered at Palo Alto\n",
    "    m = folium.Map(location=[37.4419, -122.1430], zoom_start=10, tiles='CartoDB Dark_Matter', control_scale=True)\n",
    "\n",
    "    # Filter data based on the selected hour\n",
    "    filtered_data = data[data['Hour'] == hour]\n",
    "\n",
    "    # Add lines from origin to destination with color based on Events per Day\n",
    "    for _, row in filtered_data.iterrows():\n",
    "        origin = (row['latitude'], row['longitude'])\n",
    "        destination = (row['Latitude'], row['Longitude'])\n",
    "        events= row['Event_p_Day']\n",
    "\n",
    "        # Create a PolyLine connecting origin to destination with custom color\n",
    "        line = folium.PolyLine(\n",
    "            locations=[origin, destination],\n",
    "            color=map_events_to_color(events),\n",
    "            weight=1,\n",
    "            popup=f'Avg Events per Day for This Route: {events:.3f}' # Popup displaying events per day\n",
    "        )\n",
    "        line.add_to(m)\n",
    "\n",
    "        # Customize icon for the postal codes (origins)\n",
    "        icon = folium.DivIcon(\n",
    "            html=f'<i class=\"fa-solid fa-house-user fa-xl\" style=\"color: #ff0096;\"></i>',  # Use an icon from FontAwesome\n",
    "        )\n",
    "        folium.Marker(\n",
    "            location=origin,\n",
    "            icon=icon,\n",
    "            popup=row['postal_code']  # Set the postal code as the popup for the origin\n",
    "        ).add_to(m)\n",
    "\n",
    "        # Customize icon for the Stations (destinations)\n",
    "        icon = folium.DivIcon(\n",
    "            html=f'<i class=\"fa-solid fa-charging-station fa-xl\" style=\"color: #0084ff;\"></i>',  # Use an icon from FontAwesome\n",
    "        )\n",
    "        folium.Marker(\n",
    "            location=destination,\n",
    "            icon=icon,\n",
    "            popup=row['Station Name']  # Set the postal code as the popup for the destination\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Display the map\n",
    "    display(m)\n",
    "\n",
    "# Create a slider to change the hour of the day\n",
    "hour_slider = widgets.IntSlider(\n",
    "    min=int(data['Hour'].min()),\n",
    "    max=int(data['Hour'].max()),\n",
    "    step=1,\n",
    "    value=int(data['Hour'].min()),\n",
    "    description='Hour of the day',\n",
    "    continuous_update=True  # Update when the slider is moving\n",
    ")\n",
    "\n",
    "# Use widgets.interactive to connect the slider to the update_flow_map function\n",
    "interactive_plot = widgets.interactive(update_flow_map, hour=hour_slider)\n",
    "\n",
    "# Display the interactive plot\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-oHhLIssfZr"
   },
   "source": [
    "This interactive map provides a comprehensive insight into the hourly dynamics of user interactions between their origin points and our stations, which serve as destinations. By adjusting the hour slider, we gain valuable perspectives on how these interactions evolve throughout the day. During the morning hours, we observe a diverse array of routes originating from various locations beyond Palo Alto, indicating significant commuter activity. Although early-morning hours also witness participation from city residents, this diversity underscores the utilization of stations by commuters, particularly those in proximity to points of interest.\n",
    "\n",
    "However, as the day progresses past the morning hours, we witness a shift in user demographics. The majority of users originate from postal codes within Palo Alto, with greater emphasis on stations located near residential areas. This trend signifies heightened usage by Palo Alto residents during these hours, reflecting their reliance on the charging infrastructure.\n",
    "\n",
    "In conclusion, this dynamic map offers a nuanced view of user-station interactions, allowing us to discern commuting patterns and resident preferences for charging stations throughout the day. Such insights are invaluable for optimizing station placement and services to cater to the distinct needs of our diverse user base."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "99I7Kv3muvAk"
   },
   "source": [
    "### **3.3.4. Origin-Destination Flowmap of Average Events per Day among Daily Routes**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oMY7La-m3Moo"
   },
   "source": [
    "In this section, we delve into another Origin-Destination (OD) flow analysis to visualize the average events per day among most frequent daily routes. To construct this flowmap, we begin by grouping the data by Driver Postal Codes, Station Names, and their corresponding coordinates. After aggregating the total events for each grouping, we normalize these counts by dividing them by the total days in the dataset. To ensure a concise map, we filter out flows with more than one event per day, focusing on the most significant routes. Let's first examine our filtered flows dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F1DC_AN249Tk"
   },
   "outputs": [],
   "source": [
    "# Calculate Total Days over the Dataset\n",
    "total_days = len(EV[\"Start Date\"].dt.date.unique())\n",
    "\n",
    "# Group by 'postal_code', 'Station Name' and their related coordinates, then calculate the Total Events\n",
    "pc_station_events = ev_zip_merged.groupby([\"postal_code\", \"latitude\",\t\"longitude\", \"Station Name\", \"Latitude\", \"Longitude\"]).size().reset_index(name='Total Events')\n",
    "\n",
    "# Calculate Events per Day\n",
    "pc_station_events[\"Event_p_Day\"]= pc_station_events[\"Total Events\"]/ total_days\n",
    "\n",
    "# Filter the routes that their average event is more than once a day\n",
    "pc_station_freq= pc_station_events[pc_station_events[\"Event_p_Day\"]> 1]\n",
    "\n",
    "# Check the first rows of pc_station_events\n",
    "pc_station_freq.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "akIXwojm4F6x"
   },
   "source": [
    "For visualizing this flowmap, we utilize a new tool, the Kepler.gl library, a powerful tool for exploring large-scale geolocation datasets. Kepler.gl offers interactive mapping capabilities, allowing us to create our favorable map."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_aFIkohCxbTI"
   },
   "source": [
    "We must specify the map configuration, which includes the addition of Postal Code points, Station Points, and Flow arcs connecting Postal Codes to their corresponding stations. Additionally, we establish a color range for the flows based on the average events per day. The map's style, map state, and interaction configuration are also defined. This process proved to be more extensive than initially anticipated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a0-Af6at5tTk"
   },
   "outputs": [],
   "source": [
    "# Define the Kepler.gl map configuration JSON\n",
    "map_config = {\n",
    "    \"version\": \"v1\",\n",
    "    \"config\": {\n",
    "        \"visState\": {\n",
    "            \"filters\": [],\n",
    "            \"legend\": True,\n",
    "            \"layers\": [\n",
    "                {\n",
    "                    \"id\": \"iwhexfq\",\n",
    "                    \"type\": \"point\",\n",
    "                    \"config\": {\n",
    "                        \"dataId\": \"flow_data\",\n",
    "                        \"label\": \"Stations\",\n",
    "                        \"color\": [218, 112, 191],\n",
    "                        \"highlightColor\": [252, 242, 26, 255],\n",
    "                        \"columns\": {\n",
    "                            \"lat\": \"Latitude\",\n",
    "                            \"lng\": \"Longitude\",\n",
    "                            \"altitude\": None\n",
    "                        },\n",
    "                        \"isVisible\": True,\n",
    "                        \"visConfig\": {\n",
    "                            \"radius\": 20,\n",
    "                            \"fixedRadius\": False,\n",
    "                            \"opacity\": 1,\n",
    "                            \"outline\": False,\n",
    "                            \"thickness\": 2,\n",
    "                            \"strokeColor\": None,\n",
    "                            \"colorRange\": {\n",
    "                                \"name\": \"Global Warming\",\n",
    "                                \"type\": \"sequential\",\n",
    "                                \"category\": \"Uber\",\n",
    "                                \"colors\": [\"#5A1846\", \"#900C3F\", \"#C70039\", \"#E3611C\", \"#F1920E\", \"#FFC300\"]\n",
    "                            },\n",
    "                            \"strokeColorRange\": {\n",
    "                                \"name\": \"Global Warming\",\n",
    "                                \"type\": \"sequential\",\n",
    "                                \"category\": \"Uber\",\n",
    "                                \"colors\": [\"#5A1846\", \"#900C3F\", \"#C70039\", \"#E3611C\", \"#F1920E\", \"#FFC300\"]\n",
    "                            },\n",
    "                            \"radiusRange\": [0, 50],\n",
    "                            \"filled\": True\n",
    "                        },\n",
    "                        \"hidden\": False,\n",
    "                        \"textLabel\": [\n",
    "                            {\n",
    "                                \"field\": {\n",
    "                                    \"name\": \"Station Name\",\n",
    "                                    \"type\": \"string\"\n",
    "                                },\n",
    "                                \"color\": [218, 112, 191],\n",
    "                                \"size\": 12,\n",
    "                                \"offset\": [0, 0],\n",
    "                                \"anchor\": \"end\",\n",
    "                                \"alignment\": \"center\"\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    \"visualChannels\": {\n",
    "                        \"colorField\": None,\n",
    "                        \"colorScale\": \"quantile\",\n",
    "                        \"strokeColorField\": None,\n",
    "                        \"strokeColorScale\": \"quantile\",\n",
    "                        \"sizeField\": None,\n",
    "                        \"sizeScale\": \"linear\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"id\": \"897h7as\",\n",
    "                    \"type\": \"point\",\n",
    "                    \"config\": {\n",
    "                        \"dataId\": \"flow_data\",\n",
    "                        \"label\": \"Driver Zip Codes\",\n",
    "                        \"color\": [71, 211, 217],\n",
    "                        \"highlightColor\": [252, 242, 26, 255],\n",
    "                        \"columns\": {\n",
    "                            \"lat\": \"latitude\",\n",
    "                            \"lng\": \"longitude\",\n",
    "                            \"altitude\": None\n",
    "                        },\n",
    "                        \"isVisible\": True,\n",
    "                        \"visConfig\": {\n",
    "                            \"radius\": 20,\n",
    "                            \"fixedRadius\": False,\n",
    "                            \"opacity\": 1,\n",
    "                            \"outline\": False,\n",
    "                            \"thickness\": 2,\n",
    "                            \"strokeColor\": None,\n",
    "                            \"colorRange\": {\n",
    "                                \"name\": \"Global Warming\",\n",
    "                                \"type\": \"sequential\",\n",
    "                                \"category\": \"Uber\",\n",
    "                                \"colors\": [\"#5A1846\", \"#900C3F\", \"#C70039\", \"#E3611C\", \"#F1920E\", \"#FFC300\"]\n",
    "                            },\n",
    "                            \"strokeColorRange\": {\n",
    "                                \"name\": \"Global Warming\",\n",
    "                                \"type\": \"sequential\",\n",
    "                                \"category\": \"Uber\",\n",
    "                                \"colors\": [\"#5A1846\", \"#900C3F\", \"#C70039\", \"#E3611C\", \"#F1920E\", \"#FFC300\"]\n",
    "                            },\n",
    "                            \"radiusRange\": [0, 50],\n",
    "                            \"filled\": True\n",
    "                        },\n",
    "                        \"hidden\": False,\n",
    "                        \"textLabel\": [\n",
    "                            {\n",
    "                                \"field\": {\n",
    "                                    \"name\": \"postal_code\",\n",
    "                                    \"type\": \"integer\"\n",
    "                                },\n",
    "                                \"color\": [117, 222, 227],\n",
    "                                \"size\": 15,\n",
    "                                \"offset\": [0, 0],\n",
    "                                \"anchor\": \"middle\",\n",
    "                                \"alignment\": \"bottom\"\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    \"visualChannels\": {\n",
    "                        \"colorField\": None,\n",
    "                        \"colorScale\": \"quantile\",\n",
    "                        \"strokeColorField\": None,\n",
    "                        \"strokeColorScale\": \"quantile\",\n",
    "                        \"sizeField\": None,\n",
    "                        \"sizeScale\": \"linear\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"id\": \"rbwl8u\",\n",
    "                    \"type\": \"arc\",\n",
    "                    \"config\": {\n",
    "                        \"dataId\": \"flow_data\",\n",
    "                        \"label\": \"Flow\",\n",
    "                        \"color\": [136, 87, 44],\n",
    "                        \"highlightColor\": [252, 242, 26, 255],\n",
    "                        \"columns\": {\n",
    "                            \"lat0\": \"latitude\",\n",
    "                            \"lng0\": \"longitude\",\n",
    "                            \"lat1\": \"Latitude\",\n",
    "                            \"lng1\": \"Longitude\"\n",
    "                        },\n",
    "                        \"isVisible\": True,\n",
    "                        \"visConfig\": {\n",
    "                            \"opacity\": 0.8,\n",
    "                            \"thickness\": 5,\n",
    "                            \"colorRange\": {\n",
    "                                \"name\": \"ColorBrewer YlOrRd-3\",\n",
    "                                \"type\": \"sequential\",\n",
    "                                \"category\": \"ColorBrewer\",\n",
    "                                \"colors\": [\"#ffeda0\", \"#feb24c\", \"#f03b20\"]\n",
    "                            },\n",
    "                            \"sizeRange\": [0, 10],\n",
    "                            \"targetColor\": None\n",
    "                        },\n",
    "                        \"hidden\": False,\n",
    "                        \"textLabel\": [\n",
    "                            {\n",
    "                                \"field\": None,\n",
    "                                \"color\": [255, 255, 255],\n",
    "                                \"size\": 18,\n",
    "                                \"offset\": [0, 0],\n",
    "                                \"anchor\": \"start\",\n",
    "                                \"alignment\": \"center\"\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    \"visualChannels\": {\n",
    "                        \"colorField\": {\n",
    "                            \"name\": \"Total Events\",\n",
    "                            \"type\": \"integer\"\n",
    "                        },\n",
    "                        \"colorScale\": \"quantile\",\n",
    "                        \"sizeField\": None,\n",
    "                        \"sizeScale\": \"linear\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"interactionConfig\": {\n",
    "                \"tooltip\": {\n",
    "                    \"fieldsToShow\": {\n",
    "                        \"flow_data\": [\n",
    "                            {\"name\": \"postal_code\", \"format\": None},\n",
    "                            {\"name\": \"Station Name\", \"format\": None},\n",
    "                            {\"name\": \"Event_p_Day\", \"format\": None}\n",
    "                        ]\n",
    "                    },\n",
    "                    \"compareMode\": False,\n",
    "                    \"compareType\": \"absolute\",\n",
    "                    \"enabled\": True\n",
    "                },\n",
    "                \"brush\": {\n",
    "                    \"size\": 0.5,\n",
    "                    \"enabled\": False\n",
    "                },\n",
    "                \"geocoder\": {\n",
    "                    \"enabled\": False\n",
    "                },\n",
    "                \"coordinate\": {\n",
    "                    \"enabled\": False\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mapState\": {\n",
    "            \"bearing\": 24,\n",
    "            \"dragRotate\": True,\n",
    "            \"latitude\": 37.435048277614534,\n",
    "            \"longitude\": -122.12024206621042,\n",
    "            \"pitch\": 50,\n",
    "            \"zoom\": 12,\n",
    "            \"isSplit\": False\n",
    "        },\n",
    "        \"mapStyle\": {\n",
    "            \"styleType\": \"muted_night\",\n",
    "            \"topLayerGroups\": {},\n",
    "            \"visibleLayerGroups\": {\n",
    "                \"label\": True,\n",
    "                \"road\": True,\n",
    "                \"border\": False,\n",
    "                \"building\": True,\n",
    "                \"water\": True,\n",
    "                \"land\": True,\n",
    "                \"3d building\": False\n",
    "            },\n",
    "            \"threeDBuildingColor\": [9.665468314072013, 17.18305478057247, 31.1442867897876],\n",
    "            \"mapStyles\": {}\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2bfikeo5euM"
   },
   "source": [
    "The map configuration is extensive and includes settings for points, arcs, tooltips, and map styles. Now, let's initialize the Kepler.gl map instance and add our filtered data to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Pd3iMS-w-wVB"
   },
   "outputs": [],
   "source": [
    "# Initialize Kepler.gl map instance with custom configuration\n",
    "flowmap = KeplerGl(height=600, config=map_config, show_docs=False)\n",
    "\n",
    "# Add data to the map\n",
    "flowmap.add_data(data=pc_station_freq, name='flow_data')\n",
    "\n",
    "# Display the map\n",
    "flowmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hmKSxMvf5lVL"
   },
   "source": [
    "The flowmap reveals interesting insights. Most frequent routes originate from three primary postal codes within Palo Alto. These postal codes predominantly visit the nearest stations:\n",
    "\n",
    "* Postal code 94301 is most frequently associated with the Webster station, followed by Bryant and Hamilton stations.\n",
    "* Postal code 94303 is farther from other stations, with Rinconda Library being the closest. However, due to limited station availability, MPL station receives a significant number of events from this area, suggesting a potential need for a closer station.\n",
    "* Postal code 94306 primarily visits the MPL station, with Cambridge and Ted Thompson stations also being utilized.\n",
    "\n",
    "Comparing the events per day of these flows with the total events per day for stations (as seen in the previous sections), it becomes evident that these popular flows account for nearly 25% of the daily events. This information can inform decisions about station placement and resource allocation to better serve high-traffic routes and areas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7371h_4ZEdC_"
   },
   "source": [
    "\n",
    "## **3.4. Long Term Trends and Insights**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HdS-c_8kEwPa"
   },
   "source": [
    "In this section, we delve into the long-term trends and gain valuable insights from our electric vehicle (EV) charging data. Our objective is to analyze how energy consumption has evolved over time and how external factors have influenced charging patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2nL6FlTExr-"
   },
   "source": [
    "### **3.4.1. Energy Consumption over Time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U_3SrH8oPA47"
   },
   "source": [
    "To begin, we resample our data at weekly intervals and aggregate the energy consumption, duration, and associated fees within these weekly intervals. This allows us to observe trends at a broader temporal scale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "k1Mx_P9DwDuY"
   },
   "outputs": [],
   "source": [
    "# Resample the data on a weekly basis and calculate sums\n",
    "wev= EV_s.resample(\"W\", on= \"Start Date\")[[\"Duration_min\", \"Energy (kWh)\", \"Fee\"]].sum()\n",
    "wev.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eZE9GysCPMdA"
   },
   "source": [
    "Now, with our weekly resampled data in hand, we proceed to visualize the energy consumption over time. To gain a comprehensive understanding of this trend, we create two additional subplots:\n",
    "\n",
    "* **Charging Fee Over Time:** This subplot enables us to examine the introduction of charging fees. As established in our data preprocessing stage, many events were free of charge before a specific date. Visualizing this feature over time provides insights into both free and paid charging events, as well as their impact on energy consumption at charging stations.\n",
    "\n",
    "* **Active Charging Ports Over Time:** Not all charging stations have records spanning our entire dataset, which covers the period from 2011 to the end of 2020. Some stations may have been established later, and there may have been developments within stations, such as the addition of more charging ports, over the years. To explore this aspect, we utilize our EV_s dataset, which we preserved before consolidating station data in Data Preorocessing section. This dataset allows us to calculate the count of active charging ports over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DsVpqF5D9S1W"
   },
   "outputs": [],
   "source": [
    "# Create a figure with 2 subplots (2 rows, 1 column)\n",
    "fig, axs = plt.subplots(3, 1, figsize=(15, 9))\n",
    "\n",
    "# Define the vertical lines position of Major Events\n",
    "Fee = pd.to_datetime(\"2017-08-01\")\n",
    "Covid = pd.to_datetime(\"2020-03-01\")\n",
    "\n",
    "# First subplot for Energy Usage\n",
    "axs[0].plot(wev.index, wev[\"Energy (kWh)\"], label='Energy Usage')\n",
    "axs[0].axvline(x=Fee, color='g', linestyle='--', linewidth=2, label='Charging Fee')\n",
    "axs[0].axvline(x=Covid, color='r', linestyle='--', linewidth=2, label='Covid-19 Lockdown')\n",
    "axs[0].yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "axs[0].set_title(\"Energy Consumption Over Time\")\n",
    "axs[0].set_xlabel(\"Date\")\n",
    "axs[0].set_ylabel(\"Energy Used\")\n",
    "axs[0].legend()\n",
    "\n",
    "# Second subplot for Charging Fee\n",
    "axs[1].plot(wev.index, wev[\"Fee\"], label='Fee')\n",
    "axs[1].axvline(x=Fee, color='g', linestyle='--', linewidth=2, label='Charging Fee')\n",
    "axs[1].axvline(x=Covid, color='r', linestyle='--', linewidth=2, label='Covid-19 Lockdown')\n",
    "axs[1].yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "axs[1].set_title(\"Charging Fee Over Time\")\n",
    "axs[1].set_xlabel(\"Date\")\n",
    "axs[1].set_ylabel(\"Fee\")\n",
    "axs[1].legend()\n",
    "\n",
    "# Third subplot for Charging Ports Count\n",
    "port = EV_s.resample('D', on=\"Start Date\")[\"Station Name\"].nunique()\n",
    "axs[2].plot(port.index, port, label='Active Ports')\n",
    "axs[2].axvline(x=Fee, color='g', linestyle='--', linewidth=2, label='Charging Fee')\n",
    "axs[2].axvline(x=Covid, color='r', linestyle='--', linewidth=2, label='Covid-19 Lockdown')\n",
    "axs[2].yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "axs[2].set_title(\"Active Charging Ports Over Time\")\n",
    "axs[2].set_xlabel(\"Date\")\n",
    "axs[2].set_ylabel(\"Ports Count\")\n",
    "axs[2].legend()\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the combined plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cRkOT55IR8I4"
   },
   "source": [
    "By examining the energy consumption trends over time, we can observe some significant spikes and shifts that can be elucidated with the aid of our Fee and Active Ports subplots. Here are the key findings:\n",
    "\n",
    "1. The rapid increase in energy consumption from 2015 to 2016 can be attributed to the establishment of new charging stations. The presence of more active charging ports is a clear driver of increased energy consumption during this period. It's worth noting that this trend coincides with the overall upward trajectory of the electric vehicle market share during those years.\n",
    "\n",
    "2. Conversely, there are substantial drops in energy consumption at two distinct junctures, as indicated by the vertical lines on the plots. The first drop occurred in August 2017. Examining the Charging Fee plot reveals that prior to August 2017, charging events at these public stations were free of charge. Investigating further, it was found that starting in August 2017, the city introduced a new fee of 23 cents per kilowatt-hour for charging. Additionally, the station technology was enhanced to encourage drivers to relocate their fully charged vehicles through mobile notifications. After a 20-minute grace period, drivers faced a fee of $2 per hour for every hour their fully charged cars remained plugged in. This policy change likely had an impact not only on energy consumption but also on the total duration of charging events, warranting further analysis in subsequent sections.\n",
    "\n",
    "3. The third substantial decline occurred after March 2020, which can be directly attributed to the COVID-19 pandemic outbreak and the subsequent lockdown measures. This external factor had a profound impact on charging behavior and, consequently, led to a notable decrease in energy consumption.\n",
    "\n",
    "While we have successfully addressed some of our initial questions, the beauty of data analysis lies in the fact that as we answer certain questions, new inquiries inevitably emerge. Therefore, our next steps involve conducting a more detailed examination of the discussed major events impacts on our dataset, with a focus on charging stations, user segments, and the duration of charging events over time.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cGCxzxT2GfG7"
   },
   "source": [
    "### **3.4.2. Station-wise Energy Consumption over Time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cHwqUEHTGfkz"
   },
   "source": [
    "In this section, we dive deeper into the impacts of the establishment of charging fees and the effects of COVID-19 lockdowns on our charging stations. However, instead of examining each station individually, we opt for a more aggregated approach. We group the stations by their postal codes and analyze the clustered energy consumption patterns for each postal code. This approach provides a more concise overview of the impact analysis.\n",
    "\n",
    "Before delving into the analysis, let's list the stations located within each postal code:\n",
    "\n",
    "* 94301: Stations include Bryant, Hamilton, High, and Webster.\n",
    "* 94303: Stations include MPL and Rinconda Lib.\n",
    "* 94306: Stations include Cambridge, Sherman, and Ted Thompson.\n",
    "\n",
    "Segmenting stations by postal code aligns with our earlier observation of behavior similarity among stations within the same postal code, reinforcing our understanding of spatially similar charging behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VfUXZrc_6euj"
   },
   "outputs": [],
   "source": [
    "# Group by \"Postal Code\" and resample using pd.Grouper\n",
    "grouped = EV.groupby(['Postal Code', pd.Grouper(key='Start Date', freq='W')])['Energy (kWh)'].sum().reset_index()\n",
    "\n",
    "# Create a line plot for each Postal Code in separate subplots\n",
    "fig, axes = plt.subplots(nrows=len(grouped['Postal Code'].unique()), figsize=(15, 9), sharex=True)\n",
    "\n",
    "for i, (postal_code, data) in enumerate(grouped.groupby('Postal Code')):\n",
    "    ax = axes[i]\n",
    "    ax.plot(data['Start Date'], data['Energy (kWh)'], label=f'Postal Code {postal_code}')\n",
    "    ax.set_title(f'Energy Consumption for Postal Code {postal_code}')\n",
    "    ax.set_ylabel('Total Energy (kWh)')\n",
    "    ax.grid(linestyle='--')\n",
    "\n",
    "    # Add vertical lines at specific dates\n",
    "    ax.axvline(x=Fee, color='g', linestyle='--', linewidth=2, label='Charging Fee')\n",
    "    ax.axvline(x=Covid, color='r', linestyle='--', linewidth=2, label='Covid-19 Lockdown')\n",
    "    ax.legend()\n",
    "\n",
    "# Set a common x-axis label for the bottom subplot\n",
    "axes[-1].set_xlabel('Date')\n",
    "\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ae6VIAJFx9RH"
   },
   "source": [
    "Analyzing the energy consumption trends over time for these postal codes reveals distinct impacts of the charging fee establishment and the COVID-19 pandemic on different sets of stations.\n",
    "\n",
    "**1. Impact of Charging Fee Establishment:**\n",
    "\n",
    "* Postal code 94301, which contains stations in proximity to major commercial and educational areas, experienced approximately a 40% drop in energy consumption after the fee establishment. This suggests that many users opted not to charge their vehicles in these stations once they had to pay for it.\n",
    "* Postal code 94303, which includes stations near residential areas, saw a more significant decline, exceeding 50% in energy consumption. This indicates that stations frequented by residents, who may tend to overstay their vehicles while charging, were particularly affected.\n",
    "* In contrast, postal code 94306, with its stations, showed a more moderate drop of around 30% after the fee imposition.\n",
    "\n",
    "**2. Transition Period between Fee Establishment and COVID-19 Lockdown:**\n",
    "\n",
    "* During the period following the introduction of charging fees, stations in postal code 94306 not only recovered but also continued to experience an upward trend in energy consumption.\n",
    "* For stations in postal code 94301, while they exhibited signs of recovery, they couldn't reach their pre-fee establishment consumption levels.\n",
    "* Notably, stations in postal code 94303 did not recover from the fee establishment, and energy consumption remained relatively stable. This suggests that users who preferred free charging events were dissatisfied with the new fee structure, and only loyal and frequent users continued to utilize these stations.\n",
    "\n",
    "**3. Impact of COVID-19 Lockdown:**\n",
    "\n",
    "* The COVID-19 pandemic had a substantial impact on all three postal codes, resulting in a significant drop of over 90% in energy consumption. Unfortunately, our dataset only extends until the end of 2021, preventing us from analyzing the full impact of the pandemic and its subsequent effects. However, based on the limited data available for 2021, it is evident that stations struggled to recover from this shock during that year.\n",
    "\n",
    "This analysis provides valuable insights into how different factors have influenced energy consumption patterns across various postal code clusters, shedding light on the changing dynamics of electric vehicle charging behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "haPjMsSLG3dc"
   },
   "source": [
    "### **3.4.3. User-wise Energy Consumption over Time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mUuREwDtG4Au"
   },
   "source": [
    "Continuing our analysis, we now turn our attention to examining energy consumption trends among different user segments. This allows us to gain deeper insights into how the establishment of charging fees and the impact of COVID-19 lockdowns affected various user types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OOgvC2u58ymm"
   },
   "outputs": [],
   "source": [
    "# Group by \"User Segment\" and resample using pd.Grouper\n",
    "grouped = EV.groupby(['User Segment', pd.Grouper(key='Start Date', freq='W')])['Energy (kWh)'].sum().reset_index()\n",
    "\n",
    "# Define the desired order for user segments\n",
    "segment_order = ['Frequent', 'Occasional', 'Infrequent', 'Passing', 'Unknown']\n",
    "\n",
    "# Create a line plot for each User Segment in separate subplots\n",
    "fig, axes = plt.subplots(nrows=len(segment_order), figsize=(15, 15), sharex=True)\n",
    "\n",
    "for i, user_segment in enumerate(segment_order):\n",
    "    data = grouped[grouped['User Segment'] == user_segment]\n",
    "    ax = axes[i]\n",
    "    ax.plot(data['Start Date'], data['Energy (kWh)'], label=f'User Segment {user_segment}')\n",
    "    ax.set_title(f'Energy Consumption for User Segment {user_segment}')\n",
    "    ax.set_ylabel('Total Energy (kWh)')\n",
    "    ax.grid(linestyle='--')\n",
    "\n",
    "    # Add vertical lines at specific dates\n",
    "    ax.axvline(x=Fee, color='g', linestyle='--', linewidth=2, label='Charging Fee')\n",
    "    ax.axvline(x=Covid, color='r', linestyle='--', linewidth=2, label='Covid-19 Lockdown')\n",
    "    ax.legend()\n",
    "\n",
    "# Set a common x-axis label for the bottom subplot\n",
    "axes[-1].set_xlabel('Date')\n",
    "\n",
    "# Adjust spacing between subplots\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m3U1G43B6dvb"
   },
   "source": [
    "This insightful plot reveals how the establishment of charging fees had varying impacts on different user segments, shedding light on their behavior before and after this event.\n",
    "\n",
    "**1. Impact of Charging Fee Establishment:**\n",
    "\n",
    "* Frequent users experienced the most significant impact, with their energy consumption dropping by almost 50% after the introduction of charging fees. This sharp decline indicates that frequent users were highly affected by the fee establishment.\n",
    "* Occasional users also witnessed a similar impact, with their energy consumption showing a substantial decline.\n",
    "* In contrast, both infrequent and passing users didn't seem to be significantly affected by the fee establishment, suggesting that they prioritize charging convenience over fees.\n",
    "* The behavior of unknown user types closely resembled that of frequent and occasional users.\n",
    "\n",
    "**2. Behavior After Charging Fee Establishment:**\n",
    "\n",
    "* Frequent users did not recover from the fee establishment, and their consumption remained relatively stable afterward. This behavior aligns with the residents' stations in postal code 94303, suggesting that many frequent users are likely residents who shifted to private or other nearby charging options.\n",
    "* Other user types, including occasional, infrequent, and passing users, displayed a different trend, with a gradual increase in energy consumption after the fee establishment. They even reached or exceeded their previous all-time high consumption levels before the fee imposition.\n",
    "\n",
    "**3. Impact of COVID-19 Lockdown:**\n",
    "\n",
    "* The COVID-19 pandemic had a massive impact on all user segments, resulting in a significant drop in energy consumption. Unfortunately, our data only extends until the end of 2021, preventing us from fully assessing the recovery from this shock. Further data on subsequent events could provide more insights into the effects of COVID-19 and the recovery patterns of these user segments.\n",
    "\n",
    "This analysis offers valuable insights into how charging behavior among different user segments evolved over time in response to charging fees and external events such as the COVID-19 pandemic. It highlights the diverse impacts on user types and raises questions about future recovery patterns and behavior shifts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LEZ2d_YfHHB2"
   },
   "source": [
    "### **3.4.4. Consumption vs Duration over Time**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ash8TWwlLFsg"
   },
   "source": [
    "In this section, we delve into the analysis of average energy consumption and charging duration per charging event over time. Our primary focus is on the significant events in our dataset: the establishment of charging fees and the impact of the COVID-19 pandemic. We aim to understand how these events influenced the average behavior of users during charging events over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qC6XonMRCCCZ"
   },
   "outputs": [],
   "source": [
    "# Calculate the count of charging events per week\n",
    "event_count = EV_s.resample('W', on=\"Start Date\")[\"User ID\"].count()\n",
    "\n",
    "# Calculate the average energy consumption and average duration per event\n",
    "avg_energy= wev[\"Energy (kWh)\"]/event_count\n",
    "avg_duration= wev[\"Duration_min\"]/event_count\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax1 = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot the first set of data on the primary y-axis\n",
    "ax1.plot(avg_energy.index, avg_energy.values, label='Average Energy Usage', color='blue')\n",
    "ax1.set_xlabel(\"Year\")\n",
    "ax1.set_ylabel(\"Average Energy Consumption (kWh)\", color='blue')\n",
    "ax1.tick_params(axis='y', labelcolor='blue')\n",
    "ax1.yaxis.grid(False)\n",
    "ax1.xaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Create a secondary y-axis\n",
    "ax2 = ax1.twinx()\n",
    "\n",
    "# Plot the second set of data on the secondary y-axis\n",
    "ax2.plot(avg_duration.index, avg_duration.values, label='Average Duration', color='orange')\n",
    "ax2.set_ylabel(\"Average Duration (minutes)\", color='orange')\n",
    "ax2.tick_params(axis='y', labelcolor='orange')\n",
    "ax2.yaxis.grid(False)\n",
    "ax2.xaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add vertical lines at specific dates\n",
    "ax1.axvline(x=Fee, color='g', linestyle='--', linewidth=2, label='Charging Fee')\n",
    "ax1.axvline(x=Covid, color='r', linestyle='--', linewidth=2, label='COVID-19 Lockdown')\n",
    "ax1.legend()\n",
    "ax2.legend()\n",
    "\n",
    "# Add a title and legend\n",
    "plt.title(\"Average Energy Consumption (kWh) and Average Duration per Charging Event Over Time\")\n",
    "fig.tight_layout()  # Ensures proper spacing of the two y-axes\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eBqGD422_YVD"
   },
   "source": [
    "This plot provides insights into how major events, such as the introduction of charging fees and the COVID-19 lockdown, influenced the average energy consumption and duration per charging event over time.\n",
    "\n",
    "**1. Impact of Charging Fee Establishment:**\n",
    "\n",
    "* Following the establishment of charging fees, there was a noticeable decrease in charging durations, driven by the city's imposition of additional payments for vehicles that overstayed their charging events. Simultaneously, average energy consumption per event experienced a slight drop, primarily due to the shorter charging durations caused by the fee establishment.\n",
    "* Subsequently, we observe a gradual increase in energy consumption per event. Interestingly, charging durations remained relatively consistent over this period. This divergence could be attributed to technological improvements in both charging stations and electric vehicles, resulting in faster charging times.\n",
    "\n",
    "**2. Impact of COVID-19 Lockdown:**\n",
    "\n",
    "* The COVID-19 lockdowns had a discernible impact on both average charging durations and energy consumption per event. There are spikes in both metrics during the lockdown periods. This could be attributed to cautious user behavior during the crisis, with users opting to fully charge their vehicles during lockdowns when access to charging stations may have been limited.\n",
    "\n",
    "This analysis underscores the dynamic nature of user behavior during critical events and how it can impact key metrics such as energy consumption and charging duration per event. Further examination of post-lockdown trends and continued technological advancements may reveal additional insights into the evolving landscape of electric vehicle charging behavior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wBI7sOQRHSYm"
   },
   "source": [
    "### **3.4.5. Seasonal Decomposition Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CoM1bpNHsFuX"
   },
   "source": [
    "In this section, we delve into Seasonal Decomposition Analysis to gain deeper insights into the Energy Consumption of charging stations over time. We utilize the Prophet library, a powerful forecasting tool developed by Facebook. Seasonal Decomposition Analysis is a statistical technique that breaks down time series data into its fundamental components, including trend and seasonality. While there are other popular libraries like statsmodels for such analysis, we opt for Prophet due to its unique ability to account for special events like holidays and even define custom events like Covid-19 lockdowns, which can provide valuable insights into energy consumption patterns.\n",
    "\n",
    "We decompose our time series data into the following components:\n",
    "\n",
    "* **Trend Component:** This component reveals the long-term underlying pattern or direction in the data. It helps us discern whether energy consumption is increasing, decreasing, or remaining relatively stable over time.\n",
    "\n",
    "* **Seasonal Components:** These components enable us to identify recurring trends and seasonal patterns related to specific time periods or seasons.\n",
    "\n",
    "* **Holiday and Covid-19 Lockdown Effects:** Prophet offers the capability to incorporate holiday effects and Covid-19 effects into the analysis. This is particularly valuable, as energy consumption behavior can vary significantly during these events.\n",
    "\n",
    "While Prophet can perform forecasting tasks, our primary focus here is to extract trends and seasonality from our dataset for inspection. We will delve into more accurate forecasting models in the subsequent section 4 of this notebook.\n",
    "\n",
    "Before we embark on the analysis, we need to prepare our data. This involves resampling our dataset, which contains information about charging station ports (as discussed in Section 2, Data Preprocessing), at hourly intervals based on the \"Start Date\" column. We calculate both the number of unique active ports and the sum of energy consumption in those hourly intervals. Subsequently, we create a new dataframe using these variables and compute energy consumption per port in hourly intervals. This transformation ensures that energy consumption values are not affected by fluctuations in the number of active stations over time, providing us with a clearer picture of trends and seasonality. Lastly, we rename the columns to conform to Prophet's input requirements, with \"ds\" representing the Date and \"y\" representing the values we want to analyze."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "InRMXPFxTeD5"
   },
   "outputs": [],
   "source": [
    "# Calculate number of unique ports in Hourly intervals\n",
    "port= EV_s.resample(\"H\", on= \"Start Date\")[\"Station Name\"].nunique()\n",
    "\n",
    "# Calculate sum of Energy Consumption in Hourly intervals\n",
    "consumption= EV_s.resample(\"H\", on= \"Start Date\")[\"Energy (kWh)\"].sum()\n",
    "\n",
    "# Create a DataFrame for Consumption per Port\n",
    "cpp = pd.DataFrame({'Port': port, 'Consumption (kWh)': consumption})\n",
    "\n",
    "# Calculate the Energy Consumption per Charging Port\n",
    "cpp['y'] = cpp['Consumption (kWh)'] / cpp['Port']\n",
    "\n",
    "# Drop the 'Port' and 'Consumption (kWh)' columns\n",
    "cpp.drop(columns=['Port', 'Consumption (kWh)'], inplace=True)\n",
    "\n",
    "# Fill NaN values with 0 in the entire DataFrame\n",
    "cpp.fillna(0, inplace=True)\n",
    "\n",
    "# Reset the index to make 'Start Date' a regular column\n",
    "cpp.reset_index(inplace=True)\n",
    "\n",
    "# Rename the columns\n",
    "cpp = cpp.rename(columns={\"Start Date\": \"ds\"})\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(cpp.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nQ_ZUQQYMmtb"
   },
   "source": [
    "Now that our data is prepared, we need to confirm its stationarity, a fundamental assumption for many time series analysis techniques, including Prophet. Non-stationary data can lead to inaccurate modeling and unreliable forecasts. We employ the Augmented Dickey-Fuller (ADF) test from the statsmodels library to assess stationarity. If the p-value from this test is less than 0.05, we can conclude that our data is stationary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RO49Ab-6w7bL"
   },
   "outputs": [],
   "source": [
    "\n",
    "# Perform the Dickey-Fuller test for stationarity\n",
    "result = adfuller(cpp['y'])\n",
    "\n",
    "# Extract and print the test statistics and p-value\n",
    "adf_statistic = result[0]\n",
    "p_value = result[1]\n",
    "\n",
    "print(f'ADF Statistic: {adf_statistic}')\n",
    "print(f'p-value: {p_value}')\n",
    "\n",
    "# Interpret the results\n",
    "if p_value <= 0.05:\n",
    "    print('The time series is stationary (reject the null hypothesis)')\n",
    "else:\n",
    "    print('The time series is not stationary (fail to reject the null hypothesis)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0b2P57NePHbO"
   },
   "source": [
    "Upon analyzing the results, if the p-value is significantly low, it indicates high seasonality in our data, allowing us to proceed with our Prophet model.\n",
    "\n",
    "We selected Prophet for its ability to handle Covid-19 lockdowns and other special events. We define lockdown dates and ranges in our data, for a more comprehensive understanding of our approach, please refer to the relevant section in Prophet's documentation page [here](https://facebook.github.io/prophet/docs/handling_shocks.html). We also tend to analyze the impact of free and paid days. Our analysis extends to understanding the seasonality variations during weekends and weekdays. These distinct analyses will provide us with a comprehensive view of how seasonality patterns change under different conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s_dK-bQC-IyW"
   },
   "outputs": [],
   "source": [
    "# Define Lockdowns dates as a DataFrame\n",
    "lockdowns = pd.DataFrame([\n",
    "    {'holiday': 'lockdown_1', 'ds': '2020-03-21', 'lower_window': 0, 'ds_upper': '2020-06-06'},\n",
    "    {'holiday': 'lockdown_2', 'ds': '2020-07-09', 'lower_window': 0, 'ds_upper': '2020-10-27'},\n",
    "])\n",
    "\n",
    "# Converte date columns 'ds' and 'ds_upper' to datetime format\n",
    "for t_col in ['ds', 'ds_upper']:\n",
    "    lockdowns[t_col] = pd.to_datetime(lockdowns[t_col])\n",
    "\n",
    "# Calculate the duration of each lockdown in days and storing it in 'upper_window'\n",
    "lockdowns['upper_window'] = (lockdowns['ds_upper'] - lockdowns['ds']).dt.days\n",
    "\n",
    "# Define Charging Fee Start date as a Datetime\n",
    "Fee = pd.to_datetime(\"2017-08-01\")\n",
    "\n",
    "# Slice Data into Lockdowns/Weekends/Weekdays/Free/Paid\n",
    "def is_free(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    return (date < Fee)\n",
    "\n",
    "def is_weekend(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    return (date.weekday() == 5 or date.weekday() == 6)\n",
    "\n",
    "def is_lockdown(ds):\n",
    "    date = pd.to_datetime(ds)\n",
    "    for _, row in lockdowns.iterrows():\n",
    "        if row['ds'] <= date <= row['ds_upper']:\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "\n",
    "# Apply the defined functions to create new columns in 'cpp' DataFrame\n",
    "cpp['free'] = cpp['ds'].apply(is_free)\n",
    "cpp['paid'] = ~cpp['ds'].apply(is_free)\n",
    "cpp['weekday'] = ~cpp['ds'].apply(is_weekend)\n",
    "cpp['weekend'] = cpp['ds'].apply(is_weekend)\n",
    "cpp['lockdown'] = cpp['ds'].apply(is_lockdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Tl26oDdgRH5U"
   },
   "source": [
    "With our dataset prepared and special conditions defined, we are now ready to conduct Seasonal Decomposition Analysis using Prophet. We configure the model to incorporate daily seasonality for free days, paid days, weekdays, weekends, and lockdown days. Additionally, we include United States holidays in the model to capture their effects. Finally, we fit the model and visualize the seasonal decompositions using the plot_components function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mjvkR_2JIaR4"
   },
   "outputs": [],
   "source": [
    "# Create the Prophet model\n",
    "m = Prophet(holidays=lockdowns, daily_seasonality=False)\n",
    "\n",
    "# Add daily seasonality for different conditions\n",
    "m.add_seasonality(name='Hour of Free Days', period=1, fourier_order=4, condition_name='free')\n",
    "m.add_seasonality(name='Hour of Paid Days', period=1, fourier_order=4, condition_name='paid')\n",
    "m.add_seasonality(name='Hour of Weekdays', period=1, fourier_order=4, condition_name='weekday')\n",
    "m.add_seasonality(name='Hour of Weekends', period=1, fourier_order=4, condition_name='weekend')\n",
    "m.add_seasonality(name='Hour of Lockdown Days', period=1, fourier_order=4, condition_name='lockdown')\n",
    "\n",
    "# Add country holidays\n",
    "m.add_country_holidays(country_name='US')\n",
    "\n",
    "# Fit the model\n",
    "m = m.fit(cpp)\n",
    "\n",
    "# Plot seasonal decompositions\n",
    "fig= m.plot_components(m.predict(cpp))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEtcHQF9zD5D"
   },
   "source": [
    "The series of decomposition plots provide us with invaluable insights into the energy consumption patterns at charging stations over time:\n",
    "\n",
    "**1. Trend Plot:** Examining the trend plot, we observe a significant shift in the energy consumption trend. After the introduction of fees in 2017, the previously upward trend in consumption has transitioned into a more neutral pattern.\n",
    "\n",
    "**2. Holidays Plot:** The holidays plot showcases the impact of special days on consumption. During holidays, there is a clear decrease in consumption. Additionally, the defined lockdown periods, represented as holidays, exhibit a noticeable dip in consumption. However, it's worth noting that there are some high consumption values during these lockdowns, potentially attributed to cautious behavior, where users fully charge their vehicles in anticipation of restricted access to charging stations.\n",
    "\n",
    "**3. Yearly Seasonality Plot:** The yearly plot unveils distinct consumption patterns throughout the year. Consumption peaks shortly after the New Year's holidays. As the year progresses, consumption levels off. Notably, the months of July and August display lower consumption seasonality, which could be explained by educational institutions experiencing reduced activity, particularly with Stanford University's presence in the area. Consumption trends then rise, culminating in another peak during October. December stands out as the month with the lowest consumption, primarily driven by holidays and reduced activity.\n",
    "\n",
    "**4. Weekly Seasonality Plot:** The weekly plot underscores the substantial difference in consumption between weekdays and weekends. Consumption during weekdays significantly surpasses that of weekends, suggesting that the majority of users are daily commuters to the station areas. The reduced activity during weekends contributes to lower energy consumption.\n",
    "\n",
    "**5. Daily Seasonality Plots:** The remaining plots examine the intraday seasonality in various scenarios, with a particular focus on the contrasting daily patterns before and after the introduction of charging fees.\n",
    "\n",
    "* **Free vs. Paid Days Plots:** Notably, during free days, energy consumption experiences a rapid increase from the early hours of the morning, culminating in a peak in the afternoon around 18. In contrast, paid days exhibit a different consumption pattern. Here, we observe an early morning spike in consumption, typically around 7 AM, followed by a gradual decline after 10 AM. This lower consumption persists until around 20 before gradually decreasing as night falls. This shift in daily seasonality underscores the profound impact of charging fees, reshaping the peak consumption hours from the afternoon to the morning. Primarily, this transformation has affected residents who previously charged their vehicles at nearby stations after work hours. With the introduction of charging fees, these residents have shifted away from charging their vehicles in these stations, leaving predominantly commuting users who now charge their cars during their morning commutes.\n",
    "\n",
    "* **Weekdays vs. Weekends Plot:** This analysis accounts for all days, without differentiating between free and paid days. On weekdays, the peak consumption occurs around 7 AM, followed by a gradual decline throughout the day. A minor increase is observed around 20, although not as pronounced as the morning peak. Conversely, weekends display a peak around 7 AM, followed by slightly lower consumption levels, maintaining a neutral trend until another modest peak around 18, with consumption declining during the night. Overall, this comparison reveals that weekend behavior is more influenced by residents, while weekdays are primarily shaped by employees and daily commuters.\n",
    "\n",
    "* **Lockdown Days Plot:** Analyzing intraday seasonality during Covid-19 lockdowns, we notice a distinct pattern unlike any other daily behavior observed. There are peaks around 1-2 AM and 10 AM, with a gradual decrease throughout the day, reaching its lowest point around 9 PM. This unique pattern underscores the significant impact of daily restrictions during lockdowns, resulting in behavior distinctly different from regular days.\n",
    "\n",
    "In conclusion, the Seasonal Decomposition Analysis using Prophet has provided us with comprehensive insights into the energy consumption dynamics at charging stations. We've observed notable trends, seasonal patterns, and the influence of various factors such as holidays, fee implementation, and lockdowns on consumption behavior. These findings will be instrumental in informing strategic decisions and optimizing resource allocation in the charging station network.\n",
    "\n",
    "In the upcoming section, we will conduct a more in-depth behavioral analysis by examining these patterns among various user types and across different charging stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GgRI4TRFa4mw"
   },
   "source": [
    "## **3.5. Behavioral Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vvSKSt6ua56Y"
   },
   "source": [
    "In this concluding segment of our exploratory data analysis, we shift our focus towards behavioral analysis across various time frames, taking into account different user segments and charging stations. Building upon our earlier observation of similar patterns among stations located in shared postal codes, we will persist in segmenting stations by postal codes to ensure more precise conclusions from our analysis.\n",
    "\n",
    "Our analysis will commence with higher time frames, starting from a monthly breakdown and gradually progressing to weekly intervals, ultimately concluding with a daily perspective. We will gauge this behavior by quantifying event counts within these defined time intervals. Additionally, in the final subsection, we will delve into the analysis of daily energy consumption and charging duration behavior. With these objectives in mind, let us proceed with our comprehensive behavioral analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WQ7AtHTUa_s8"
   },
   "source": [
    "### **3.5.1. Monthly Charging Behavior**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0YE7-ixVbBKd"
   },
   "source": [
    "To initiate our monthly behavioral analysis, the initial step involves data preparation. To achieve this, we begin by extracting the month from the \"Start Date\" column and subsequently calculating the total number of months within our dataset. Next, we perform a grouping operation on our EV dataset, categorizing it by both month and user segments. This categorized dataset, denoted as \"dfmonth,\" serves as the foundation for both the total events plot and user segments plot. Within this dataset, we utilize the previously computed \"total_months\" values to normalize the monthly events. This normalization process involves dividing the monthly event counts by the total number of months, resulting in the average events per month value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHCkCRMBfHwx"
   },
   "outputs": [],
   "source": [
    "# Extract Month from Date\n",
    "EV[\"Month\"]= EV[\"Start Date\"].dt.month\n",
    "\n",
    "# Calculate the total months over the dataset\n",
    "total_months = len(EV['Start Date'].dt.to_period('Y').dt.to_timestamp().dt.date.unique())\n",
    "\n",
    "# Group the data by Month and User Segments, and count the occurrences\n",
    "dfmonth= EV.groupby([\"Month\", \"User Segment\"]).size().reset_index(name='Monthly Events')\n",
    "\n",
    "# Calculate and add a new column for Events per Month by dividing Monthly Events by total_months\n",
    "dfmonth[\"Events per Month\"]= dfmonth[\"Monthly Events\"]/ total_months\n",
    "\n",
    "# Display the first few rows of the resulting DataFrame\n",
    "dfmonth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oqGmfapK8WPB"
   },
   "source": [
    "With our dataset prepared, we can proceed to create a bar plot that visualizes the average monthly events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKCMWp2Gs3GN"
   },
   "outputs": [],
   "source": [
    "# Calculate the total events per month and store it in the 'total_m' DataFrame\n",
    "total_m = dfmonth.groupby(\"Month\")[\"Events per Month\"].sum().reset_index(name='Total M')\n",
    "\n",
    "# Set Seaborn style\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a bar plot with Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=\"Month\", y=\"Total M\", data=total_m, color='dodgerblue')\n",
    "\n",
    "# Adding a horizontal line as an average line (for example)\n",
    "average_value = total_m[\"Total M\"].mean()\n",
    "plt.axhline(y=average_value, color='red', linestyle='--', label='Average', linewidth=2)\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Monthly Charging Behavior\", fontsize=16)\n",
    "plt.xlabel(\"Month of Year\", fontsize=12)\n",
    "plt.ylabel(\"Average Events per Month\", fontsize=12)\n",
    "\n",
    "# Add data labels on top of the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height()):}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# Set the positions and labels for each month on the x-axis\n",
    "month_positions = range(12)\n",
    "short_month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "plt.xticks(month_positions, short_month_names)\n",
    "\n",
    "# Adjust the plot layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dh6FSDHh-Deh"
   },
   "source": [
    "The bar plot provides insights into monthly consumption patterns. Notably, January and October stand out as months with the highest event records, while April and December exhibit the lowest event levels. These variations in events number can be attributed to the influence of educational activity, with certain months experiencing higher or lower charging demand. However, for a comprehensive understanding of monthly behavior, further analysis involving spatial and point-of-interest (POI) data around charging stations is required. Moving forward, we will delve into a detailed analysis of user-wise monthly behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rpy9PIPD87Dt"
   },
   "outputs": [],
   "source": [
    "# Get unique values in the \"User Segments\" column\n",
    "unique_segments = dfmonth[\"User Segment\"].unique()\n",
    "\n",
    "# Create a custom palette with the same number of colors as unique segments\n",
    "custom_palette = sns.color_palette(\"Set2\", len(unique_segments))\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Use a lineplot with separate lines for each station\n",
    "sns.lineplot(data=dfmonth, x=\"Month\", y=\"Events per Month\", hue=\"User Segment\", palette=custom_palette)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set(xlabel=\"Month of the Year\", title=\"Monthly Charging Behavior by User Segment\")\n",
    "plt.xlabel('')\n",
    "plt.ylabel(\"Average Events per Month\")\n",
    "\n",
    "# Set the positions and labels for each month on the x-axis\n",
    "month_positions = range(13)\n",
    "short_month_names = ['', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "plt.xticks(month_positions, short_month_names)\n",
    "\n",
    "# Customize gridlines for both x and y axes\n",
    "ax.grid(axis='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(title=\"User Segment\", title_fontsize=12, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0l3UtDC_B2H"
   },
   "source": [
    "Upon conducting a user-wise analysis, intriguing patterns emerge. Notably, there is a striking similarity in behavior between frequent and occasional users. However, it's worth noting that frequent users exhibit a peak in March, whereas occasional users' peak occurs in October. The remaining user segments display relatively stable behvioral patterns, indicating that their charging habits remain unaffected by the activities surrounding the stations, and their primary objective is simply to charge their vehicles. Interestingly, the first two segments experience a decline in events during December, attributed to holidays and reduced activity. In contrast, the other three segments maintain a slightly positive trend or remain stable during the same period."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3x5AYUQv1P65"
   },
   "outputs": [],
   "source": [
    "# Group the data by Month and Postal Code, and count the occurrences\n",
    "stationmonth= EV.groupby([\"Month\", \"Postal Code\"]).size().reset_index(name='Monthly Events')\n",
    "\n",
    "# Calculate and add a new column for Events per Month by dividing Monthly Events by total_months\n",
    "stationmonth[\"Events per Month\"]= stationmonth[\"Monthly Events\"]/ total_months\n",
    "\n",
    "# Get unique values in the \"Postal Code\" column\n",
    "unique_stations = stationmonth[\"Postal Code\"].unique()\n",
    "\n",
    "# Create a custom palette with the same number of colors as unique segments\n",
    "custom_palette = sns.color_palette(\"Set2\", len(unique_stations))\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Use a lineplot with separate lines for each station\n",
    "sns.lineplot(data=stationmonth, x=\"Month\", y=\"Events per Month\", hue=\"Postal Code\", palette=custom_palette)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set(xlabel=\"Month of the Year\", title=\"Monthly Charging Behavior by Stations' Postal Code\")\n",
    "plt.xlabel('')\n",
    "plt.ylabel(\"Average Events per Month\")\n",
    "\n",
    "# Set the positions and labels for each month on the x-axis\n",
    "month_positions = range(13)\n",
    "short_month_names = ['', 'Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', 'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "plt.xticks(month_positions, short_month_names)\n",
    "\n",
    "# Customize gridlines for both x and y axes\n",
    "ax.grid(axis='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(title=\"Postal Code\", title_fontsize=12, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgeoypAI-4-T"
   },
   "source": [
    "The postal code-wise monthly behavior analysis reveals a certain degree of similarity between postal codes 94301 and 94306 in terms of event peaks and lows. Both of these postal codes exhibit high event occurrence in January and October while experiencing lower events in April. Notably, the stations located in 94301, which contribute significantly to the overall events share among all stations, demonstrate more pronounced fluctuations throughout the year when compared to the stations in 94306. Conversely, the stations in postal code 94303 display distinct behavior, with peak events occurring in March and May, and the lowest events observed in August. These variations underscore the influence of different user classes, e.g. Residents vs. Commuters, on the monthly behavior patterns of these stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YBCDlf9J-2_-"
   },
   "source": [
    "### **3.5.2. Weekly Charging Behavior**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eUN3O0-W-3AI"
   },
   "source": [
    "In this section, we will continue our behavioral analysis using a weekly dataframe, focusing on understanding how behavior evolves throughout the weekdays across various user segments and different stations. Our approach will involve similar data preparation tasks as those employed in the monthly behavior analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xUTIvwBR-3AI"
   },
   "outputs": [],
   "source": [
    "# Extract Weekday from Date\n",
    "EV[\"Weekday\"]= EV[\"Start Date\"].dt.weekday\n",
    "\n",
    "# Calculate the total weeks over the dataset\n",
    "total_weeks = len(EV['Start Date'].dt.to_period('W').dt.to_timestamp().dt.date.unique())\n",
    "\n",
    "# Group the data by Weekday and User Segments, then count the occurrences\n",
    "dfweek= EV.groupby([\"Weekday\", \"User Segment\"]).size().reset_index(name='Weekday Events')\n",
    "\n",
    "# Calculate and add a new column for Events per Weekday by dividing Weekday Events by total_weeks\n",
    "dfweek[\"Events per Weekday\"]= dfweek[\"Weekday Events\"]/ total_weeks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cmhS0KZYFQBH"
   },
   "source": [
    "With the dfweek DataFrame ready, we can proceed to create the desired visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uliSyEnw-3AJ"
   },
   "outputs": [],
   "source": [
    "# Group the data by Weekday and calculate the sum of Events per Weekday\n",
    "total_w= dfweek.groupby(\"Weekday\")[\"Events per Weekday\"].sum().reset_index(name='Total W')\n",
    "\n",
    "# Set Seaborn style (optional)\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Create a bar plot with Seaborn\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=\"Weekday\", y=\"Total W\", data=total_w, color='dodgerblue')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Weekly Charging Behavior\", fontsize=16)\n",
    "plt.xlabel(\"Day of Week\", fontsize=12)\n",
    "plt.ylabel(\"Average Events per Week\", fontsize=12)\n",
    "\n",
    "# Add data labels on top of the bars\n",
    "for p in ax.patches:\n",
    "    ax.annotate(f'{int(p.get_height()):}', (p.get_x() + p.get_width() / 2., p.get_height()), ha='center', va='center', fontsize=10, color='black', xytext=(0, 5), textcoords='offset points')\n",
    "\n",
    "# Set the positions and labels for each weekday on the x-axis\n",
    "weekday_positions = range(7)\n",
    "weekday_order= ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "plt.xticks(weekday_positions, weekday_order)\n",
    "\n",
    "# Add a horizontal line for the average\n",
    "average_value = total_w[\"Total W\"].mean()\n",
    "plt.axhline(average_value, color='red', linestyle='--', label=f'Average ({average_value:.2f})')\n",
    "\n",
    "# Adjust the plot layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POvBok-0F8iB"
   },
   "source": [
    "As observed in the plot, weekends consistently exhibit the lowest number of events, indicating decreased activity during this time. On weekdays, Mondays show a slightly lower event count compared to other weekdays, but overall, weekdays exhibit a stable level of consumption."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fllm8rYH-3AK"
   },
   "outputs": [],
   "source": [
    "# Get unique values in the \"User Segment\" column\n",
    "unique_segments = dfweek[\"User Segment\"].unique()\n",
    "\n",
    "# Create a custom palette with the same number of colors as unique segments\n",
    "custom_palette = sns.color_palette(\"Set2\", len(unique_segments))\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Use a lineplot with separate lines for each station\n",
    "sns.lineplot(data=dfweek, x=\"Weekday\", y=\"Events per Weekday\", hue=\"User Segment\", palette=custom_palette)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set(xlabel=\"Day of Week\", title=\"Weekly Charging Behavior by User Segment\")\n",
    "plt.xlabel('')\n",
    "plt.ylabel(\"Average Events per Weekday\")\n",
    "\n",
    "# Set the positions and labels for each weekday on the x-axis\n",
    "weekday_positions = range(7)\n",
    "weekday_order= ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "plt.xticks(weekday_positions, weekday_order)\n",
    "\n",
    "# Customize gridlines for both x and y axes\n",
    "ax.grid(axis='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(title=\"User Segment\", title_fontsize=12, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r7BBlpsuHmla"
   },
   "source": [
    "By analyzing weekly behavior among user types, we gain valuable insights. Frequent users exhibit a pattern consistent with the overall data observed in the previous section, showing lower event activity on weekends and higher events on weekdays. However, occasional users deviate from this pattern; their event activity remains relatively steady during the week, with a slight increase in events on Fridays. In contrast, Infrequent and Passing users display a distinct behavior, with their peak event activity occurring during the weekends. The behavior of the Unknown segment remains consistent throughout the week, providing limited insights.\n",
    "\n",
    "These varying behaviors suggest that frequent users are likely commuters who have reduced activity during weekends, whereas Infrequent and Passing users have higher activity levels during the weekends, possibly indicating leisure or recreational vehicle usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lIjWVSs-3AK"
   },
   "outputs": [],
   "source": [
    "# Group the data by weekday and Postal Code, and calculate the total events for each combination\n",
    "stationweek= EV.groupby([\"Weekday\", \"Postal Code\"]).size().reset_index(name='Weekday Events')\n",
    "\n",
    "# Calculate the average events per weekday by dividing the total events by the total number of weeks\n",
    "stationweek[\"Events per Weekday\"]= stationweek[\"Weekday Events\"]/ total_weeks\n",
    "\n",
    "# Get unique values in the \"Postal Code\" column\n",
    "unique_postalcodes = stationweek[\"Postal Code\"].unique()\n",
    "\n",
    "# Create a custom palette with the same number of colors as unique segments\n",
    "custom_palette = sns.color_palette(\"Set2\", len(unique_postalcodes))\n",
    "\n",
    "# Create the plot\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Use a lineplot with separate lines for each Postal Code\n",
    "sns.lineplot(data=stationweek, x=\"Weekday\", y=\"Events per Weekday\", hue=\"Postal Code\", palette=custom_palette)\n",
    "\n",
    "# Set labels and title\n",
    "ax.set(xlabel=\"Day of the Week\", title=\"Weekly Charging Behavior by Stations' Postal Code\")\n",
    "plt.xlabel('')\n",
    "plt.ylabel(\"Average Events per Weekday\")\n",
    "\n",
    "# Set the positions and labels for each weekday on the x-axis\n",
    "weekday_positions = range(7)\n",
    "weekday_order= ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "plt.xticks(weekday_positions, weekday_order)\n",
    "\n",
    "# Customize gridlines for both x and y axes\n",
    "ax.grid(axis='both', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Add a legend\n",
    "plt.legend(title=\"Postal Code\", title_fontsize=12, bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gxldOk0NHr_T"
   },
   "source": [
    "Analyzing weekly charging behavior based on stations' postal codes provides valuable insights. Postal code 94301, which hosts the highest number of events, exhibits lower activity during weekends. This pattern suggests a strong association with frequent users, implying that the charging stations in this area are primarily influenced by this user segment.\n",
    "\n",
    "In contrast, stations in postal code 94303, strategically located near residential areas, maintain relatively consistent event levels throughout the week, with a slight increase on weekends. This trend hints at a connection with infrequent users, possibly due to recreational activities in the vicinity during weekends.\n",
    "\n",
    "Lastly, stations in postal code 94306 maintain a steady behavior throughout the week, with a significant drop in events on Saturdays. However, what distinguishes this area from 94301 is the higher event occurrence on Sundays, possibly influenced by the surrounding land uses.\n",
    "\n",
    "While these monthly and weekly analyses have provided valuable insights into user and station behaviors, a comprehensive spatial analysis would benefit from additional data on the city's land uses, demographics, points of interest, traffic patterns, and other relevant information. Such data would enable a more thorough investigation into the factors driving these behavioral variations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ue0rtcK1U1WK"
   },
   "source": [
    "### **3.5.3. Daily Charging Behavior**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "em8-slt7jBVy"
   },
   "source": [
    "When conducting a daily charging behavior analysis, it is essential to examine both the start time and end time to gain a comprehensive understanding of daily charging activity patterns. To achieve this, I have opted to resample the data at 30-minute intervals. This resampling approach allows us to gain a more detailed insight into the day's charging behaviors.\n",
    "\n",
    "To accomplish this, I resampled the Start Date and End Date columns into 30-minute intervals, counting the number of events for each time interval. This information was then organized into a new dataset. Subsequently, I grouped this new dataset by time using .index.time and computed the mean for each half-hour interval. This process provided us with the average daily events per 30-minute intervals, offering a more nuanced perspective on charging activity throughout the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1n173n5a_wvp"
   },
   "outputs": [],
   "source": [
    "# Resample the charging events based on the start time into 30-minute intervals and calculate the mean events for each interval\n",
    "hh_start_ev= EV.resample(\"30min\", on= \"Start Date\").size()\n",
    "hh_start_gp = hh_start_ev.groupby(hh_start_ev.index.time).mean()\n",
    "\n",
    "# Resample the charging events based on the end time into 30-minute intervals and calculate the mean events for each interval\n",
    "hh_end_ev= EV.resample(\"30min\", on= \"End Date\").size()\n",
    "hh_end_gp= hh_end_ev.groupby(hh_end_ev.index.time).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mPh5wfNAQ_wn"
   },
   "source": [
    "Now that we have our start and end times data organized into half-hour intervals, we'll create separate visualizations for both the start times and end times. Additionally, we'll generate a combined plot that showcases both the plug-in and plug-out activities, which will serve as our standard visualization for this section, offering a comprehensive view of both aspects of the charging behavior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EOmb6hayR-gq"
   },
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(3, 1, figsize=(10, 9))\n",
    "\n",
    "# Set the default style for plot\n",
    "plt.style.use('default')\n",
    "\n",
    "# Plot interval_avg on the first subplot\n",
    "hh_start_gp.plot(kind='bar', color='steelblue', ax=ax1, label='Plug-in Event')\n",
    "ax1.set_xlabel('Time of the Day')\n",
    "ax1.set_ylabel('Average Number of Events')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plot hh_end_gp on the second subplot\n",
    "hh_end_gp.plot(kind='bar', color='indianred', ax=ax2, label='Plug-out Event')\n",
    "ax2.set_xlabel('Time of the Day')\n",
    "ax2.set_ylabel('Average Number of Events')\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "# Create the mixed plot on the third subplot\n",
    "hh_start_gp.plot(kind='bar', color='steelblue', alpha=0.5, label='Plug-in Event', ax=ax3)\n",
    "hh_end_gp.plot(kind='bar', color='indianred', alpha=0.5, label='Plug-out Event', ax=ax3)\n",
    "ax3.set_xlabel('Time of the Day')\n",
    "ax3.set_ylabel('Average Number of Events')\n",
    "ax3.legend(loc='upper left')\n",
    "\n",
    "# Add gridlines for the y-axis\n",
    "ax1.yaxis.grid(True, linestyle='dotted', alpha=0.5)\n",
    "ax2.yaxis.grid(True, linestyle='dotted', alpha=0.5)\n",
    "ax3.yaxis.grid(True, linestyle='dotted', alpha=0.5)\n",
    "\n",
    "# Add a common title for the entire figure\n",
    "plt.suptitle('Daily Plug-in / Plug-out Behavior', fontsize=15)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqzHGRix6u2t"
   },
   "source": [
    "In the presented plot, we can discern the daily charging behavior for plug-in events. It initiates its ascent during the early morning hours, maintaining a steady pace from 8 to 11 before reaching its zenith at 11:30. Subsequently, there is a gradual decline until 16, followed by another upturn, peaking at 18, and gradually tapering off until the following morning. This intricate pattern can be attributed to two distinct user groups: morning commuters who charge their vehicles before or during work, and afternoon residents who recharge their cars in proximity to their residential areas, also we might need to consider afternoon commuters and passing users with a longer drive pattern which may need to charge their vehicles.\n",
    "\n",
    "In contrast, for plug-out events, we observe a different trend. The uptick in activity commences with the start of working hours, yet the peak occurs approximately three hours later, around noon. Afterward, there is a gentle decline with a slight resurgence around 17-18, followed by a more pronounced downturn after 20.\n",
    "\n",
    "In summary, our exploration of daily charging behavior has unveiled intriguing patterns in both plug-in and plug-out events, showcasing the influence of commuters and residents on these dynamics. To gain a deeper understanding of this behavior, we will further dissect it across different user segments and stations. In our daily analysis, which we have detailed on 30-minute intervals, we will shift our focus to station-specific analysis rather than grouping them by postal codes. This adjustment will provide us with more detailed insights into the charging behavior at individual stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YnvEnDzPKpC8"
   },
   "outputs": [],
   "source": [
    "# Group by 'User Segments' and resample by 30 minutes on 'Start Date', then calculate the size\n",
    "hh_start_user= EV.groupby(\"User Segment\").resample(\"30min\", on= \"Start Date\").size().reset_index(name='user_count')\n",
    "\n",
    "# Group by 'User Segments' and 'Start Date' by using the .dt accessor to extract the time component and calculate the mean of user_count\n",
    "hh_start_user_gp = hh_start_user.groupby([\"User Segment\", hh_start_user[\"Start Date\"].dt.time])[\"user_count\"].mean().reset_index(name='Plug-in')\n",
    "\n",
    "# Do the same operations for 'End Date' column\n",
    "hh_end_user= EV.groupby(\"User Segment\").resample(\"30min\", on= \"End Date\").size().reset_index(name='user_count')\n",
    "hh_end_user_gp = hh_end_user.groupby([\"User Segment\", hh_end_user[\"End Date\"].dt.time])[\"user_count\"].mean().reset_index(name='Plug-out')\n",
    "\n",
    "# Define fig and axes\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 10))\n",
    "\n",
    "# Define plot style\n",
    "plt.style.use('default')\n",
    "\n",
    "# Define the order of the User Segments\n",
    "segment_order = [\"Frequent\", \"Occasional\", \"Infrequent\", \"Passing\", \"Unknown\"]\n",
    "\n",
    "# Label the xaxis by half hour intervals\n",
    "x_labels = [f'{hour:02d}:{minute:02d}' for hour in range(24) for minute in (0, 30)]\n",
    "\n",
    "# Enomerate over segments and generate the plot\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(segment_order):\n",
    "        segment = segment_order[i]\n",
    "        start_data = hh_start_user_gp[hh_start_user_gp[\"User Segment\"] == segment]\n",
    "        start_data.plot(kind='bar', color='steelblue', alpha=0.5, label='Plug-in Event', ax=ax)\n",
    "        end_data = hh_end_user_gp[hh_end_user_gp[\"User Segment\"] == segment]\n",
    "        end_data.plot(kind='bar', color='indianred', alpha=0.5, label='Plug-out Event', ax=ax)\n",
    "        ax.set(xlabel=\"Time of the Day\", xticks=range(48), xticklabels=x_labels, ylabel=\"Average Number of Events\", title=segment)\n",
    "        ax.yaxis.grid(True, linestyle='dotted', alpha=0.5)\n",
    "        ax.legend(loc='upper left')\n",
    "\n",
    "# Add a common title for the entire figure and show the plot\n",
    "plt.suptitle('Daily Plug-in / Plug-out Behavior by User Segment', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jx9WRRtQOn2R"
   },
   "source": [
    "When examining daily charging behavior across various user segments, distinct patterns emerge. Frequent users exhibit a prominent uptrend beginning in the early morning, with peak activity around 8 AM, followed by the highest number of plug-out events occurring at noon around 12 PM. Activity gradually diminishes throughout the day, only to start anew the next day. On the other hand, occasional users display a delayed start in plugin events, with peaks around 12 PM and 18 PM, while plug-out events peak around 13 PM, maintaining a relatively steady trend with a slight decrease until 21 PM, followed by a steeper decline.\n",
    "\n",
    "Infrequent, Passing, and Unknown user segments share similar charging behavior. Plugin activity in these segments showcases two peaks around 12 PM and 18 PM, with increased afternoon activity. Correspondingly, plug-out activities follow a comparable pattern with peak hours around 13-14 PM and 20-21 PM.\n",
    "\n",
    "In summary, distinct user groups exhibit varying daily charging behavior patterns, with frequent users showing an early peak and occasional users demonstrating a more delayed but dual-peak trend. Meanwhile, Infrequent, Passing, and Unknown users share similar behavior characterized by dual peaks for both plug-in and plug-out activities.\n",
    "\n",
    "Now, let's delve into our daily behavioral analysis by examining individual stations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qV6xTSniPLMl"
   },
   "outputs": [],
   "source": [
    "# Group by 'Station Name' and resample by 30 minutes on 'Start Date', then calculate the size\n",
    "hh_start_station= EV.groupby(\"Station Name\").resample(\"30min\", on= \"Start Date\").size().reset_index(name='user_count')\n",
    "\n",
    "# Group by 'Staton Name' and 'Start Date' by using the .dt accessor to extract the time component and calculate the mean of user_count\n",
    "hh_start_station_gp = hh_start_station.groupby([\"Station Name\", hh_start_station[\"Start Date\"].dt.time])[\"user_count\"].mean().reset_index(name='Plug-in')\n",
    "\n",
    "# Do the same operations for 'End Date' column\n",
    "hh_end_station= EV.groupby(\"Station Name\").resample(\"30min\", on= \"End Date\").size().reset_index(name='user_count')\n",
    "hh_end_station_gp = hh_end_station.groupby([\"Station Name\", hh_end_station[\"End Date\"].dt.time])[\"user_count\"].mean().reset_index(name='Plug-out')\n",
    "\n",
    "# Define fig and axes\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(20, 10))\n",
    "\n",
    "# Define plot style\n",
    "plt.style.use('default')\n",
    "\n",
    "# Define the Station Names list\n",
    "stations = EV[\"Station Name\"].unique()\n",
    "\n",
    "# Label the xaxis by half hour intervals\n",
    "x_labels = [f'{hour:02d}:{minute:02d}' for hour in range(24) for minute in (0, 30)]\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(stations):\n",
    "        station = stations[i]\n",
    "        start_data = hh_start_station_gp[hh_start_station_gp[\"Station Name\"] == station]\n",
    "        start_data.plot(kind='bar', color='steelblue', alpha=0.5, label='Plug-in Event', ax=ax)\n",
    "        end_data = hh_end_station_gp[hh_end_station_gp[\"Station Name\"] == station]\n",
    "        end_data.plot(kind='bar', color='indianred', alpha=0.5, label='Plug-out Event', ax=ax)\n",
    "        ax.set(xlabel=\"Time of the Day\", xticks=range(48), xticklabels=x_labels, ylabel=\"Average Number of Events\", title=station)\n",
    "        ax.yaxis.grid(True, linestyle='dotted', alpha=0.5)\n",
    "        ax.legend(loc='upper left')\n",
    "\n",
    "# Add a common title for the entire figure\n",
    "plt.suptitle('Daily Plug-in / Plug-out Behavior by Charging Station', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SVO9Lp_Rfw4H"
   },
   "source": [
    "Through an analysis of the daily plug-in and plug-out behaviors at various stations, we can discern notable distinctions in station behavior. There are unique spikes in plug-in events during the early morning hours, particularly around 6 AM, at stations such as Hamilton, Cambridge, and MPL, which exhibit the earliest activity of the day. Stations like High, Ted Thompson, and Bryant follow suit with spikes around 9-10 AM. Stations like Cambridge, Webster, and Sherman experience peak activity around 11-12 AM, while stations like MPL and Rinconda Lib show heightened activity in the afternoon.\n",
    "\n",
    "By scrutinizing the daily behaviors of these stations and considering the surrounding land use and points of interest data, we could gain even more granular insights into how these stations operate throughout the day."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n1tOBEh6o_kb"
   },
   "source": [
    "### **3.5.4. Daily Charging Duration and Energy Consumption Behavior**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BdrqhD_6hUQK"
   },
   "source": [
    "In this section, we will delve into the daily behavior of two key variables. First, we will examine how the duration of charging events varies with respect to their start times throughout the day. Subsequently, we will explore the energy consumption patterns of these events to identify the hours with the highest and lowest consumption levels within our dataset.\n",
    "\n",
    "For our initial analysis, we will resample our EV dataset at 30-minute intervals based on the start date and compute the mean duration of events that commenced during each specific 30-minute window. We will then aggregate this resampled data into time intervals, calculate the mean duration for these intervals across the dataset, and finally present these findings in the form of a bar plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hSP6Cp8f2J2d"
   },
   "outputs": [],
   "source": [
    "# Resample the charging events based on the start time into 30-minute intervals and calculate the mean duration for each interval\n",
    "hh_duration = EV.resample(\"30min\", on= \"Start Date\")[\"Duration_min\"].mean()\n",
    "\n",
    "# Group by 30-minute intervals and calculate the mean of Duration_min\n",
    "hh_duration_gp = hh_duration.groupby(hh_duration.index.time).mean()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Set the default style for plot\n",
    "plt.style.use('default')\n",
    "\n",
    "# Plot hh_duration_gp\n",
    "hh_duration_gp.plot(kind='bar', color='steelblue')\n",
    "plt.xlabel('Time of the Day')\n",
    "plt.ylabel('Average Charging Duration')\n",
    "\n",
    "# Add gridlines for the y-axis\n",
    "plt.gca().yaxis.grid(True, linestyle='dotted', alpha=0.5)  # Corrected line\n",
    "\n",
    "# Add a title for the plot\n",
    "plt.title('Daily Charging Duration Behavior by Plug-in Time', fontsize=15)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46kUguivhZWO"
   },
   "source": [
    "The charging duration for plug-in events displays a distinct pattern throughout the day. On average, it hovers around 2 hours during the daytime. However, as evening sets in, the duration tends to extend. Notably, after 19:00, it experiences a steady increase, reaching its zenith between 22:00 and 01:00 midnight, where the average duration exceeds 5 hours. This trend is indicative of users who leave their vehicles charging overnight, ensuring a full charge for the morning. Subsequently, after these peak hours, the duration gradually diminishes, reaching its lowest point around 05:00 in the morning. Throughout the rest of the day, these durations oscillate between 100 to 200 minutes until the cycle repeats itself."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RIg3_k6BBaTg"
   },
   "source": [
    "To analyze energy consumption behavior throughout the day, we'll first resample the EV dataset into 30-minute intervals based on both the start date and end date. Within each interval, we'll calculate the sum of energy consumption. Then, we'll group these results by time intervals and calculate the mean consumption for each interval. With these average consumption values for 30-minute intervals based on both start and end times, we'll further average them to obtain more accurate results for each interval's energy consumption.\n",
    "\n",
    "Finally, we'll employ a heatmap to visualize the patterns of high and low energy consumption throughout the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6gbJVrli5NT4"
   },
   "outputs": [],
   "source": [
    "# Calculate the sum of energy consumption for start times in 30-minute intervals and obtain the mean for each interval\n",
    "en_start_ev = EV.resample(\"30min\", on=\"Start Date\")[\"Energy (kWh)\"].sum()\n",
    "en_start_gp = en_start_ev.groupby(en_start_ev.index.time).mean()\n",
    "\n",
    "# Calculate the sum of energy consumption for end times in 30-minute intervals and obtain the mean for each interval\n",
    "en_end_ev = EV.resample(\"30min\", on=\"End Date\")[\"Energy (kWh)\"].sum()\n",
    "en_end_gp = en_end_ev.groupby(en_end_ev.index.time).mean()\n",
    "\n",
    "# Merge the two DataFrames on the time index\n",
    "merged_df = pd.merge(en_start_gp, en_end_gp, left_index=True, right_index=True, suffixes=('_start', '_end'))\n",
    "\n",
    "# Combine the mean of Start and End to represent the approximate consumption in one column\n",
    "merged_df[\"Approximate Energy Consumption (kWh)\"] = (merged_df[\"Energy (kWh)_start\"] + merged_df[\"Energy (kWh)_end\"]) / 2\n",
    "merged_df.drop(merged_df[[\"Energy (kWh)_start\", \"Energy (kWh)_end\"]], axis=1, inplace=True)\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set(font_scale=1.2)  # Adjust the font size\n",
    "\n",
    "# Customize the color map (colormap of your choice)\n",
    "cmap = sns.color_palette(\"coolwarm\", as_cmap=True)\n",
    "\n",
    "# Create the heatmap\n",
    "sns.heatmap(merged_df, cmap=cmap)\n",
    "plt.title('Average Daily Energy Consumption Heatmap')\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oVh9LF0mpkVS"
   },
   "source": [
    "By examining the heatmap, we can discern distinct patterns in daily energy consumption. Consumption levels begin to rise in the early morning, reaching a peak around 11-12, after which they remain relatively high but with slightly reduced values until approximately 14. Subsequently, consumption gradually decreases until it starts to rise again from 17 to 19, driven by afternoon user activity. Finally, it steadily decreases until the next day's cycle commences."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PFbaUfiO7CJC"
   },
   "source": [
    "> In conclusion, our analysis of charging behavior across various time intervalsâ€”monthly, weekly, and dailyâ€”has provided valuable insights into the habits of electric vehicle users. We've observed fluctuations in activity related to factors such as user types, stations, and time of day. Frequent commuters exhibit different behavior from occasional users, and various postal code areas experience distinct charging patterns. Additionally, daily charging behaviors, including plug-in and plug-out durations and energy consumption, showcase intriguing trends throughout the day. These findings highlight the need for further investigation into the underlying reasons for these variations, possibly by incorporating additional data on land use, demographics, points of interest, and traffic patterns. Such comprehensive analyses can aid in optimizing charging station operations and services, ultimately promoting the widespread adoption of electric vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dnTHORKxe3Ym"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-QtBIxZe4Hc"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j3uZrDde9pa"
   },
   "source": [
    "# **4. Analyzing the Impact of Policy Intervention (Fee Policy) and Station Characteristics on the Station Utilization Rates**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeDpXpHhZMwo"
   },
   "source": [
    "## **4.1. Data Acquisition, Cleaning, and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uqPp0mNaZMwo"
   },
   "outputs": [],
   "source": [
    "# Check the python version of this notebook\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jC2EXM3VZMwo"
   },
   "outputs": [],
   "source": [
    "# Install and Import necessary libraries\n",
    "# Data processing tasks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Colab settings\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "# Visualization tasks\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import MarkerCluster\n",
    "from branca.colormap import linear\n",
    "!pip install keplergl\n",
    "from keplergl import KeplerGl\n",
    "\n",
    "# Geocoding tasks\n",
    "!pip install pgeocode\n",
    "import pgeocode\n",
    "\n",
    "# Time-series tasks\n",
    "import datetime as dt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from prophet import Prophet\n",
    "!pip install pandas holidays\n",
    "import holidays\n",
    "import calendar\n",
    "\n",
    "\n",
    "# Machine learning tasks\n",
    "!pip install catboost\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "# Deep learning tasks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "!pip install optuna\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hoH1mbF7ZMwp"
   },
   "outputs": [],
   "source": [
    "# Reading the Electric Vehicle (EV) charging station dataset from a remote CSV file\n",
    "# The 'low_memory' parameter is set to False to ensure efficient memory usage during loading\n",
    "EV=pd.read_csv(\"https://data.cityofpaloalto.org/datasets/194693-electric-vehicle-charging-station-usage-july-2011-dec-2020.download/\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qBEhsND6ZMwp"
   },
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "EV.drop(EV[[\"MAC Address\", \"Org Name\", \"Transaction Date (Pacific Time)\", \"GHG Savings (kg)\", \"Gasoline Savings (gallons)\", \"Address 1\", \"City\", \"EVSE ID\", \"Port Type\", \"Port Number\", \"Plug Type\", \"Plug In Event Id\", \"State/Province\", \"Country\", \"Currency\", \"County\", \"System S/N\", \"Ended By\", \"Model Number\"]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q8S_q1V0ZMwp"
   },
   "outputs": [],
   "source": [
    "duplicates = EV.duplicated()\n",
    "\n",
    "num_duplicates = duplicates.sum()\n",
    "print(\"Number of duplicate rows:\", num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ErZhzj3wZMwq"
   },
   "outputs": [],
   "source": [
    "# Check for duplicate rows in the dataset\n",
    "duplicates = EV.duplicated()\n",
    "\n",
    "# Calculate the number of duplicate rows\n",
    "num_duplicates = duplicates.sum()\n",
    "print(\"Number of duplicate rows:\", num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lGol5DJJZMwq"
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows from the dataset\n",
    "EV = EV.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ww190eXbZMwq"
   },
   "outputs": [],
   "source": [
    "# Convert 'Start Date' and 'End Date' columns to Datetime\n",
    "EV[[\"Start Date\", \"End Date\"]]= EV[[\"Start Date\", \"End Date\"]].apply(pd.to_datetime, format='%m/%d/%Y %H:%M', errors='coerce') # Invalid formats would become NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vb2NeJKHZMwr"
   },
   "outputs": [],
   "source": [
    "# Convert 'Total Duration (hh:mm:ss)' and 'Charging Time (hh:mm:ss)' columns to Timedelta\n",
    "EV[[\"Total Duration (hh:mm:ss)\", \"Charging Time (hh:mm:ss)\"]]= EV[[\"Total Duration (hh:mm:ss)\", \"Charging Time (hh:mm:ss)\"]].apply(pd.to_timedelta, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z2rjTWQtZMwr"
   },
   "outputs": [],
   "source": [
    "# Creating Charging_min Column\n",
    "EV[\"Charging_min\"] = EV[\"Charging Time (hh:mm:ss)\"].dt.total_seconds() / 60\n",
    "\n",
    "# Creating Duration_min Column\n",
    "EV[\"Duration_min\"] = EV[\"Total Duration (hh:mm:ss)\"].dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1CKCEjWsZMwr"
   },
   "outputs": [],
   "source": [
    "# Calculate missing \"End Date\" values based on \"Start Date\" and \"Total Duration (hh:mm:ss)\" when \"End Date\" is NaT\n",
    "EV['End Date'] = EV.apply(lambda row: row['Start Date'] + row['Total Duration (hh:mm:ss)'] if pd.isnull(row['End Date']) else row['End Date'], axis=1)\n",
    "\n",
    "# Fill missing values in the \"Driver Postal Code\" and \"User ID\" columns with \"Unknown\" as a placeholder\n",
    "EV = EV.fillna({\"Driver Postal Code\": \"Unknown\", \"User ID\": \"Unknown\"})\n",
    "\n",
    "# Verify the effectiveness of missing value handling by checking the remaining missing values in the dataset\n",
    "EV.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bJseEda-ZMwr"
   },
   "outputs": [],
   "source": [
    "# Remove the \"Start Time Zone\" and \"End Time Zone\" columns as they are no longer needed in the dataset\n",
    "EV.drop(EV[[\"Total Duration (hh:mm:ss)\", \"Charging Time (hh:mm:ss)\"]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dqBldUPhZMws"
   },
   "outputs": [],
   "source": [
    "# Define a time delta representing the difference between UTC and Pacific Standard Time (PST)\n",
    "PST= pd.to_timedelta(\"0 days 08:00:00\")\n",
    "# Define a time delta representing the difference between UTC and Pacific Daylight Time (PDT)\n",
    "PDT= pd.to_timedelta(\"0 days 07:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WK4bwk2WZMws"
   },
   "outputs": [],
   "source": [
    "# Adjust the \"Start Date\" for events where the \"Start Time Zone\" is UTC and the \"End Time Zone\" is PST\n",
    "EV.loc[(EV[\"Start Time Zone\"]== \"UTC\") & (EV[\"End Time Zone\"]== \"PST\"), \"Start Date\"] = EV.loc[(EV[\"Start Time Zone\"]== \"UTC\") & (EV[\"End Time Zone\"]== \"PST\"), \"Start Date\"] - PST\n",
    "\n",
    "# Adjust the \"Start Date\" for events where the \"Start Time Zone\" is UTC and the \"End Time Zone\" is PDT\n",
    "EV.loc[(EV[\"Start Time Zone\"]== \"UTC\") & (EV[\"End Time Zone\"]== \"PDT\"), \"Start Date\"] = EV.loc[(EV[\"Start Time Zone\"]== \"UTC\") & (EV[\"End Time Zone\"]== \"PDT\"), \"Start Date\"] - PDT\n",
    "\n",
    "# Adjust the \"End Date\" for events where the \"End Time Zone\" is UTC by subtracting the PDT time delta\n",
    "EV.loc[EV[\"End Time Zone\"]== \"UTC\", \"End Date\"] = EV[EV[\"End Time Zone\"]== \"UTC\"][\"End Date\"] - PDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "55eGQ3whZMws"
   },
   "outputs": [],
   "source": [
    "# Remove the \"Start Time Zone\" and \"End Time Zone\" columns as they are no longer needed in the dataset\n",
    "EV.drop(EV[[\"Start Time Zone\",\t\"End Time Zone\"]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "48siynh_ZMws"
   },
   "outputs": [],
   "source": [
    "# Create a copy of the original dataset 'EV' to preserve the plug identification data\n",
    "EV_s = EV.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TmFK5QvSZMws"
   },
   "outputs": [],
   "source": [
    "# Define a list of values to be removed or modified from the \"Station Name\" column\n",
    "dropping_values= ['PALO ALTO CA / ', '#', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "# Loop through each value in 'dropping_values' and apply string operations to clean 'Station Name'\n",
    "for value in dropping_values:\n",
    "  EV[\"Station Name\"]= EV[\"Station Name\"].str.replace(value, '').str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8UeI6OH-ZMws"
   },
   "outputs": [],
   "source": [
    "# Group the 'EV' dataset by the cleaned \"Station Name\" column and calculate the mean latitude and longitude values\n",
    "EV[[\"Latitude\", \"Longitude\"]]= EV.groupby(\"Station Name\")[[\"Latitude\", \"Longitude\"]].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5TgsTW-0ZMwt"
   },
   "outputs": [],
   "source": [
    "# Count charging events by User ID\n",
    "users= EV[\"User ID\"].value_counts()\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9wo197yEZMwt"
   },
   "outputs": [],
   "source": [
    "# Define bins and labels for user segmentation\n",
    "bins = [0, 2, 10, 100, 2000, float('inf')]\n",
    "labels = ['Passing', 'Infrequent', 'Occasional', 'Frequent', 'Unknown']\n",
    "\n",
    "# Perform user segmentation based on event counts\n",
    "user_segments = pd.cut(users, bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count the number of users in each segment\n",
    "segment_counts = user_segments.value_counts()\n",
    "\n",
    "# Display the user segmentation results\n",
    "print(\"User Segmentation:\")\n",
    "print(segment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FgWnOSgYZMwt"
   },
   "outputs": [],
   "source": [
    "# Create a mapping dictionary from the user_segments Series\n",
    "user_segments_mapping = user_segments.to_dict()\n",
    "\n",
    "# Add a new 'User Segments' column to the EV DataFrame by mapping user_segments\n",
    "EV['User Segment'] = EV['User ID'].map(user_segments_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dQamEt_ReaTI"
   },
   "source": [
    "ADD FEE STATUS COLUMN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uZf_k-VzeeIc"
   },
   "outputs": [],
   "source": [
    "fee_establishment = pd.to_datetime(\"2017-08-01 00:00:00\")    # End date before\n",
    "\n",
    "df['Fee_Status'] = (df['Start Date'] >= fee_establishment).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bH-2uw6Xe4V7"
   },
   "outputs": [],
   "source": [
    "dur_extreme_outliers= df[df[\"Duration_min\"] > 2000]\n",
    "\n",
    "dur_extreme_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X3zY9u-Re4V7"
   },
   "outputs": [],
   "source": [
    "# Drop extreme outliers from Duration_min column\n",
    "df.drop(dur_extreme_outliers.index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ViApyNQe4V7"
   },
   "outputs": [],
   "source": [
    "# shape of cleaned data\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ieMh9dNe4V8"
   },
   "source": [
    "## **4.2. Exploratory Data Analysis**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOb9gC57e4V8"
   },
   "source": [
    "### **4.2.1. Station-wise Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bi_G-xGcgO-Y"
   },
   "outputs": [],
   "source": [
    "# Create non_charge_min Column\n",
    "df[\"non_charge_min\"] = df[\"Duration_min\"] - df[\"Charging_min\"]\n",
    "\n",
    "# Split to before and after\n",
    "before= df[df[\"Fee_Status\"]==0]\n",
    "after= df[df[\"Fee_Status\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nMZfcfidSna4"
   },
   "outputs": [],
   "source": [
    "# Group by 'Station Name' and 'Start Date', then counting the number of events for 'before' and 'after'\n",
    "charging_events_per_day_before = before.groupby([\"Station Name\", before[\"Start Date\"].dt.date]).size().reset_index(name='Event Count')\n",
    "charging_events_per_day_after = after.groupby([\"Station Name\", after[\"Start Date\"].dt.date]).size().reset_index(name='Event Count')\n",
    "\n",
    "# Group by 'Station Name' and calculate the average charging events per day for 'before' and 'after'\n",
    "average_events_per_day_before = charging_events_per_day_before.groupby(\"Station Name\")[\"Event Count\"].mean().reset_index(name='Average Events (Before)')\n",
    "average_events_per_day_after = charging_events_per_day_after.groupby(\"Station Name\")[\"Event Count\"].mean().reset_index(name='Average Events (After)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6qGYY1woThXu"
   },
   "outputs": [],
   "source": [
    "# Concatenate DataFrames along columns\n",
    "result_df = pd.concat([average_events_per_day_before, average_events_per_day_after], axis=1)\n",
    "\n",
    "# Drop the duplicate Station Name column\n",
    "result_df = result_df.loc[:, ~result_df.columns.duplicated()]\n",
    "\n",
    "# Calculate the percentage change\n",
    "result_df['Percent Change'] = ((result_df['Average Events (After)'] - result_df['Average Events (Before)']) / result_df['Average Events (Before)']) * 100\n",
    "\n",
    "# Rename columns for clarity\n",
    "result_df.columns = ['Station', 'Average Events (Before)', 'Average Events (After)', 'Percent Change']\n",
    "\n",
    "# Display the result\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "g-iaNB6GiUZh"
   },
   "outputs": [],
   "source": [
    "# Calculate the count of charging events per station for 'before' and 'after'\n",
    "station_counts_before = before.groupby(\"Station Name\")[\"Station Name\"].count()\n",
    "station_counts_after = after.groupby(\"Station Name\")[\"Station Name\"].count()\n",
    "\n",
    "# Create a figure for the grouped bar plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define the width of each bar\n",
    "bar_width = 0.35\n",
    "\n",
    "# Generate the x-axis positions for the bars\n",
    "x = np.arange(len(station_counts_before))\n",
    "\n",
    "# Plot the 'before' data bars\n",
    "ax.bar(x - bar_width/2, station_counts_before, width=bar_width, label='Before Policy Intervention', color='dodgerblue')\n",
    "# Plot the 'after' data bars next to the 'before' bars\n",
    "ax.bar(x + bar_width/2, station_counts_after, width=bar_width, label='After Policy Intervention', color='orange')\n",
    "\n",
    "# Set the x-axis labels and rotate them for readability\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(station_counts_before.index)\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Add title and labels\n",
    "ax.set_title(\"Distribution of Stations (Before vs. After Policy Intervention)\")\n",
    "ax.set_xlabel(\"Station Name\")\n",
    "ax.set_ylabel(\"Count\")\n",
    "\n",
    "# Add a legend\n",
    "ax.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cQSe12tCixoU"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate Median Energy Consumption over all events for 'before' and 'after'\n",
    "median_energy_before = before[\"Energy (kWh)\"].median()\n",
    "median_energy_after = after[\"Energy (kWh)\"].median()\n",
    "\n",
    "# Calculate Median Energy Consumption per Station for 'before' and 'after'\n",
    "avg_station_energy_before = before.groupby(\"Station Name\")[[\"Energy (kWh)\"]].median().reset_index()\n",
    "avg_station_energy_after = after.groupby(\"Station Name\")[[\"Energy (kWh)\"]].median().reset_index()\n",
    "\n",
    "# Create a figure for the grouped bar plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define the width of each bar\n",
    "bar_width = 0.35\n",
    "\n",
    "# Generate the x-axis positions for the bars\n",
    "x = np.arange(len(avg_station_energy_before))\n",
    "\n",
    "# Plot the 'before' data bars\n",
    "ax.bar(x - bar_width/2, avg_station_energy_before[\"Energy (kWh)\"], width=bar_width, label='Before Policy Intervention', color='dodgerblue')\n",
    "# Plot the 'after' data bars next to the 'before' bars\n",
    "ax.bar(x + bar_width/2, avg_station_energy_after[\"Energy (kWh)\"], width=bar_width, label='After Policy Intervention', color='orange')\n",
    "\n",
    "# Set the x-axis labels and rotate them for readability\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(avg_station_energy_before[\"Station Name\"])\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"Station Name\")\n",
    "ax.set_ylabel(\"Energy (kWh)\")\n",
    "ax.set_title(\"Energy Consumption per Event by Station (Before vs. After Policy Intervention)\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Add horizontal line for median energy consumption for 'before' and 'after'\n",
    "ax.axhline(median_energy_before, color='dodgerblue', linestyle='--', label='Median Energy Consumption (Before Policy Intervention)')\n",
    "ax.axhline(median_energy_after, color='orange', linestyle='--', label='Median Energy Consumption (After Policy Intervention)')\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZGVfTlceLCZI"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate Median Energy Consumption over all events for 'before' and 'after'\n",
    "median_energy_after = after[\"Fee\"].median()\n",
    "\n",
    "# Calculate Median Energy Consumption per Station for 'before' and 'after'\n",
    "avg_station_energy_after = after.groupby(\"Station Name\")[[\"Fee\"]].median().reset_index()\n",
    "\n",
    "# Create a figure for the grouped bar plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Plot the 'after' data bars next to the 'before' bars\n",
    "ax.bar(x, avg_station_energy_after[\"Fee\"], color='orange')\n",
    "\n",
    "# Set the x-axis labels and rotate them for readability\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(avg_station_energy_before[\"Station Name\"])\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"Station Name\")\n",
    "ax.set_ylabel(\"Fee\")\n",
    "ax.set_title(\"Fee per Event by Station After Policy Intervention\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Add horizontal line for median energy consumption for 'before' and 'after'\n",
    "ax.axhline(median_energy_after, color='orange', linestyle='--', label='Median Fee per Event (After Policy Intervention)')\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='best')\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hMBnRXxxDiRr"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate Median Energy Consumption over all events for 'before' and 'after'\n",
    "median_energy_after = after[\"Fee\"].median()\n",
    "\n",
    "# Calculate Median Energy Consumption per Station for 'before' and 'after'\n",
    "avg_station_energy_after = after.groupby(\"Station Name\")[[\"Fee\"]].median().reset_index()\n",
    "\n",
    "# Calculate Median Energy Consumption per User Segment for 'before' and 'after'\n",
    "avg_segment_energy_after = after.groupby(\"User Segment\")[[\"Fee\"]].median().reset_index()\n",
    "\n",
    "# Define the order for the x-axis labels for user segment\n",
    "segment_order = ['Frequent', 'Occasional', 'Infrequent', 'Passing', 'Unknown']\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Plot the 'after' data for Station Name in the first subplot\n",
    "axs[0].bar(range(len(avg_station_energy_after)), avg_station_energy_after[\"Fee\"], color='orange')\n",
    "axs[0].set_xticks(range(len(avg_station_energy_after)))\n",
    "axs[0].set_xticklabels(avg_station_energy_after[\"Station Name\"], rotation=45)\n",
    "axs[0].set_xlabel(\"Station Name\")\n",
    "axs[0].set_ylabel(\"Fee\")\n",
    "axs[0].set_title(\"Fee per Event by Station After Policy Intervention\")\n",
    "axs[0].yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "axs[0].xaxis.grid(False)\n",
    "axs[0].axhline(median_energy_after, color='orange', linestyle='--', label='Median Fee per Event (After Policy Intervention)')\n",
    "axs[0].legend(loc='best')\n",
    "\n",
    "# Plot the 'after' data for User Segment in the second subplot\n",
    "axs[1].bar(range(len(avg_segment_energy_after)), avg_segment_energy_after[\"Fee\"], color='orange')\n",
    "axs[1].set_xticks(range(len(segment_order)))\n",
    "axs[1].set_xticklabels(segment_order, rotation=45)\n",
    "axs[1].set_xlabel(\"User Segment\")\n",
    "axs[1].set_ylabel(\"Fee\")\n",
    "axs[1].set_title(\"Fee per Event by User Segment After Policy Intervention\")\n",
    "axs[1].yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "axs[1].xaxis.grid(False)\n",
    "axs[1].axhline(median_energy_after, color='orange', linestyle='--', label='Median Fee per Event (After Policy Intervention)')\n",
    "axs[1].legend(loc='best')\n",
    "\n",
    "# Adjust the layout and spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the combined plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuTapvksEfLU"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate Median Energy Consumption over all events for 'before' and 'after'\n",
    "median_energy_after = after[\"Fee\"].median()\n",
    "\n",
    "# Calculate Median Energy Consumption per Station for 'before' and 'after'\n",
    "avg_station_energy_after = after.groupby(\"Station Name\")[[\"Fee\"]].median().reset_index()\n",
    "\n",
    "# Calculate Median Energy Consumption per User Segment for 'before' and 'after'\n",
    "avg_segment_energy_after = after.groupby(\"User Segment\")[[\"Fee\"]].median().reset_index()\n",
    "\n",
    "# Define the order for the x-axis labels for user segment\n",
    "segment_order = ['Frequent', 'Occasional', 'Infrequent', 'Passing', 'Unknown']\n",
    "\n",
    "# Create a figure with subplots\n",
    "fig, axs = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "# Plot the 'after' data for Station Name in the first subplot\n",
    "axs[0].bar(range(len(avg_station_energy_after)), avg_station_energy_after[\"Fee\"], color='orange', edgecolor='black', linewidth=0.5)\n",
    "axs[0].set_xticks(range(len(avg_station_energy_after)))\n",
    "axs[0].set_xticklabels(avg_station_energy_after[\"Station Name\"], rotation=45)\n",
    "axs[0].set_xlabel(\"Station Name\")\n",
    "axs[0].set_ylabel(\"Fee\")\n",
    "axs[0].set_title(\"Fee per Event by Station\")\n",
    "axs[0].yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "axs[0].xaxis.grid(False)\n",
    "axs[0].axhline(median_energy_after, color='red', linestyle='--', label='Median Fee per Event (After Policy Intervention)')\n",
    "axs[0].legend(loc='best')\n",
    "\n",
    "# Plot the 'after' data for User Segment in the second subplot\n",
    "axs[1].bar(range(len(avg_segment_energy_after)), avg_segment_energy_after[\"Fee\"], color='orange', edgecolor='black', linewidth=0.5)\n",
    "axs[1].set_xticks(range(len(segment_order)))\n",
    "axs[1].set_xticklabels(segment_order, rotation=45)\n",
    "axs[1].set_xlabel(\"User Segment\")\n",
    "axs[1].set_title(\"Fee per Event by User Segment\")\n",
    "axs[1].yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "axs[1].xaxis.grid(False)\n",
    "axs[1].axhline(median_energy_after, color='red', linestyle='--', label='Median Fee per Event (After Policy Intervention)')\n",
    "axs[1].legend(loc='best')\n",
    "\n",
    "# Remove the legend from the second subplot\n",
    "axs[1].get_legend().remove()\n",
    "\n",
    "# Adjust the layout and spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as an image for the paper\n",
    "plt.savefig(\"Fee_plot.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the combined plot (optional)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YUbRz4J1kOM3"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate Median Charging Duration over all events for 'before' and 'after'\n",
    "median_duration_before = before[\"Charging_min\"].median()\n",
    "median_duration_after = after[\"Charging_min\"].median()\n",
    "\n",
    "# Calculate Average Charging Duration per Station for 'before' and 'after'\n",
    "avg_station_duration_before = before.groupby(\"Station Name\")[[\"Charging_min\"]].median().reset_index()\n",
    "avg_station_duration_after = after.groupby(\"Station Name\")[[\"Charging_min\"]].median().reset_index()\n",
    "\n",
    "# Create a figure for the grouped bar plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define the width of each bar\n",
    "bar_width = 0.35\n",
    "\n",
    "# Generate the x-axis positions for the bars\n",
    "x = np.arange(len(avg_station_duration_before))\n",
    "\n",
    "# Plot the 'before' data bars\n",
    "ax.bar(x - bar_width/2, avg_station_duration_before[\"Charging_min\"], width=bar_width, label='Before Policy Intervention', color='dodgerblue')\n",
    "# Plot the 'after' data bars next to the 'before' bars\n",
    "ax.bar(x + bar_width/2, avg_station_duration_after[\"Charging_min\"], width=bar_width, label='After Policy Intervention', color='orange')\n",
    "\n",
    "# Add horizontal line for median Charging Duration for 'before' and 'after'\n",
    "ax.axhline(median_duration_before, color='dodgerblue', linestyle='--', label='Median Duration (Before Policy Intervention)')\n",
    "ax.axhline(median_duration_after, color='orange', linestyle='--', label='Median Duration (After Policy Intervention)')\n",
    "\n",
    "# Set the x-axis labels and rotate them for readability\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(avg_station_duration_before[\"Station Name\"])\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"Station Name\")\n",
    "ax.set_ylabel(\"Charging Duration (minutes)\")\n",
    "ax.set_title(\"Charging Duration per Event by Station (Before vs. After Policy Intervention)\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='lower left')\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZLb8wGNMe4WA"
   },
   "source": [
    "### **4.2.2. Charging Stations Spatial Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3wH31Uz_l8qS"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate Average non_charge_min for 'before' and 'after'\n",
    "avg_non_charge_before = before[\"Duration_min\"].mean()\n",
    "avg_non_charge_after = after[\"Duration_min\"].mean()\n",
    "\n",
    "# Calculate Average non_charge_min duration per Station for 'before' and 'after'\n",
    "non_charge_before = before.groupby(\"Station Name\")[\"Duration_min\"].mean().reset_index()\n",
    "non_charge_after = after.groupby(\"Station Name\")[\"Duration_min\"].mean().reset_index()\n",
    "\n",
    "# Create a figure for the grouped bar plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define the width of each bar\n",
    "bar_width = 0.35\n",
    "\n",
    "# Generate the x-axis positions for the bars\n",
    "x = np.arange(len(non_charge_before))\n",
    "\n",
    "# Plot the 'before' data bars\n",
    "ax.bar(x - bar_width/2, non_charge_before[\"Duration_min\"], width=bar_width, label='Before Policy Intervention', color='dodgerblue')\n",
    "# Plot the 'after' data bars next to the 'before' bars\n",
    "ax.bar(x + bar_width/2, non_charge_after[\"Duration_min\"], width=bar_width, label='After Policy Intervention', color='orange')\n",
    "\n",
    "# Add horizontal line for average wait time for 'before' and 'after'\n",
    "ax.axhline(avg_non_charge_before, color='dodgerblue', linestyle='--', label='Average Line (Before Policy Intervention)')\n",
    "ax.axhline(avg_non_charge_after, color='orange', linestyle='--', label='Average Line (After Policy Intervention)')\n",
    "\n",
    "# Set the x-axis labels and rotate them for readability\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(non_charge_before[\"Station Name\"])\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"Station Name\")\n",
    "ax.set_ylabel(\"Minutes\")\n",
    "ax.set_title(\"Average Time Spent Without Charging at Stations (Before vs. After Policy Intervention)\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc=\"upper left\")\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TPzx8In0nOGt"
   },
   "outputs": [],
   "source": [
    "import branca.colormap as cm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w7Xb7ndSJ-g5"
   },
   "outputs": [],
   "source": [
    "# Group the data to calculate the mean duration_min for each station for 'before' and 'after'\n",
    "duration_map_before = before.groupby([\"Station Name\", \"Latitude\", \"Longitude\"])[\"Duration_min\"].mean().reset_index()\n",
    "duration_map_after = after.groupby([\"Station Name\", \"Latitude\", \"Longitude\"])[\"Duration_min\"].mean().reset_index()\n",
    "\n",
    "# Define a colormap for marker colors for data\n",
    "colormap_before = cm.LinearColormap(['yellow', 'red', 'black'], vmin=non_charge_map_after['Duration_min'].min(), vmax=non_charge_map_before['Duration_min'].max())\n",
    "\n",
    "# Create a base map centered at Palo Alto with a different tileset\n",
    "base_map = folium.Map(location=[37.4419, -122.143936], zoom_start=13, tiles=\"CartoDB positron\")\n",
    "\n",
    "# Create a feature group for 'before' data\n",
    "fg_before = folium.FeatureGroup(name='Average Event Duration (Before Policy Intervention)')\n",
    "\n",
    "# Create a feature group for 'after' data\n",
    "fg_after = folium.FeatureGroup(name='Average Event Duration (After Policy Intervention)')\n",
    "\n",
    "# Iterate over stations and add markers for 'before' data\n",
    "for index, row in duration_map_before.iterrows():\n",
    "    station_name = row['Station Name']\n",
    "    duration = row['Duration_min']\n",
    "    latitude = row['Latitude']\n",
    "    longitude = row['Longitude']\n",
    "\n",
    "    # Get the corresponding marker color from the colormap\n",
    "    marker_color = colormap_before(duration)\n",
    "\n",
    "    # Create a custom HTML icon as an HTML string with inline style for color\n",
    "    icon_html = f'<i class=\"fa-solid fa-charging-station fa-xl\" style=\"color: {marker_color};\"></i>'\n",
    "\n",
    "    # Create the marker and set its icon using the custom HTML\n",
    "    marker = folium.Marker(\n",
    "        location=[latitude, longitude],\n",
    "        icon=folium.DivIcon(html=icon_html)\n",
    "    )\n",
    "\n",
    "    # Add the marker to the 'before' feature group\n",
    "    marker.add_to(fg_before)\n",
    "\n",
    "    # Add the station name as a label to the left of the marker\n",
    "    folium.Marker(\n",
    "        location=[latitude, longitude],\n",
    "        icon=folium.DivIcon(icon_size=(10, 10), icon_anchor=(10, 0),  # Adjust icon_anchor to move text to the left\n",
    "            html=f'<div style=\"font-size: 8pt; width: 1000%; height: 0.1em; position: relative; bottom:1px; left: 25px;\">{station_name}</div>'\n",
    "        )\n",
    "    ).add_to(fg_before)\n",
    "\n",
    "# Iterate over stations and add markers for 'after' data\n",
    "for index, row in duration_map_after.iterrows():\n",
    "    station_name = row['Station Name']\n",
    "    duration = row['Duration_min']\n",
    "    latitude = row['Latitude']\n",
    "    longitude = row['Longitude']\n",
    "\n",
    "    # Get the corresponding marker color from the colormap\n",
    "    marker_color = colormap_before(duration)\n",
    "\n",
    "    # Create a custom HTML icon as an HTML string with inline style for color\n",
    "    icon_html = f'<i class=\"fa-solid fa-charging-station fa-xl\" style=\"color: {marker_color};\"></i>'\n",
    "\n",
    "    # Create the marker and set its icon using the custom HTML\n",
    "    marker = folium.Marker(\n",
    "        location=[latitude, longitude],\n",
    "        icon=folium.DivIcon(html=icon_html)\n",
    "    )\n",
    "\n",
    "    # Add the marker to the 'after' feature group\n",
    "    marker.add_to(fg_after)\n",
    "\n",
    "    # Add the station name as a label to the left of the marker\n",
    "    folium.Marker(\n",
    "        location=[latitude, longitude],\n",
    "        icon=folium.DivIcon(icon_size=(10, 10), icon_anchor=(10, 0),  # Adjust icon_anchor to move text to the left\n",
    "            html=f'<div style=\"font-size: 8pt; width: 1000%; height: 0.1em; position: relative; bottom:1px; left: 25px;\">{station_name}</div>'\n",
    "        )\n",
    "    ).add_to(fg_after)\n",
    "\n",
    "# Add both feature groups to the base map\n",
    "fg_before.add_to(base_map)\n",
    "fg_after.add_to(base_map)\n",
    "\n",
    "# Add the colormap legend for 'before' data\n",
    "colormap_before.caption = 'Average Event Duration'\n",
    "colormap_before.add_to(base_map)\n",
    "\n",
    "# Add layer control to switch between 'before' and 'after' data\n",
    "folium.LayerControl().add_to(base_map)\n",
    "\n",
    "# Save the map as an HTML file for inclusion in the paper\n",
    "base_map.save(\"event_duration_map.html\")\n",
    "\n",
    "# Display the map\n",
    "base_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rZF-Z0JMe4WC"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Group by 'Station Name' and 'Start Date', then counting the number of events for 'before' and 'after'\n",
    "charging_events_per_day_before = before.groupby([\"Station Name\", \"Latitude\", \"Longitude\", EV[\"Start Date\"].dt.date]).size().reset_index(name='Event Count')\n",
    "charging_events_per_day_after = after.groupby([\"Station Name\", \"Latitude\", \"Longitude\", EV[\"Start Date\"].dt.date]).size().reset_index(name='Event Count')\n",
    "\n",
    "# Calculate Average Event Count per day for 'before' and 'after'\n",
    "avg_event_before = charging_events_per_day_before[\"Event Count\"].mean()\n",
    "avg_event_after = charging_events_per_day_after[\"Event Count\"].mean()\n",
    "\n",
    "# Group by 'Station Name' and calculate the average charging events per day for 'before' and 'after'\n",
    "average_events_per_day_before = charging_events_per_day_before.groupby([\"Station Name\", \"Latitude\", \"Longitude\"])[\"Event Count\"].mean().reset_index(name='Average Events (Before)')\n",
    "average_events_per_day_after = charging_events_per_day_after.groupby([\"Station Name\", \"Latitude\", \"Longitude\"])[\"Event Count\"].mean().reset_index(name='Average Events (After)')\n",
    "\n",
    "# Create a figure for the grouped bar plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define the width of each bar\n",
    "bar_width = 0.35\n",
    "\n",
    "# Generate the x-axis positions for the bars\n",
    "x = np.arange(len(average_events_per_day_before))\n",
    "\n",
    "# Plot the 'before' data bars\n",
    "ax.bar(x - bar_width/2, average_events_per_day_before[\"Average Events (Before)\"], width=bar_width, label='Before Policy Intervention', color='dodgerblue')\n",
    "# Plot the 'after' data bars next to the 'before' bars\n",
    "ax.bar(x + bar_width/2, average_events_per_day_after[\"Average Events (After)\"], width=bar_width, label='After Policy Intervention', color='orange')\n",
    "\n",
    "# Add horizontal line for average Event Count for 'before' and 'after'\n",
    "ax.axhline(avg_event_before, color='dodgerblue', linestyle='--', label='Average Line (Before Policy Intervention)')\n",
    "ax.axhline(avg_event_after, color='orange', linestyle='--', label='Average Line (After Policy Intervention)')\n",
    "\n",
    "# Set the x-axis labels and rotate them for readability\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(average_events_per_day_before[\"Station Name\"])\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"Station Name\")\n",
    "ax.set_ylabel(\"Events per Day\")\n",
    "ax.set_title(\"Average Charging Events per Day in Stations (Before vs. After Policy Intervention)\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc=\"best\")\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jC7G051hAY3s"
   },
   "outputs": [],
   "source": [
    "from folium.plugins import MarkerCluster, FastMarkerCluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVf0Qv7aCLmX"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium import plugins\n",
    "from folium.plugins import MarkerCluster, FeatureGroupSubGroup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wvEUbbhcAcOr"
   },
   "outputs": [],
   "source": [
    "# Define a colormap for marker colors\n",
    "colormap = cm.LinearColormap(['yellow', 'red', 'black'], vmin=average_events_per_day_after[\"Average Events (After)\"].min(), vmax=average_events_per_day_before[\"Average Events (Before)\"].max())\n",
    "\n",
    "# Create a base map centered at Palo Alto with a different tileset\n",
    "event_map = folium.Map(location=[37.4419, -122.143936], zoom_start=13, tiles=\"CartoDB positron\")\n",
    "\n",
    "# Create a feature group for 'before' data\n",
    "fg_before = folium.FeatureGroup(name='Events per Day (Before)')\n",
    "\n",
    "# Create a feature group for 'after' data\n",
    "fg_after = folium.FeatureGroup(name='Events per Day (After)')\n",
    "\n",
    "# Iterate over stations and add markers for 'before' data\n",
    "for index, row in average_events_per_day_before.iterrows():\n",
    "    station_name = row['Station Name']\n",
    "    average_events = row['Average Events (Before)']\n",
    "    latitude = row['Latitude']\n",
    "    longitude = row['Longitude']\n",
    "\n",
    "    # Get the corresponding marker color from the colormap\n",
    "    marker_color = colormap(average_events)\n",
    "\n",
    "    # Create a custom HTML icon as an HTML string with inline style for color\n",
    "    icon_html = f'<i class=\"fa-solid fa-charging-station fa-xl\" style=\"color: {marker_color};\"></i>'\n",
    "\n",
    "    # Create the marker and set its icon using the custom HTML\n",
    "    marker = folium.Marker(\n",
    "        location=[latitude, longitude],\n",
    "        icon=folium.DivIcon(html=icon_html)\n",
    "    )\n",
    "\n",
    "    # Add the marker to the 'before' feature group\n",
    "    marker.add_to(fg_before)\n",
    "\n",
    "    # Add the station name as a label to the left of the marker\n",
    "    folium.Marker(\n",
    "        location=[latitude, longitude],\n",
    "        icon=folium.DivIcon(icon_size=(10, 10), icon_anchor=(10, 0),  # Adjust icon_anchor to move text to the left\n",
    "            html=f'<div style=\"font-size: 8pt; width: 1000%; height: 0.1em; position: relative; bottom:1px; left: 25px;\">{station_name}</div>'\n",
    "        )\n",
    "    ).add_to(fg_before)\n",
    "\n",
    "# Iterate over stations and add markers for 'after' data\n",
    "for index, row in average_events_per_day_after.iterrows():\n",
    "    station_name = row['Station Name']\n",
    "    average_events = row['Average Events (After)']\n",
    "    latitude = row['Latitude']\n",
    "    longitude = row['Longitude']\n",
    "\n",
    "    # Get the corresponding marker color from the colormap\n",
    "    marker_color = colormap(average_events)\n",
    "\n",
    "    # Create a custom HTML icon as an HTML string with inline style for color\n",
    "    icon_html = f'<i class=\"fa-solid fa-charging-station fa-xl\" style=\"color: {marker_color};\"></i>'\n",
    "\n",
    "    # Create the marker and set its icon using the custom HTML\n",
    "    marker = folium.Marker(\n",
    "        location=[latitude, longitude],\n",
    "        icon=folium.DivIcon(html=icon_html)\n",
    "    )\n",
    "\n",
    "    # Add the marker to the 'after' feature group\n",
    "    marker.add_to(fg_after)\n",
    "\n",
    "    # Add the station name as a label to the left of the marker\n",
    "    folium.Marker(\n",
    "        location=[latitude, longitude],\n",
    "        icon=folium.DivIcon(icon_size=(10, 10), icon_anchor=(10, 0),  # Adjust icon_anchor to move text to the left\n",
    "            html=f'<div style=\"font-size: 8pt; width: 1000%; height: 0.1em; position: relative; bottom:1px; left: 25px;\">{station_name}</div>'\n",
    "        )\n",
    "    ).add_to(fg_after)\n",
    "\n",
    "\n",
    "# Add both feature groups to the base map\n",
    "fg_before.add_to(event_map)\n",
    "fg_after.add_to(event_map)\n",
    "\n",
    "# Add the colormap legend\n",
    "colormap.caption = 'Average Events per Day'\n",
    "event_map.add_child(colormap)\n",
    "\n",
    "# Add layer control to switch between 'before' and 'after' data\n",
    "folium.LayerControl().add_to(event_map)\n",
    "\n",
    "# Save the map as an HTML file for inclusion in the paper\n",
    "event_map.save(\"charging_events_map.html\")\n",
    "\n",
    "# Display the map\n",
    "event_map\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gwqtgeo2NZPB"
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "import base64\n",
    "\n",
    "# Create a figure with two subplots for side-by-side maps and resize it\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(10, 5))\n",
    "\n",
    "# Save the 'before' and 'after' maps as HTML files\n",
    "before_map.save('before_map.html')\n",
    "after_map.save('after_map.html')\n",
    "\n",
    "# Display the 'before' map in the first subplot\n",
    "with open('before_map.html', 'r') as f:\n",
    "    before_map_html = f.read()\n",
    "    before_map_html = before_map_html.replace('width: 10%; height: 10%;', 'width: 10%; height: 10%;')  # Maintain original size\n",
    "    ax1.set_title('Before Policy Intervention')\n",
    "    ax1.get_xaxis().set_visible(False)\n",
    "    ax1.get_yaxis().set_visible(False)\n",
    "    ax1.text(0.5, 0.5, 'Loading...', horizontalalignment='center', verticalalignment='center')\n",
    "    ax1.axis('off')\n",
    "    ax1.text(0.5, 0.5, before_map_html, horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "# Display the 'after' map in the second subplot\n",
    "with open('after_map.html', 'r') as f:\n",
    "    after_map_html = f.read()\n",
    "    after_map_html = after_map_html.replace('width: 10%; height: 10%;', 'width: 10%; height: 10%;')  # Maintain original size\n",
    "    ax2.set_title('After Policy Intervention')\n",
    "    ax2.get_xaxis().set_visible(False)\n",
    "    ax2.get_yaxis().set_visible(False)\n",
    "    ax2.text(0.5, 0.5, 'Loading...', horizontalalignment='center', verticalalignment='center')\n",
    "    ax2.axis('off')\n",
    "    ax2.text(0.5, 0.5, after_map_html, horizontalalignment='center', verticalalignment='center')\n",
    "\n",
    "# Save the combined maps as an image for the paper\n",
    "plt.savefig('combined_maps.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the combined image\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MgixkBaIe4WD"
   },
   "source": [
    "### **4.2.3. User-wise Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVlnPeg4QzcI"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Define the order of segments\n",
    "segment_order = ['Frequent', 'Occasional', 'Infrequent', 'Passing', 'Unknown']\n",
    "\n",
    "# Grouping by 'User Segments' and 'Start Date', then counting the number of events for 'before' and 'after'\n",
    "charging_events_per_day_user_before = before.groupby([\"User Segment\", before[\"Start Date\"].dt.date]).size().reset_index(name='Event Count')\n",
    "charging_events_per_day_user_after = after.groupby([\"User Segment\", after[\"Start Date\"].dt.date]).size().reset_index(name='Event Count')\n",
    "\n",
    "# Reorder segments based on the defined order\n",
    "charging_events_per_day_user_before['User Segment'] = pd.Categorical(charging_events_per_day_user_before['User Segment'], categories=segment_order, ordered=True)\n",
    "charging_events_per_day_user_after['User Segment'] = pd.Categorical(charging_events_per_day_user_after['User Segment'], categories=segment_order, ordered=True)\n",
    "\n",
    "# Calculating Average Event Count per day for each user segment for 'before' and 'after'\n",
    "avg_event_per_day_user_before = charging_events_per_day_user_before.groupby(\"User Segment\")[\"Event Count\"].mean().reset_index(name='Average Events (Before)')\n",
    "avg_event_per_day_user_after = charging_events_per_day_user_after.groupby(\"User Segment\")[\"Event Count\"].mean().reset_index(name='Average Events (After)')\n",
    "\n",
    "# Calculate Average Event Count over for 'before' and 'after'\n",
    "avg_event_before = charging_events_per_day_user_before[\"Event Count\"].mean()\n",
    "avg_event_after = charging_events_per_day_user_after[\"Event Count\"].mean()\n",
    "\n",
    "# Create a figure for the grouped bar plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define the width of each bar\n",
    "bar_width = 0.35\n",
    "\n",
    "# Generate the x-axis positions for the bars\n",
    "x = np.arange(len(avg_event_per_day_user_before))\n",
    "\n",
    "# Plot the 'before' data bars\n",
    "ax.bar(x - bar_width/2, avg_event_per_day_user_before[\"Average Events (Before)\"], width=bar_width, label='Before Policy Intervention', color='dodgerblue')\n",
    "# Plot the 'after' data bars next to the 'before' bars\n",
    "ax.bar(x + bar_width/2, avg_event_per_day_user_after[\"Average Events (After)\"], width=bar_width, label='After Policy Intervention', color='orange')\n",
    "\n",
    "# Add horizontal line for average event count for 'before' and 'after'\n",
    "ax.axhline(avg_event_before, color='dodgerblue', linestyle='--', label='Median Events count (Before Policy Intervention)')\n",
    "ax.axhline(avg_event_after, color='orange', linestyle='--', label='Median Events count (After Policy Intervention)')\n",
    "\n",
    "# Set the x-axis labels in the specified order and rotate them for readability\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(segment_order)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"User Segment\")\n",
    "ax.set_ylabel(\"Average Events per Day\")\n",
    "ax.set_title(\"Average Charging Events per Day by User Segment (Before vs. After Policy Intervention)\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc=\"upper right\")\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "srO3ZvjBI1A6"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Define the order of segments\n",
    "segment_order = ['Frequent', 'Occasional', 'Infrequent', 'Passing', 'Unknown']\n",
    "\n",
    "# Grouping by 'User Segments' and 'Start Date', then counting the number of events for 'before' and 'after'\n",
    "charging_events_per_day_user_before = before.groupby([\"User Segment\", before[\"Start Date\"].dt.date]).size().reset_index(name='Event Count')\n",
    "charging_events_per_day_user_after = after.groupby([\"User Segment\", after[\"Start Date\"].dt.date]).size().reset_index(name='Event Count')\n",
    "\n",
    "# Reorder segments based on the defined order\n",
    "charging_events_per_day_user_before['User Segment'] = pd.Categorical(charging_events_per_day_user_before['User Segment'], categories=segment_order, ordered=True)\n",
    "charging_events_per_day_user_after['User Segment'] = pd.Categorical(charging_events_per_day_user_after['User Segment'], categories=segment_order, ordered=True)\n",
    "\n",
    "# Calculating Average Event Count per day for each user segment for 'before' and 'after'\n",
    "avg_event_per_day_user_before = charging_events_per_day_user_before.groupby(\"User Segment\")[\"Event Count\"].mean().reset_index(name='Average Events (Before)')\n",
    "avg_event_per_day_user_after = charging_events_per_day_user_after.groupby(\"User Segment\")[\"Event Count\"].mean().reset_index(name='Average Events (After)')\n",
    "\n",
    "# Create a figure for the grouped bar plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define the width of each bar\n",
    "bar_width = 0.35\n",
    "\n",
    "# Generate the x-axis positions for the bars\n",
    "x = np.arange(len(avg_event_per_day_user_before))\n",
    "\n",
    "# Plot the 'before' data bars\n",
    "ax.bar(x - bar_width/2, avg_event_per_day_user_before[\"Average Events (Before)\"], width=bar_width, label='Before Policy Intervention', color='dodgerblue', edgecolor='black', linewidth=0.5)\n",
    "# Plot the 'after' data bars next to the 'before' bars\n",
    "ax.bar(x + bar_width/2, avg_event_per_day_user_after[\"Average Events (After)\"], width=bar_width, label='After Policy Intervention', color='orange', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Set the x-axis labels in the specified order and rotate them for readability\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(segment_order)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"User Segment\")\n",
    "ax.set_ylabel(\"Average Events per Day\")\n",
    "ax.set_title(\"Average Charging Events per Day by User Segment (Before vs. After Policy Intervention)\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc=\"upper right\")\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as an image for the paper\n",
    "plt.savefig(\"Avg_Events_p_Day_User_Segment.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot (optional)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KO0AX2edK3tC"
   },
   "outputs": [],
   "source": [
    "# Calculate Average Charging Duration per User Segment for 'before' and 'after'\n",
    "avg_segment_duration_before = before.groupby(\"User Segment\")[[\"Duration_min\"]].median().reset_index()\n",
    "avg_segment_duration_after = after.groupby(\"User Segment\")[[\"Duration_min\"]].median().reset_index()\n",
    "\n",
    "# Define the order of segments\n",
    "segment_order = ['Frequent', 'Occasional', 'Infrequent', 'Passing', 'Unknown']\n",
    "\n",
    "# Reorder segments based on the defined order\n",
    "avg_segment_duration_before['User Segment'] = pd.Categorical(avg_segment_duration_before['User Segment'], categories=segment_order, ordered=True)\n",
    "avg_segment_duration_after['User Segment'] = pd.Categorical(avg_segment_duration_after['User Segment'], categories=segment_order, ordered=True)\n",
    "\n",
    "# Create a figure for the grouped bar plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define the width of each bar\n",
    "bar_width = 0.35\n",
    "\n",
    "# Generate the x-axis positions for the bars\n",
    "x = np.arange(len(avg_segment_duration_before))\n",
    "\n",
    "# Plot the 'before' data bars\n",
    "ax.bar(x - bar_width/2, avg_segment_duration_before[\"Duration_min\"], width=bar_width, label='Before Policy Intervention', color='dodgerblue', edgecolor='black', linewidth=0.5)\n",
    "# Plot the 'after' data bars next to the 'before' bars\n",
    "ax.bar(x + bar_width/2, avg_segment_duration_after[\"Duration_min\"], width=bar_width, label='After Policy Intervention', color='orange', edgecolor='black', linewidth=0.5)\n",
    "\n",
    "# Set the x-axis labels in the specified order and rotate them for readability\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(segment_order)\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"User Segment\")\n",
    "ax.set_ylabel(\"Average Event Duration (minutes)\")\n",
    "ax.set_title(\"Average Event Duration by User Type (Before vs. After Policy Intervention)\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "ax.xaxis.grid(False)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='best')\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as an image for the paper\n",
    "plt.savefig(\"Avg_Duration_by_User_Type.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WjKdpLRCe4WJ"
   },
   "outputs": [],
   "source": [
    "# Set a style using Seaborn\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Calculate Average non_charge_min for 'before' and 'after'\n",
    "avg_non_charge_before = before[\"non_charge_min\"].mean()\n",
    "avg_non_charge_after = after[\"non_charge_min\"].mean()\n",
    "\n",
    "# Calculate Average non_charge_min duration per User Segment for 'before' and 'after'\n",
    "non_charge_before = before.groupby(\"User Segment\")[\"non_charge_min\"].mean().reset_index()\n",
    "non_charge_after = after.groupby(\"User Segment\")[\"non_charge_min\"].mean().reset_index()\n",
    "\n",
    "# Create a figure for the grouped bar plot\n",
    "fig, ax = plt.subplots(figsize=(12, 6))\n",
    "\n",
    "# Define the width of each bar\n",
    "bar_width = 0.35\n",
    "\n",
    "# Generate the x-axis positions for the bars\n",
    "x = np.arange(len(non_charge_before))\n",
    "\n",
    "# Plot the 'before' data bars\n",
    "ax.bar(x - bar_width/2, non_charge_before[\"non_charge_min\"], width=bar_width, label='Before Policy Intervention', color='dodgerblue')\n",
    "# Plot the 'after' data bars next to the 'before' bars\n",
    "ax.bar(x + bar_width/2, non_charge_after[\"non_charge_min\"], width=bar_width, label='After Policy Intervention', color='orange')\n",
    "\n",
    "# Add horizontal line for average wait time for 'before' and 'after'\n",
    "ax.axhline(avg_non_charge_before, color='dodgerblue', linestyle='--', label='Average Line (Before Policy Intervention)')\n",
    "ax.axhline(avg_non_charge_after, color='orange', linestyle='--', label='Average Line (After Policy Intervention)')\n",
    "\n",
    "# Set the x-axis labels and rotate them for readability\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(non_charge_before[\"User Segment\"], rotation=45, ha=\"right\")\n",
    "\n",
    "# Add labels and title\n",
    "ax.set_xlabel(\"User Segment\")\n",
    "ax.set_ylabel(\"Minutes\")\n",
    "ax.set_title(\"Average Time Spent Without Charging by User Type (Before vs. After Policy Intervention)\")\n",
    "\n",
    "# Add grid lines\n",
    "ax.yaxis.grid(True, linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc=\"upper center\")\n",
    "\n",
    "# Improve spacing and layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "n3A_ExBfe4WJ"
   },
   "source": [
    "### **4.2.4. Origin-Destination Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2pU_rwrze4WJ"
   },
   "outputs": [],
   "source": [
    "# Making a list of Driver Postal Codes\n",
    "postal_codes_list= df[df[\"Driver Postal Code\"] != \"Unknown\"][\"Driver Postal Code\"].astype(int)\n",
    "postal_codes_list= postal_codes_list.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T54QOO3ye4WK"
   },
   "outputs": [],
   "source": [
    "# Create a geocoder instance for the United States\n",
    "geolocator = pgeocode.Nominatim('us')\n",
    "\n",
    "postal_code_data = []\n",
    "\n",
    "# Iterate through the list of postal codes and fetch their coordinates\n",
    "for postal_code in postal_codes_list:\n",
    "    location = geolocator.query_postal_code(postal_code)\n",
    "\n",
    "    if not location.empty:\n",
    "        data = {\n",
    "            \"postal_code\": postal_code,\n",
    "            \"latitude\": location.latitude,\n",
    "            \"longitude\": location.longitude\n",
    "        }\n",
    "        postal_code_data.append(data)\n",
    "    else:\n",
    "        print(f\"No location found for postal code: {postal_code}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qLQGNYeve4WK"
   },
   "outputs": [],
   "source": [
    "# Create a postal code DataFrame from the postal code data list\n",
    "df_postal = pd.DataFrame(postal_code_data)\n",
    "df_postal = df_postal.dropna(subset=['latitude', 'longitude'])\n",
    "\n",
    "# Create a new DataFrame from the EV dataset, excluding records with \"Unknown\" postal codes\n",
    "zip_codes = df[df[\"Driver Postal Code\"] != \"Unknown\"].copy()  # Use .copy() to create a copy of the DataFrame\n",
    "# Changing postal code data type to integer for merging\n",
    "zip_codes[\"Driver Postal Code\"] = zip_codes[\"Driver Postal Code\"].astype(int)\n",
    "\n",
    "# Merge the postal code information with the original DataFrame\n",
    "ev_zip_merged = pd.merge(zip_codes, df_postal, left_on=\"Driver Postal Code\", right_on=\"postal_code\")\n",
    "\n",
    "# Print the tail of the merged DataFrame to inspect the result\n",
    "ev_zip_merged.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hrsiajaru8MR"
   },
   "outputs": [],
   "source": [
    "# Split to before and after\n",
    "beforee= ev_zip_merged[ev_zip_merged[\"Fee_Status\"]==0]\n",
    "afterr= ev_zip_merged[ev_zip_merged[\"Fee_Status\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "F0_86NlBe4WL"
   },
   "outputs": [],
   "source": [
    "# Calculat3 Total Days over the Dataset\n",
    "total_days_before = len(beforee[\"Start Date\"].dt.date.unique())\n",
    "\n",
    "# Group by 'Station Name' and calculate the average charging events per day\n",
    "pc_total_events_before = beforee.groupby([\"postal_code\", \"latitude\",\t\"longitude\"]).size().reset_index(name='Total Events')\n",
    "\n",
    "# Calculate Events per Day by Postal Code\n",
    "pc_total_events_before[\"Event_p_Day\"]= pc_total_events_before[\"Total Events\"]/ total_days_before\n",
    "\n",
    "# Filter the Frequent Postal codes with average event more than once a month\n",
    "pc_total_events_before= pc_total_events_before[pc_total_events_before[\"Event_p_Day\"]> 0.033]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DcvzAwwdPX8v"
   },
   "outputs": [],
   "source": [
    "# Calculat3 Total Days over the Dataset\n",
    "total_days_after = len(afterr[\"Start Date\"].dt.date.unique())\n",
    "\n",
    "# Group by 'Station Name' and calculate the average charging events per day\n",
    "pc_total_events_after = afterr.groupby([\"postal_code\", \"latitude\",\t\"longitude\"]).size().reset_index(name='Total Events')\n",
    "\n",
    "# Calculate Events per Day by Postal Code\n",
    "pc_total_events_after[\"Event_p_Day\"]= pc_total_events_after[\"Total Events\"]/ total_days_after\n",
    "\n",
    "# Filter the Frequent Postal codes with average event more than once a month\n",
    "pc_total_events_after= pc_total_events_after[pc_total_events_after[\"Event_p_Day\"]> 0.033]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z8VQuLh4V6ss"
   },
   "outputs": [],
   "source": [
    "pc_total_events_after[\"Event_p_Day\"].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hug8OWWLe4WL"
   },
   "outputs": [],
   "source": [
    "# Create a base map centered at Palo Alto\n",
    "pcmap = folium.Map(location=[37.4419, -122.143936], zoom_start=8.5, tiles=\"CartoDB positron\")\n",
    "\n",
    "# Convert the data into a list of (latitude, longitude, value) tuples\n",
    "heatmap_data = [\n",
    "    (row['latitude'], row['longitude'], row['Event_p_Day']) for index, row in pc_total_events_before.iterrows()\n",
    "]\n",
    "\n",
    "# Determine the range of values for the colorbar\n",
    "min_value = pc_total_events_after[\"Event_p_Day\"].min()\n",
    "max_value = pc_total_events_before[\"Event_p_Day\"].max()\n",
    "\n",
    "# Create a heatmap layer with default colormap\n",
    "heatmap_layer = HeatMap(heatmap_data, radius=10, blur=7)\n",
    "heatmap_layer.add_to(pcmap)\n",
    "heatmap_layer.layer_name = 'Heatmap of Frequent Events'\n",
    "\n",
    "# Add colorbar legend\n",
    "colormap_caption = 'Events per Day'\n",
    "colormap = folium.LinearColormap(colors=['steelblue', 'lime', 'yellow', 'orange', 'red'], vmin=min_value, vmax=max_value)\n",
    "colormap.caption = colormap_caption\n",
    "pcmap.add_child(colormap)\n",
    "\n",
    "# Save the map as an HTML file for inclusion in the paper\n",
    "pcmap.save(\"heatmap_before_positron.html\")\n",
    "\n",
    "# Show map\n",
    "pcmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Le-8sAyGQdxC"
   },
   "outputs": [],
   "source": [
    "# Create a base map centered at Palo Alto\n",
    "pcmap = folium.Map(location=[37.4419, -122.143936], zoom_start=8.5, tiles=\"CartoDB positron\")\n",
    "\n",
    "# Convert the data into a list of (latitude, longitude, value) tuples\n",
    "heatmap_data = [\n",
    "    (row['latitude'], row['longitude'], row['Event_p_Day']) for index, row in pc_total_events_after.iterrows()\n",
    "]\n",
    "\n",
    "# Create a heatmap layer with default colormap\n",
    "heatmap_layer = HeatMap(heatmap_data, radius=10, blur=7)\n",
    "heatmap_layer.add_to(pcmap)\n",
    "heatmap_layer.layer_name = 'Heatmap of Frequent Events'\n",
    "\n",
    "# Add colorbar legend\n",
    "colormap_caption = 'Events per Day'\n",
    "colormap = folium.LinearColormap(colors=['steelblue', 'lime', 'yellow', 'orange', 'red'], vmin=min_value, vmax=max_value)\n",
    "colormap.caption = colormap_caption\n",
    "pcmap.add_child(colormap)\n",
    "\n",
    "# Save the map as an HTML file for inclusion in the paper\n",
    "pcmap.save(\"heatmap_after_positron.html\")\n",
    "\n",
    "# Show map\n",
    "pcmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYVUiyche4WM"
   },
   "outputs": [],
   "source": [
    "# Calculate Total Days over the Dataset\n",
    "total_days = len(beforee[\"Start Date\"].dt.date.unique())\n",
    "\n",
    "# Add Hour column to ev_zip_merged\n",
    "beforee[\"Hour\"]= beforee[\"Start Date\"].dt.hour\n",
    "\n",
    "# Group by 'Hour', 'postal_code', 'Station Name' and their related coordinates, then calculate the Event Counts\n",
    "hourly_OD_beforee = beforee.groupby([\"Hour\", \"postal_code\", \"latitude\",\t\"longitude\", \"Station Name\", \"Latitude\", \"Longitude\"]).size().reset_index(name='Event Count')\n",
    "\n",
    "# Calculate Events per Day\n",
    "hourly_OD_beforee[\"Event_p_Day\"]= hourly_OD_beforee[\"Event Count\"] / total_days\n",
    "\n",
    "# Filter the Frequent Routes with average event more than once a week for that perticular hour\n",
    "OD_freq_beforee= hourly_OD_beforee[hourly_OD_beforee[\"Event_p_Day\"]> 0.033]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q7knPrb6w0eu"
   },
   "outputs": [],
   "source": [
    "# Calculate Total Days over the Dataset\n",
    "total_days = len(afterr[\"Start Date\"].dt.date.unique())\n",
    "\n",
    "# Add Hour column to ev_zip_merged\n",
    "afterr[\"Hour\"]= afterr[\"Start Date\"].dt.hour\n",
    "\n",
    "# Group by 'Hour', 'postal_code', 'Station Name' and their related coordinates, then calculate the Event Counts\n",
    "hourly_OD_afterr = afterr.groupby([\"Hour\", \"postal_code\", \"latitude\",\t\"longitude\", \"Station Name\", \"Latitude\", \"Longitude\"]).size().reset_index(name='Event Count')\n",
    "\n",
    "# Calculate Events per Day\n",
    "hourly_OD_afterr[\"Event_p_Day\"]= hourly_OD_afterr[\"Event Count\"] / total_days\n",
    "\n",
    "# Filter the Frequent Routes with average event more than once a week for that perticular hour\n",
    "OD_freq_afterr= hourly_OD_afterr[hourly_OD_afterr[\"Event_p_Day\"]> 0.033]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4qjs_gBfe4WM"
   },
   "outputs": [],
   "source": [
    "# Load origin-destination dataset\n",
    "data = OD_freq_beforee\n",
    "\n",
    "# Define a list of colors from yellow to red\n",
    "colors = ['#FFFF00', '#FF7700', '#FF0000']\n",
    "\n",
    "# Create a custom colormap from the list of colors\n",
    "cmap = LinearSegmentedColormap.from_list('custom', colors, N=256)\n",
    "\n",
    "# Normalize the colormap based on the range of your events values\n",
    "norm = mcolors.Normalize(vmin=data['Event_p_Day'].min(), vmax=data['Event_p_Day'].max())\n",
    "\n",
    "# Define a function to map events to colors\n",
    "def map_events_to_color(events):\n",
    "    color = cmap(norm(events))\n",
    "    color_hex = mcolors.rgb2hex(color)\n",
    "    return color_hex\n",
    "\n",
    "# Define a function to update the flow map based on the selected hour\n",
    "def update_flow_map(hour):\n",
    "    # Create a map centered at Palo Alto\n",
    "    m = folium.Map(location=[37.4419, -122.1430], zoom_start=10, tiles='CartoDB Dark_Matter', control_scale=True)\n",
    "\n",
    "    # Filter data based on the selected hour\n",
    "    filtered_data = data[data['Hour'] == hour]\n",
    "\n",
    "    # Add lines from origin to destination with color based on Events per Day\n",
    "    for _, row in filtered_data.iterrows():\n",
    "        origin = (row['latitude'], row['longitude'])\n",
    "        destination = (row['Latitude'], row['Longitude'])\n",
    "        events= row['Event_p_Day']\n",
    "\n",
    "        # Create a PolyLine connecting origin to destination with custom color\n",
    "        line = folium.PolyLine(\n",
    "            locations=[origin, destination],\n",
    "            color=map_events_to_color(events),\n",
    "            weight=1,\n",
    "            popup=f'Avg Events per Day for This Route: {events:.3f}' # Popup displaying events per day\n",
    "        )\n",
    "        line.add_to(m)\n",
    "\n",
    "        # Customize icon for the postal codes (origins)\n",
    "        icon = folium.DivIcon(\n",
    "            html=f'<i class=\"fa-solid fa-house-user fa-xl\" style=\"color: #ff0096;\"></i>',  # Use an icon from FontAwesome\n",
    "        )\n",
    "        folium.Marker(\n",
    "            location=origin,\n",
    "            icon=icon,\n",
    "            popup=row['postal_code']  # Set the postal code as the popup for the origin\n",
    "        ).add_to(m)\n",
    "\n",
    "        # Customize icon for the Stations (destinations)\n",
    "        icon = folium.DivIcon(\n",
    "            html=f'<i class=\"fa-solid fa-charging-station fa-xl\" style=\"color: #0084ff;\"></i>',  # Use an icon from FontAwesome\n",
    "        )\n",
    "        folium.Marker(\n",
    "            location=destination,\n",
    "            icon=icon,\n",
    "            popup=row['Station Name']  # Set the postal code as the popup for the destination\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Display the map\n",
    "    display(m)\n",
    "\n",
    "# Create a slider to change the hour of the day\n",
    "hour_slider = widgets.IntSlider(\n",
    "    min=int(data['Hour'].min()),\n",
    "    max=int(data['Hour'].max()),\n",
    "    step=1,\n",
    "    value=int(data['Hour'].min()),\n",
    "    description='Hour of the day',\n",
    "    continuous_update=True  # Update when the slider is moving\n",
    ")\n",
    "\n",
    "# Use widgets.interactive to connect the slider to the update_flow_map function\n",
    "interactive_plot = widgets.interactive(update_flow_map, hour=hour_slider)\n",
    "\n",
    "# Display the interactive plot\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-OV7yhactTmx"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from folium.plugins import MiniMap\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display\n",
    "\n",
    "# Your data and functions here...\n",
    "\n",
    "# Define a function to update the flow map based on the selected hour\n",
    "def update_flow_map(hour, save=False):\n",
    "    # Create a map centered at Palo Alto\n",
    "    m = folium.Map(location=[37.4419, -122.1430], zoom_start=10, tiles='CartoDB positron', control_scale=True)\n",
    "\n",
    "    # Filter data based on the selected hour\n",
    "    filtered_data = data[data['Hour'] == hour]\n",
    "\n",
    "    # Add lines from origin to destination with color based on Events per Day\n",
    "    for _, row in filtered_data.iterrows():\n",
    "        origin = (row['latitude'], row['longitude'])\n",
    "        destination = (row['Latitude'], row['Longitude'])\n",
    "        events = row['Event_p_Day']\n",
    "\n",
    "        # Create a PolyLine connecting origin to destination with custom color\n",
    "        line = folium.PolyLine(\n",
    "            locations=[origin, destination],\n",
    "            color=map_events_to_color(events),\n",
    "            weight=2,\n",
    "            popup=f'Avg Events per Day for This Route: {events:.3f}'  # Popup displaying events per day\n",
    "        )\n",
    "        line.add_to(m)\n",
    "\n",
    "        # Customize icon for the postal codes (origins)\n",
    "        icon = folium.DivIcon(\n",
    "            html=f'<i class=\"fa-solid fa-house-user fa-xl\" style=\"color: #ff0096;\"></i>',  # Use an icon from FontAwesome\n",
    "        )\n",
    "        folium.Marker(\n",
    "            location=origin,\n",
    "            icon=icon,\n",
    "            popup=row['postal_code']  # Set the postal code as the popup for the origin\n",
    "        ).add_to(m)\n",
    "\n",
    "        # Customize icon for the Stations (destinations)\n",
    "        icon = folium.DivIcon(\n",
    "            html=f'<i class=\"fa-solid fa-charging-station fa-xl\" style=\"color: #0084ff;\"></i>',  # Use an icon from FontAwesome\n",
    "        )\n",
    "        folium.Marker(\n",
    "            location=destination,\n",
    "            icon=icon,\n",
    "            popup=row['Station Name']  # Set the postal code as the popup for the destination\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Display the map\n",
    "    if save:\n",
    "        m.save(\"od_hourly_before.html\")\n",
    "    else:\n",
    "        display(m)\n",
    "\n",
    "# Create a slider to change the hour of the day\n",
    "hour_slider = widgets.IntSlider(\n",
    "    min=int(data['Hour'].min()),\n",
    "    max=int(data['Hour'].max()),\n",
    "    step=1,\n",
    "    value=int(data['Hour'].min()),\n",
    "    description='Hour of the day',\n",
    "    continuous_update=True  # Update when the slider is moving\n",
    ")\n",
    "\n",
    "# Use widgets.interactive to connect the slider to the update_flow_map function\n",
    "interactive_plot = widgets.interactive(update_flow_map, hour=hour_slider)\n",
    "\n",
    "# Display the interactive plot\n",
    "display(interactive_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ondocXKyJi8"
   },
   "outputs": [],
   "source": [
    "# Load origin-destination dataset\n",
    "data = OD_freq_afterr\n",
    "\n",
    "# Define a list of colors from yellow to red\n",
    "colors = ['#FFFF00', '#FF7700', '#FF0000']\n",
    "\n",
    "# Create a custom colormap from the list of colors\n",
    "cmap = LinearSegmentedColormap.from_list('custom', colors, N=256)\n",
    "\n",
    "# Normalize the colormap based on the range of your events values\n",
    "norm = mcolors.Normalize(vmin=data['Event_p_Day'].min(), vmax=data['Event_p_Day'].max())\n",
    "\n",
    "# Define a function to map events to colors\n",
    "def map_events_to_color(events):\n",
    "    color = cmap(norm(events))\n",
    "    color_hex = mcolors.rgb2hex(color)\n",
    "    return color_hex\n",
    "\n",
    "# Define a function to update the flow map based on the selected hour\n",
    "def update_flow_map(hour):\n",
    "    # Create a map centered at Palo Alto\n",
    "    m = folium.Map(location=[37.4419, -122.1430], zoom_start=10, tiles='CartoDB Dark_Matter', control_scale=True)\n",
    "\n",
    "    # Filter data based on the selected hour\n",
    "    filtered_data = data[data['Hour'] == hour]\n",
    "\n",
    "    # Add lines from origin to destination with color based on Events per Day\n",
    "    for _, row in filtered_data.iterrows():\n",
    "        origin = (row['latitude'], row['longitude'])\n",
    "        destination = (row['Latitude'], row['Longitude'])\n",
    "        events= row['Event_p_Day']\n",
    "\n",
    "        # Create a PolyLine connecting origin to destination with custom color\n",
    "        line = folium.PolyLine(\n",
    "            locations=[origin, destination],\n",
    "            color=map_events_to_color(events),\n",
    "            weight=1,\n",
    "            popup=f'Avg Events per Day for This Route: {events:.3f}' # Popup displaying events per day\n",
    "        )\n",
    "        line.add_to(m)\n",
    "\n",
    "        # Customize icon for the postal codes (origins)\n",
    "        icon = folium.DivIcon(\n",
    "            html=f'<i class=\"fa-solid fa-house-user fa-xl\" style=\"color: #ff0096;\"></i>',  # Use an icon from FontAwesome\n",
    "        )\n",
    "        folium.Marker(\n",
    "            location=origin,\n",
    "            icon=icon,\n",
    "            popup=row['postal_code']  # Set the postal code as the popup for the origin\n",
    "        ).add_to(m)\n",
    "\n",
    "        # Customize icon for the Stations (destinations)\n",
    "        icon = folium.DivIcon(\n",
    "            html=f'<i class=\"fa-solid fa-charging-station fa-xl\" style=\"color: #0084ff;\"></i>',  # Use an icon from FontAwesome\n",
    "        )\n",
    "        folium.Marker(\n",
    "            location=destination,\n",
    "            icon=icon,\n",
    "            popup=row['Station Name']  # Set the postal code as the popup for the destination\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Display the map\n",
    "    display(m)\n",
    "\n",
    "# Create a slider to change the hour of the day\n",
    "hour_slider = widgets.IntSlider(\n",
    "    min=int(data['Hour'].min()),\n",
    "    max=int(data['Hour'].max()),\n",
    "    step=1,\n",
    "    value=int(data['Hour'].min()),\n",
    "    description='Hour of the day',\n",
    "    continuous_update=True  # Update when the slider is moving\n",
    ")\n",
    "\n",
    "# Use widgets.interactive to connect the slider to the update_flow_map function\n",
    "interactive_plot = widgets.interactive(update_flow_map, hour=hour_slider)\n",
    "\n",
    "# Display the interactive plot\n",
    "display(interactive_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zsovQdtGvC8u"
   },
   "outputs": [],
   "source": [
    "# Load origin-destination dataset\n",
    "data = OD_freq_afterr\n",
    "\n",
    "# Define a list of colors from yellow to red\n",
    "colors = ['#FFFF00', '#FF7700', '#FF0000']\n",
    "\n",
    "# Create a custom colormap from the list of colors\n",
    "cmap = LinearSegmentedColormap.from_list('custom', colors, N=256)\n",
    "\n",
    "# Normalize the colormap based on the range of your events values\n",
    "norm = mcolors.Normalize(vmin=data['Event_p_Day'].min(), vmax=data['Event_p_Day'].max())\n",
    "\n",
    "# Define a function to map events to colors\n",
    "def map_events_to_color(events):\n",
    "    color = cmap(norm(events))\n",
    "    color_hex = mcolors.rgb2hex(color)\n",
    "    return color_hex\n",
    "\n",
    "# Define a function to update the flow map based on the selected hour\n",
    "def update_flow_map(hour, save=False):\n",
    "    # Create a map centered at Palo Alto\n",
    "    m = folium.Map(location=[37.4419, -122.1430], zoom_start=10, tiles='CartoDB positron', control_scale=True)\n",
    "\n",
    "    # Filter data based on the selected hour\n",
    "    filtered_data = data[data['Hour'] == hour]\n",
    "\n",
    "    # Add lines from origin to destination with color based on Events per Day\n",
    "    for _, row in filtered_data.iterrows():\n",
    "        origin = (row['latitude'], row['longitude'])\n",
    "        destination = (row['Latitude'], row['Longitude'])\n",
    "        events = row['Event_p_Day']\n",
    "\n",
    "        # Create a PolyLine connecting origin to destination with custom color\n",
    "        line = folium.PolyLine(\n",
    "            locations=[origin, destination],\n",
    "            color=map_events_to_color(events),\n",
    "            weight=2,\n",
    "            popup=f'Avg Events per Day for This Route: {events:.3f}'  # Popup displaying events per day\n",
    "        )\n",
    "        line.add_to(m)\n",
    "\n",
    "        # Customize icon for the postal codes (origins)\n",
    "        icon = folium.DivIcon(\n",
    "            html=f'<i class=\"fa-solid fa-house-user fa-xl\" style=\"color: #ff0096;\"></i>',  # Use an icon from FontAwesome\n",
    "        )\n",
    "        folium.Marker(\n",
    "            location=origin,\n",
    "            icon=icon,\n",
    "            popup=row['postal_code']  # Set the postal code as the popup for the origin\n",
    "        ).add_to(m)\n",
    "\n",
    "        # Customize icon for the Stations (destinations)\n",
    "        icon = folium.DivIcon(\n",
    "            html=f'<i class=\"fa-solid fa-charging-station fa-xl\" style=\"color: #0084ff;\"></i>',  # Use an icon from FontAwesome\n",
    "        )\n",
    "        folium.Marker(\n",
    "            location=destination,\n",
    "            icon=icon,\n",
    "            popup=row['Station Name']  # Set the postal code as the popup for the destination\n",
    "        ).add_to(m)\n",
    "\n",
    "    # Display the map\n",
    "    if save:\n",
    "        m.save(\"od_hourly_after.html\")\n",
    "    else:\n",
    "        display(m)\n",
    "\n",
    "# Create a slider to change the hour of the day\n",
    "hour_slider = widgets.IntSlider(\n",
    "    min=int(data['Hour'].min()),\n",
    "    max=int(data['Hour'].max()),\n",
    "    step=1,\n",
    "    value=int(data['Hour'].min()),\n",
    "    description='Hour of the day',\n",
    "    continuous_update=True  # Update when the slider is moving\n",
    ")\n",
    "\n",
    "# Use widgets.interactive to connect the slider to the update_flow_map function\n",
    "interactive_plot = widgets.interactive(update_flow_map, hour=hour_slider)\n",
    "\n",
    "# Display the interactive plot\n",
    "display(interactive_plot)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VmMv9oPVe4WN"
   },
   "outputs": [],
   "source": [
    "# Calculate Total Days over the Dataset\n",
    "total_days_beforee = len(beforee[\"Start Date\"].dt.date.unique())\n",
    "\n",
    "# Group by 'postal_code', 'Station Name' and their related coordinates, then calculate the Total Events\n",
    "pc_station_events_beforee = beforee.groupby([\"postal_code\", \"latitude\",\t\"longitude\", \"Station Name\", \"Latitude\", \"Longitude\"]).size().reset_index(name='Total Events')\n",
    "\n",
    "# Calculate Events per Day\n",
    "pc_station_events_beforee[\"Event_p_Day\"]= pc_station_events_beforee[\"Total Events\"]/ total_days_beforee\n",
    "\n",
    "# Filter the routes that their average event is more than once a day\n",
    "pc_station_freq_beforee= pc_station_events_beforee[pc_station_events_beforee[\"Event_p_Day\"]>= 1.5]\n",
    "\n",
    "# Check the first rows of pc_station_events\n",
    "pc_station_freq_beforee.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cLuNxRC2ztRh"
   },
   "outputs": [],
   "source": [
    "# Calculate Total Days over the Dataset\n",
    "total_days_after = len(afterr[\"Start Date\"].dt.date.unique())\n",
    "\n",
    "# Group by 'postal_code', 'Station Name' and their related coordinates, then calculate the Total Events\n",
    "pc_station_events_after = afterr.groupby([\"postal_code\", \"latitude\",\t\"longitude\", \"Station Name\", \"Latitude\", \"Longitude\"]).size().reset_index(name='Total Events')\n",
    "\n",
    "# Calculate Events per Day\n",
    "pc_station_events_after[\"Event_p_Day\"]= pc_station_events_after[\"Total Events\"]/ total_days_after\n",
    "\n",
    "# Filter the routes that their average event is more than once a day\n",
    "pc_station_freq_after= pc_station_events_after[pc_station_events_after[\"Event_p_Day\"]>= 1.5]\n",
    "\n",
    "# Check the first rows of pc_station_events\n",
    "pc_station_freq_after.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2vB0R3h_YDp3"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "import pandas as pd\n",
    "import folium.plugins as plugins\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import matplotlib.colors as mcolors\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kTH30oHxaifY"
   },
   "outputs": [],
   "source": [
    "# Load origin-destination dataset\n",
    "data = pc_station_freq_beforee\n",
    "\n",
    "# Define a list of colors from yellow to red\n",
    "colors = ['#FFFF00', '#FF7700', '#FF0000']\n",
    "\n",
    "# Create a custom colormap from the list of colors\n",
    "cmap = LinearSegmentedColormap.from_list('custom', colors, N=256)\n",
    "\n",
    "# Normalize the colormap based on the range of your events values\n",
    "norm = mcolors.Normalize(vmin=pc_station_freq_after['Event_p_Day'].min(), vmax=pc_station_freq_beforee['Event_p_Day'].max())\n",
    "\n",
    "# Define a function to map events to colors\n",
    "def map_events_to_color(events):\n",
    "    color = cmap(norm(events))\n",
    "    color_hex = mcolors.rgb2hex(color)\n",
    "    return color_hex\n",
    "\n",
    "# Create a map centered at Palo Alto\n",
    "m = folium.Map(location=[37.4419, -122.1430], zoom_start=13, tiles='CartoDB positron', control_scale=True)\n",
    "\n",
    "# Add lines from origin to destination with color based on Events per Day\n",
    "for _, row in data.iterrows():\n",
    "    origin = (row['latitude'], row['longitude'])\n",
    "    destination = (row['Latitude'], row['Longitude'])\n",
    "    events = row['Event_p_Day']\n",
    "\n",
    "    # Create a PolyLine connecting origin to destination with custom color\n",
    "    line = folium.PolyLine(\n",
    "        locations=[origin, destination],\n",
    "        color=map_events_to_color(events),\n",
    "        weight=3,\n",
    "        popup=f'Avg Events per Day for This Route: {events:.3f}'  # Popup displaying events per day\n",
    "    )\n",
    "    line.add_to(m)\n",
    "\n",
    "    # Customize icon for the postal codes (origins)\n",
    "    icon = folium.DivIcon(\n",
    "        html=f'<i class=\"fa-solid fa-house-user fa-l\" style=\"color: #ff0096;\"></i>',  # Use an icon from FontAwesome\n",
    "    )\n",
    "    folium.Marker(\n",
    "        location=origin,\n",
    "        icon=icon,\n",
    "        popup=row['postal_code']  # Set the postal code as the popup for the origin\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add the postal_code as a label to the left of the marker\n",
    "    folium.Marker(\n",
    "        origin,\n",
    "        icon=folium.DivIcon(icon_size=(5, 5), icon_anchor=(0, 0),  # Adjust icon_anchor to move text to the left\n",
    "            html=f'<div style=\"font-size: 6pt; width: 10000%; height: 0.01em; position: relative; bottom:-3px; left: 5px; color: #ff0096;\">{row[\"postal_code\"]}</div>'\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Customize icon for the Stations (destinations)\n",
    "    icon = folium.DivIcon(\n",
    "        html=f'<i class=\"fa-solid fa-charging-station\" style=\"color: #0084ff;\"></i>',  # Use an icon from FontAwesome\n",
    "    )\n",
    "    folium.Marker(\n",
    "        location=destination,\n",
    "        icon=icon,\n",
    "        popup=row['Station Name']  # Set the station name as the popup for the destination\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add the station name as a label to the left of the marker\n",
    "    folium.Marker(\n",
    "        destination,\n",
    "        icon=folium.DivIcon(icon_size=(5, 5), icon_anchor=(0, 30),  # Adjust icon_anchor to move text to the left\n",
    "            html=f'<div style=\"font-size: 6pt; width: 10000%; height: 0.01em; position: relative; bottom:-15px; left: -40px; color: #0084ff;\">{row[\"Station Name\"]}</div>'\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed;\n",
    "             bottom: 40px; left: 5px; width: auto; height: auto;\n",
    "             background-color: white; border:2px solid grey; z-index:9999; font-size:14px; padding: 10px;\">\n",
    "             <strong>Average Events per Day</strong><br>\n",
    "             <strong>Before Policy Intervention</strong><br>\n",
    "             <i class=\"fa fa-square fa-1x\" style=\"color:#FFFF00\"></i>&nbsp;Low (1.5 - 3 Events)<br>\n",
    "             <i class=\"fa fa-square fa-1x\" style=\"color:#FF7700\"></i>&nbsp;Medium (3 - 5 Events)<br>\n",
    "             <i class=\"fa fa-square fa-1x\" style=\"color:#FF0000\"></i>&nbsp;High (More Than 5 Events)\n",
    "  </div>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Save the map as an HTML file for inclusion in the paper\n",
    "m.save(\"flowmap_of_freq_before.html\")\n",
    "\n",
    "# Display the map\n",
    "display(m)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L6s0AWhgYjEu"
   },
   "outputs": [],
   "source": [
    "# Load origin-destination dataset\n",
    "data = pc_station_freq_after\n",
    "\n",
    "# Define a function to map events to colors\n",
    "def map_events_to_color(events):\n",
    "    color = cmap(norm(events))\n",
    "    color_hex = mcolors.rgb2hex(color)\n",
    "    return color_hex\n",
    "\n",
    "# Create a map centered at Palo Alto\n",
    "m = folium.Map(location=[37.4419, -122.1430], zoom_start=13, tiles='CartoDB positron', control_scale=True)\n",
    "\n",
    "# Add lines from origin to destination with color based on Events per Day\n",
    "for _, row in data.iterrows():\n",
    "    origin = (row['latitude'], row['longitude'])\n",
    "    destination = (row['Latitude'], row['Longitude'])\n",
    "    events = row['Event_p_Day']\n",
    "\n",
    "    # Create a PolyLine connecting origin to destination with custom color\n",
    "    line = folium.PolyLine(\n",
    "        locations=[origin, destination],\n",
    "        color=map_events_to_color(events),\n",
    "        weight=3,\n",
    "        popup=f'Avg Events per Day for This Route: {events:.3f}'  # Popup displaying events per day\n",
    "    )\n",
    "    line.add_to(m)\n",
    "\n",
    "    # Customize icon for the postal codes (origins)\n",
    "    icon = folium.DivIcon(\n",
    "        html=f'<i class=\"fa-solid fa-house-user fa-l\" style=\"color: #ff0096;\"></i>',  # Use an icon from FontAwesome\n",
    "    )\n",
    "    folium.Marker(\n",
    "        location=origin,\n",
    "        icon=icon,\n",
    "        popup=row['postal_code']  # Set the postal code as the popup for the origin\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add the postal_code as a label to the left of the marker\n",
    "    folium.Marker(\n",
    "        origin,\n",
    "        icon=folium.DivIcon(icon_size=(5, 5), icon_anchor=(0, 0),  # Adjust icon_anchor to move text to the left\n",
    "            html=f'<div style=\"font-size: 6pt; width: 10000%; height: 0.01em; position: relative; bottom:-3px; left: 5px; color: #ff0096;\">{row[\"postal_code\"]}</div>'\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Customize icon for the Stations (destinations)\n",
    "    icon = folium.DivIcon(\n",
    "        html=f'<i class=\"fa-solid fa-charging-station\" style=\"color: #0084ff;\"></i>',  # Use an icon from FontAwesome\n",
    "    )\n",
    "    folium.Marker(\n",
    "        location=destination,\n",
    "        icon=icon,\n",
    "        popup=row['Station Name']  # Set the postal code as the popup for the destination\n",
    "    ).add_to(m)\n",
    "\n",
    "    # Add the station name as a label to the left of the marker\n",
    "    folium.Marker(\n",
    "        destination,\n",
    "        icon=folium.DivIcon(icon_size=(5, 5), icon_anchor=(0, 30),  # Adjust icon_anchor to move text to the left\n",
    "            html=f'<div style=\"font-size: 6pt; width: 10000%; height: 0.01em; position: relative; bottom:-15px; left: -40px; color: #0084ff;\">{row[\"Station Name\"]}</div>'\n",
    "        )\n",
    "    ).add_to(m)\n",
    "\n",
    "legend_html = '''\n",
    "<div style=\"position: fixed;\n",
    "             bottom: 40px; left: 5px; width: auto; height: auto;\n",
    "             background-color: white; border:2px solid grey; z-index:9999; font-size:14px; padding: 10px;\">\n",
    "             <strong>Average Events per Day</strong><br>\n",
    "             <strong>After Policy Intervention</strong><br>\n",
    "             <i class=\"fa fa-square fa-1x\" style=\"color:#FFFF00\"></i>&nbsp;Low (1.5 - 3 Events)<br>\n",
    "             <i class=\"fa fa-square fa-1x\" style=\"color:#FF7700\"></i>&nbsp;Medium (3 - 5 Events)<br>\n",
    "             <i class=\"fa fa-square fa-1x\" style=\"color:#FF0000\"></i>&nbsp;High (More Than 5 Events)\n",
    "  </div>\n",
    "'''\n",
    "m.get_root().html.add_child(folium.Element(legend_html))\n",
    "\n",
    "# Save the map as an HTML file for inclusion in the paper\n",
    "m.save(\"flowmap_of_freq_after.html\")\n",
    "\n",
    "# Display the map\n",
    "display(m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5PGpeVsTfTaN"
   },
   "outputs": [],
   "source": [
    "# Define the Kepler.gl map configuration JSON\n",
    "map_config = {\n",
    "    \"version\": \"v1\",\n",
    "    \"config\": {\n",
    "        \"visState\": {\n",
    "            \"filters\": [],\n",
    "            \"legend\": True,\n",
    "            \"layers\": [\n",
    "                {\n",
    "                    \"id\": \"iwhexfq\",\n",
    "                    \"type\": \"point\",\n",
    "                    \"config\": {\n",
    "                        \"dataId\": \"flow_data\",\n",
    "                        \"label\": \"Stations\",\n",
    "                        \"color\": [218, 112, 191],\n",
    "                        \"highlightColor\": [252, 242, 26, 255],\n",
    "                        \"columns\": {\n",
    "                            \"lat\": \"Latitude\",\n",
    "                            \"lng\": \"Longitude\",\n",
    "                            \"altitude\": None\n",
    "                        },\n",
    "                        \"isVisible\": True,\n",
    "                        \"visConfig\": {\n",
    "                            \"radius\": 20,\n",
    "                            \"fixedRadius\": False,\n",
    "                            \"opacity\": 1,\n",
    "                            \"outline\": False,\n",
    "                            \"thickness\": 2,\n",
    "                            \"strokeColor\": None,\n",
    "                            \"colorRange\": {\n",
    "                                \"name\": \"Global Warming\",\n",
    "                                \"type\": \"sequential\",\n",
    "                                \"category\": \"Uber\",\n",
    "                                \"colors\": [\"#5A1846\", \"#900C3F\", \"#C70039\", \"#E3611C\", \"#F1920E\", \"#FFC300\"]\n",
    "                            },\n",
    "                            \"strokeColorRange\": {\n",
    "                                \"name\": \"Global Warming\",\n",
    "                                \"type\": \"sequential\",\n",
    "                                \"category\": \"Uber\",\n",
    "                                \"colors\": [\"#5A1846\", \"#900C3F\", \"#C70039\", \"#E3611C\", \"#F1920E\", \"#FFC300\"]\n",
    "                            },\n",
    "                            \"radiusRange\": [0, 50],\n",
    "                            \"filled\": True\n",
    "                        },\n",
    "                        \"hidden\": False,\n",
    "                        \"textLabel\": [\n",
    "                            {\n",
    "                                \"field\": {\n",
    "                                    \"name\": \"Station Name\",\n",
    "                                    \"type\": \"string\"\n",
    "                                },\n",
    "                                \"color\": [218, 112, 191],\n",
    "                                \"size\": 12,\n",
    "                                \"offset\": [0, 0],\n",
    "                                \"anchor\": \"end\",\n",
    "                                \"alignment\": \"center\"\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    \"visualChannels\": {\n",
    "                        \"colorField\": None,\n",
    "                        \"colorScale\": \"quantile\",\n",
    "                        \"strokeColorField\": None,\n",
    "                        \"strokeColorScale\": \"quantile\",\n",
    "                        \"sizeField\": None,\n",
    "                        \"sizeScale\": \"linear\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"id\": \"897h7as\",\n",
    "                    \"type\": \"point\",\n",
    "                    \"config\": {\n",
    "                        \"dataId\": \"flow_data\",\n",
    "                        \"label\": \"Driver Zip Codes\",\n",
    "                        \"color\": [71, 211, 217],\n",
    "                        \"highlightColor\": [252, 242, 26, 255],\n",
    "                        \"columns\": {\n",
    "                            \"lat\": \"latitude\",\n",
    "                            \"lng\": \"longitude\",\n",
    "                            \"altitude\": None\n",
    "                        },\n",
    "                        \"isVisible\": True,\n",
    "                        \"visConfig\": {\n",
    "                            \"radius\": 20,\n",
    "                            \"fixedRadius\": False,\n",
    "                            \"opacity\": 1,\n",
    "                            \"outline\": False,\n",
    "                            \"thickness\": 2,\n",
    "                            \"strokeColor\": None,\n",
    "                            \"colorRange\": {\n",
    "                                \"name\": \"Global Warming\",\n",
    "                                \"type\": \"sequential\",\n",
    "                                \"category\": \"Uber\",\n",
    "                                \"colors\": [\"#5A1846\", \"#900C3F\", \"#C70039\", \"#E3611C\", \"#F1920E\", \"#FFC300\"]\n",
    "                            },\n",
    "                            \"strokeColorRange\": {\n",
    "                                \"name\": \"Global Warming\",\n",
    "                                \"type\": \"sequential\",\n",
    "                                \"category\": \"Uber\",\n",
    "                                \"colors\": [\"#5A1846\", \"#900C3F\", \"#C70039\", \"#E3611C\", \"#F1920E\", \"#FFC300\"]\n",
    "                            },\n",
    "                            \"radiusRange\": [0, 50],\n",
    "                            \"filled\": True\n",
    "                        },\n",
    "                        \"hidden\": False,\n",
    "                        \"textLabel\": [\n",
    "                            {\n",
    "                                \"field\": {\n",
    "                                    \"name\": \"postal_code\",\n",
    "                                    \"type\": \"integer\"\n",
    "                                },\n",
    "                                \"color\": [117, 222, 227],\n",
    "                                \"size\": 15,\n",
    "                                \"offset\": [0, 0],\n",
    "                                \"anchor\": \"middle\",\n",
    "                                \"alignment\": \"bottom\"\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    \"visualChannels\": {\n",
    "                        \"colorField\": None,\n",
    "                        \"colorScale\": \"quantile\",\n",
    "                        \"strokeColorField\": None,\n",
    "                        \"strokeColorScale\": \"quantile\",\n",
    "                        \"sizeField\": None,\n",
    "                        \"sizeScale\": \"linear\"\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    \"id\": \"rbwl8u\",\n",
    "                    \"type\": \"arc\",\n",
    "                    \"config\": {\n",
    "                        \"dataId\": \"flow_data\",\n",
    "                        \"label\": \"Flow\",\n",
    "                        \"color\": [136, 87, 44],\n",
    "                        \"highlightColor\": [252, 242, 26, 255],\n",
    "                        \"columns\": {\n",
    "                            \"lat0\": \"latitude\",\n",
    "                            \"lng0\": \"longitude\",\n",
    "                            \"lat1\": \"Latitude\",\n",
    "                            \"lng1\": \"Longitude\"\n",
    "                        },\n",
    "                        \"isVisible\": True,\n",
    "                        \"visConfig\": {\n",
    "                            \"opacity\": 0.8,\n",
    "                            \"thickness\": 5,\n",
    "                            \"colorRange\": {\n",
    "                                \"name\": \"ColorBrewer YlOrRd-3\",\n",
    "                                \"type\": \"sequential\",\n",
    "                                \"category\": \"ColorBrewer\",\n",
    "                                \"colors\": [\"#ffeda0\", \"#feb24c\", \"#f03b20\"]\n",
    "                            },\n",
    "                            \"sizeRange\": [0, 10],\n",
    "                            \"targetColor\": None\n",
    "                        },\n",
    "                        \"hidden\": False,\n",
    "                        \"textLabel\": [\n",
    "                            {\n",
    "                                \"field\": None,\n",
    "                                \"color\": [255, 255, 255],\n",
    "                                \"size\": 18,\n",
    "                                \"offset\": [0, 0],\n",
    "                                \"anchor\": \"start\",\n",
    "                                \"alignment\": \"center\"\n",
    "                            }\n",
    "                        ]\n",
    "                    },\n",
    "                    \"visualChannels\": {\n",
    "                        \"colorField\": {\n",
    "                            \"name\": \"Total Events\",\n",
    "                            \"type\": \"integer\"\n",
    "                        },\n",
    "                        \"colorScale\": \"quantile\",\n",
    "                        \"sizeField\": None,\n",
    "                        \"sizeScale\": \"linear\"\n",
    "                    }\n",
    "                }\n",
    "            ],\n",
    "            \"interactionConfig\": {\n",
    "                \"tooltip\": {\n",
    "                    \"fieldsToShow\": {\n",
    "                        \"flow_data\": [\n",
    "                            {\"name\": \"postal_code\", \"format\": None},\n",
    "                            {\"name\": \"Station Name\", \"format\": None},\n",
    "                            {\"name\": \"Event_p_Day\", \"format\": None}\n",
    "                        ]\n",
    "                    },\n",
    "                    \"compareMode\": False,\n",
    "                    \"compareType\": \"absolute\",\n",
    "                    \"enabled\": True\n",
    "                },\n",
    "                \"brush\": {\n",
    "                    \"size\": 0.5,\n",
    "                    \"enabled\": False\n",
    "                },\n",
    "                \"geocoder\": {\n",
    "                    \"enabled\": False\n",
    "                },\n",
    "                \"coordinate\": {\n",
    "                    \"enabled\": False\n",
    "                }\n",
    "            }\n",
    "        },\n",
    "        \"mapState\": {\n",
    "            \"bearing\": 24,\n",
    "            \"dragRotate\": True,\n",
    "            \"latitude\": 37.435048277614534,\n",
    "            \"longitude\": -122.12024206621042,\n",
    "            \"pitch\": 50,\n",
    "            \"zoom\": 12,\n",
    "            \"isSplit\": False\n",
    "        },\n",
    "        \"mapStyle\": {\n",
    "            \"styleType\": \"muted_night\",\n",
    "            \"topLayerGroups\": {},\n",
    "            \"visibleLayerGroups\": {\n",
    "                \"label\": True,\n",
    "                \"road\": True,\n",
    "                \"border\": False,\n",
    "                \"building\": True,\n",
    "                \"water\": True,\n",
    "                \"land\": True,\n",
    "                \"3d building\": False\n",
    "            },\n",
    "            \"threeDBuildingColor\": [9.665468314072013, 17.18305478057247, 31.1442867897876],\n",
    "            \"mapStyles\": {}\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1H4tWTApe4WP"
   },
   "outputs": [],
   "source": [
    "# Initialize Kepler.gl map instance with custom configuration\n",
    "flowmap = KeplerGl(height=600, config=map_config, show_docs=False)\n",
    "\n",
    "# Add data to the map\n",
    "flowmap.add_data(data=pc_station_freq_beforee, name='flow_data')\n",
    "\n",
    "# Display the map\n",
    "flowmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQ-sIxiyY41i"
   },
   "outputs": [],
   "source": [
    "from keplergl import KeplerGl\n",
    "\n",
    "# Initialize Kepler.gl map instance with custom configuration\n",
    "flowmap = KeplerGl(height=600, config=map_config, show_docs=False)\n",
    "\n",
    "# Add data to the map\n",
    "flowmap.add_data(data=pc_station_freq_beforee, name='flow_data')\n",
    "\n",
    "# Create a function to generate the HTML content of the map\n",
    "def generate_keplergl_html(map_obj):\n",
    "    map_obj.save_to_html(file_name=\"flowmap_of_freq_kepler_before.html\")\n",
    "\n",
    "# Call the function to generate the HTML content and save it to a file\n",
    "generate_keplergl_html(flowmap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YkUumM7g0nZS"
   },
   "outputs": [],
   "source": [
    "# Initialize Kepler.gl map instance with custom configuration\n",
    "flowmap = KeplerGl(height=600, config=map_config, show_docs=False)\n",
    "\n",
    "# Add data to the map\n",
    "flowmap.add_data(data=pc_station_freq_after, name='flow_data')\n",
    "\n",
    "# Display the map\n",
    "flowmap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C6mm7kjJZlaI"
   },
   "outputs": [],
   "source": [
    "# Initialize Kepler.gl map instance with custom configuration\n",
    "flowmap = KeplerGl(height=600, config=map_config, show_docs=False)\n",
    "\n",
    "# Add data to the map\n",
    "flowmap.add_data(data=pc_station_freq_after, name='flow_data')\n",
    "\n",
    "# Create a function to generate the HTML content of the map\n",
    "def generate_keplergl_html(map_obj, file_name):\n",
    "    map_obj.save_to_html(file_name=file_name)\n",
    "\n",
    "# Call the function to generate the HTML content and save it to a file\n",
    "generate_keplergl_html(flowmap, \"flowmap_of_freq_kepler_after.html\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LlDIeTlYe4WR"
   },
   "source": [
    "### **4.2.5. Behavioral Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f3rfnYAge4WR"
   },
   "outputs": [],
   "source": [
    "# Resample the charging events based on the start time into 30-minute intervals and calculate the mean events for each interval\n",
    "hh_start_ev_before= before.resample(\"30min\", on= \"Start Date\").size()\n",
    "hh_start_gp_before = hh_start_ev_before.groupby(hh_start_ev_before.index.time).mean()\n",
    "\n",
    "# Resample the charging events based on the end time into 30-minute intervals and calculate the mean events for each interval\n",
    "hh_end_ev_before= before.resample(\"30min\", on= \"End Date\").size()\n",
    "hh_end_gp_before= hh_end_ev_before.groupby(hh_end_ev_before.index.time).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aoj_wkNh-hvb"
   },
   "outputs": [],
   "source": [
    "# Resample the charging events based on the start time into 30-minute intervals and calculate the mean events for each interval\n",
    "hh_start_ev_after= after.resample(\"30min\", on= \"Start Date\").size()\n",
    "hh_start_gp_after = hh_start_ev_after.groupby(hh_start_ev_after.index.time).mean()\n",
    "\n",
    "# Resample the charging events based on the end time into 30-minute intervals and calculate the mean events for each interval\n",
    "hh_end_ev_after= after.resample(\"30min\", on= \"End Date\").size()\n",
    "hh_end_gp_after= hh_end_ev_after.groupby(hh_end_ev_after.index.time).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TCTcwy4Fe4WS"
   },
   "outputs": [],
   "source": [
    "# Create subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(10, 6))\n",
    "\n",
    "# Set the default style for plot\n",
    "plt.style.use('default')\n",
    "\n",
    "# Plot interval_avg on the first subplot\n",
    "hh_start_gp_before.plot(kind='bar', color='steelblue', ax=ax1, alpha=0.5, label='Plug-in Event before policy')\n",
    "hh_end_gp_before.plot(kind='bar', color='indianred', ax=ax1, alpha=0.5, label='Plug-out Event before policy')\n",
    "ax1.set_xlabel('Time of the Day')\n",
    "ax1.set_ylabel('Average Number of Events')\n",
    "ax1.legend(loc='upper left')\n",
    "\n",
    "# Plot hh_end_gp on the second subplot\n",
    "hh_start_gp_after.plot(kind='bar', color='steelblue', ax=ax2, alpha=0.5, label='Plug-in Event after policy')\n",
    "hh_end_gp_after.plot(kind='bar', color='indianred', ax=ax2, alpha=0.5, label='Plug-out Event after policy')\n",
    "ax2.set_xlabel('Time of the Day')\n",
    "ax2.set_ylabel('Average Number of Events')\n",
    "ax2.legend(loc='upper left')\n",
    "\n",
    "# Add gridlines for the y-axis\n",
    "ax1.yaxis.grid(True, linestyle='dotted', alpha=0.5)\n",
    "ax2.yaxis.grid(True, linestyle='dotted', alpha=0.5)\n",
    "\n",
    "# Add a common title for the entire figure\n",
    "plt.suptitle('Daily Plug-in / Plug-out Behavior', fontsize=15)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WQfRzxNWe4WS"
   },
   "outputs": [],
   "source": [
    "# Group by 'User Segments' and resample by 30 minutes on 'Start Date', then calculate the size\n",
    "hh_start_user_before= before.groupby(\"User Segment\").resample(\"30min\", on= \"Start Date\").size().reset_index(name='user_count')\n",
    "\n",
    "# Group by 'User Segments' and 'Start Date' by using the .dt accessor to extract the time component and calculate the mean of user_count\n",
    "hh_start_user_gp_before = hh_start_user_before.groupby([\"User Segment\", hh_start_user_before[\"Start Date\"].dt.time])[\"user_count\"].mean().reset_index(name='Plug-in')\n",
    "\n",
    "# Do the same operations for 'End Date' column\n",
    "hh_end_user_before= before.groupby(\"User Segment\").resample(\"30min\", on= \"End Date\").size().reset_index(name='user_count')\n",
    "hh_end_user_gp_before = hh_end_user_before.groupby([\"User Segment\", hh_end_user_before[\"End Date\"].dt.time])[\"user_count\"].mean().reset_index(name='Plug-out')\n",
    "\n",
    "# Define fig and axes\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 10))\n",
    "\n",
    "# Define plot style\n",
    "plt.style.use('default')\n",
    "\n",
    "# Define the order of the User Segments\n",
    "segment_order = [\"Frequent\", \"Occasional\", \"Infrequent\", \"Passing\", \"Unknown\"]\n",
    "\n",
    "# Label the xaxis by half hour intervals\n",
    "x_labels = [f'{hour:02d}:{minute:02d}' for hour in range(24) for minute in (0, 30)]\n",
    "\n",
    "# Enomerate over segments and generate the plot\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(segment_order):\n",
    "        segment = segment_order[i]\n",
    "        start_data = hh_start_user_gp_before[hh_start_user_gp_before[\"User Segment\"] == segment]\n",
    "        start_data.plot(kind='bar', color='steelblue', alpha=0.5, label='Plug-in Event', ax=ax)\n",
    "        end_data = hh_end_user_gp_before[hh_end_user_gp_before[\"User Segment\"] == segment]\n",
    "        end_data.plot(kind='bar', color='indianred', alpha=0.5, label='Plug-out Event', ax=ax)\n",
    "        ax.set(xlabel=\"Time of the Day\", xticks=range(48), xticklabels=x_labels, ylabel=\"Average Number of Events\", title=segment)\n",
    "        ax.yaxis.grid(True, linestyle='dotted', alpha=0.5)\n",
    "        ax.legend(loc='upper left')\n",
    "\n",
    "# Add a common title for the entire figure and show the plot\n",
    "plt.suptitle('Daily Plug-in / Plug-out Behavior by User Segment', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AZOR72gsEdC0"
   },
   "outputs": [],
   "source": [
    "# Group by 'User Segments' and resample by 30 minutes on 'Start Date', then calculate the size\n",
    "hh_start_user_after= after.groupby(\"User Segment\").resample(\"30min\", on= \"Start Date\").size().reset_index(name='user_count')\n",
    "\n",
    "# Group by 'User Segments' and 'Start Date' by using the .dt accessor to extract the time component and calculate the mean of user_count\n",
    "hh_start_user_gp_after = hh_start_user_after.groupby([\"User Segment\", hh_start_user_after[\"Start Date\"].dt.time])[\"user_count\"].mean().reset_index(name='Plug-in')\n",
    "\n",
    "# Do the same operations for 'End Date' column\n",
    "hh_end_user_after= after.groupby(\"User Segment\").resample(\"30min\", on= \"End Date\").size().reset_index(name='user_count')\n",
    "hh_end_user_gp_after = hh_end_user_after.groupby([\"User Segment\", hh_end_user_after[\"End Date\"].dt.time])[\"user_count\"].mean().reset_index(name='Plug-out')\n",
    "\n",
    "# Define fig and axes\n",
    "fig, axes = plt.subplots(nrows=3, ncols=2, figsize=(15, 10))\n",
    "\n",
    "# Define plot style\n",
    "plt.style.use('default')\n",
    "\n",
    "# Define the order of the User Segments\n",
    "segment_order = [\"Frequent\", \"Occasional\", \"Infrequent\", \"Passing\", \"Unknown\"]\n",
    "\n",
    "# Label the xaxis by half hour intervals\n",
    "x_labels = [f'{hour:02d}:{minute:02d}' for hour in range(24) for minute in (0, 30)]\n",
    "\n",
    "# Enomerate over segments and generate the plot\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(segment_order):\n",
    "        segment = segment_order[i]\n",
    "        start_data = hh_start_user_gp_after[hh_start_user_gp_after[\"User Segment\"] == segment]\n",
    "        start_data.plot(kind='bar', color='steelblue', alpha=0.5, label='Plug-in Event', ax=ax)\n",
    "        end_data = hh_end_user_gp_after[hh_end_user_gp_after[\"User Segment\"] == segment]\n",
    "        end_data.plot(kind='bar', color='indianred', alpha=0.5, label='Plug-out Event', ax=ax)\n",
    "        ax.set(xlabel=\"Time of the Day\", xticks=range(48), xticklabels=x_labels, ylabel=\"Average Number of Events\", title=segment)\n",
    "        ax.yaxis.grid(True, linestyle='dotted', alpha=0.5)\n",
    "        ax.legend(loc='upper left')\n",
    "\n",
    "# Add a common title for the entire figure and show the plot\n",
    "plt.suptitle('Daily Plug-in / Plug-out Behavior by User Segment', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_dBeN0Xee4WS"
   },
   "outputs": [],
   "source": [
    "# Group by 'Station Name' and resample by 30 minutes on 'Start Date', then calculate the size\n",
    "hh_start_station_before= before.groupby(\"Station Name\").resample(\"30min\", on= \"Start Date\").size().reset_index(name='user_count')\n",
    "\n",
    "# Group by 'Staton Name' and 'Start Date' by using the .dt accessor to extract the time component and calculate the mean of user_count\n",
    "hh_start_station_gp_before = hh_start_station_before.groupby([\"Station Name\", hh_start_station_before[\"Start Date\"].dt.time])[\"user_count\"].mean().reset_index(name='Plug-in')\n",
    "\n",
    "# Do the same operations for 'End Date' column\n",
    "hh_end_station_before= before.groupby(\"Station Name\").resample(\"30min\", on= \"End Date\").size().reset_index(name='user_count')\n",
    "hh_end_station_gp_before = hh_end_station_before.groupby([\"Station Name\", hh_end_station_before[\"End Date\"].dt.time])[\"user_count\"].mean().reset_index(name='Plug-out')\n",
    "\n",
    "# Define fig and axes\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(20, 10))\n",
    "\n",
    "# Define plot style\n",
    "plt.style.use('default')\n",
    "\n",
    "# Define the Station Names list\n",
    "stations = before[\"Station Name\"].unique()\n",
    "\n",
    "# Label the xaxis by half hour intervals\n",
    "x_labels = [f'{hour:02d}:{minute:02d}' for hour in range(24) for minute in (0, 30)]\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(stations):\n",
    "        station = stations[i]\n",
    "        start_data = hh_start_station_gp_before[hh_start_station_gp_before[\"Station Name\"] == station]\n",
    "        start_data.plot(kind='bar', color='steelblue', alpha=0.5, label='Plug-in Event', ax=ax)\n",
    "        end_data = hh_end_station_gp_before[hh_end_station_gp_before[\"Station Name\"] == station]\n",
    "        end_data.plot(kind='bar', color='indianred', alpha=0.5, label='Plug-out Event', ax=ax)\n",
    "        ax.set(xlabel=\"Time of the Day\", xticks=range(48), xticklabels=x_labels, ylabel=\"Average Number of Events\", title=station)\n",
    "        ax.yaxis.grid(True, linestyle='dotted', alpha=0.5)\n",
    "        ax.legend(loc='upper left')\n",
    "\n",
    "# Add a common title for the entire figure\n",
    "plt.suptitle('Daily Plug-in / Plug-out Behavior by Charging Station', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3KE3_AmlFQBU"
   },
   "outputs": [],
   "source": [
    "# Group by 'Station Name' and resample by 30 minutes on 'Start Date', then calculate the size\n",
    "hh_start_station_after= after.groupby(\"Station Name\").resample(\"30min\", on= \"Start Date\").size().reset_index(name='user_count')\n",
    "\n",
    "# Group by 'Staton Name' and 'Start Date' by using the .dt accessor to extract the time component and calculate the mean of user_count\n",
    "hh_start_station_gp_after = hh_start_station_after.groupby([\"Station Name\", hh_start_station_after[\"Start Date\"].dt.time])[\"user_count\"].mean().reset_index(name='Plug-in')\n",
    "\n",
    "# Do the same operations for 'End Date' column\n",
    "hh_end_station_after= after.groupby(\"Station Name\").resample(\"30min\", on= \"End Date\").size().reset_index(name='user_count')\n",
    "hh_end_station_gp_after = hh_end_station_after.groupby([\"Station Name\", hh_end_station_after[\"End Date\"].dt.time])[\"user_count\"].mean().reset_index(name='Plug-out')\n",
    "\n",
    "# Define fig and axes\n",
    "fig, axes = plt.subplots(nrows=3, ncols=3, figsize=(20, 10))\n",
    "\n",
    "# Define plot style\n",
    "plt.style.use('default')\n",
    "\n",
    "# Define the Station Names list\n",
    "stations = after[\"Station Name\"].unique()\n",
    "\n",
    "# Label the xaxis by half hour intervals\n",
    "x_labels = [f'{hour:02d}:{minute:02d}' for hour in range(24) for minute in (0, 30)]\n",
    "\n",
    "for i, ax in enumerate(axes.flatten()):\n",
    "    if i < len(stations):\n",
    "        station = stations[i]\n",
    "        start_data = hh_start_station_gp_after[hh_start_station_gp_after[\"Station Name\"] == station]\n",
    "        start_data.plot(kind='bar', color='steelblue', alpha=0.5, label='Plug-in Event', ax=ax)\n",
    "        end_data = hh_end_station_gp_after[hh_end_station_gp_after[\"Station Name\"] == station]\n",
    "        end_data.plot(kind='bar', color='indianred', alpha=0.5, label='Plug-out Event', ax=ax)\n",
    "        ax.set(xlabel=\"Time of the Day\", xticks=range(48), xticklabels=x_labels, ylabel=\"Average Number of Events\", title=station)\n",
    "        ax.yaxis.grid(True, linestyle='dotted', alpha=0.5)\n",
    "        ax.legend(loc='upper left')\n",
    "\n",
    "# Add a common title for the entire figure\n",
    "plt.suptitle('Daily Plug-in / Plug-out Behavior by Charging Station', fontsize=15)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_eDUiyVte4WT"
   },
   "outputs": [],
   "source": [
    "# Resample the charging events based on the start time into 30-minute intervals and calculate the mean duration for each interval\n",
    "hh_duration_before = before.resample(\"H\", on= \"Start Date\")[\"Duration_min\"].mean()\n",
    "hh_duration_after = after.resample(\"H\", on= \"Start Date\")[\"Duration_min\"].mean()\n",
    "\n",
    "\n",
    "# Group by 30-minute intervals and calculate the mean of Duration_min\n",
    "hh_duration_gp_before = hh_duration_before.groupby(hh_duration_before.index.time).mean()\n",
    "hh_duration_gp_after = hh_duration_after.groupby(hh_duration_after.index.time).mean()\n",
    "\n",
    "# Create a heatmap\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Set the default style for plot\n",
    "plt.style.use('default')\n",
    "\n",
    "# Plot hh_duration_gp\n",
    "hh_duration_gp_before.plot(kind='bar', color='steelblue', alpha=0.7, label='Before Policy Intervention')\n",
    "hh_duration_gp_after.plot(kind='bar', color='indianred', alpha=0.7, label='After Policy Intervention')\n",
    "plt.xlabel('Time of the Day')\n",
    "plt.ylabel('Average Charging Duration')\n",
    "\n",
    "# Add gridlines for the y-axis\n",
    "plt.gca().yaxis.grid(True, linestyle='dotted', alpha=0.5)  # Corrected line\n",
    "\n",
    "# Add a title for the plot\n",
    "plt.title('Daily Charging Duration Behavior by Plug-in Time', fontsize=15)\n",
    "\n",
    "plt.legend(loc='best')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z5FO5LotaGLC"
   },
   "outputs": [],
   "source": [
    "# Create a figure and set the figure size\n",
    "fig, ax = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "# Plot hh_duration_gp_before and hh_duration_gp_after as side-by-side bars\n",
    "bar_width = 0.4\n",
    "x = range(len(hh_duration_gp_before))\n",
    "ax.bar(x, hh_duration_gp_before, width=bar_width, color='dodgerblue', label='Before Policy Intervention', linewidth=0.5)\n",
    "ax.bar([i + bar_width for i in x], hh_duration_gp_after, width=bar_width, color='orange', label='After Policy Intervention', linewidth=0.5)\n",
    "\n",
    "# Set the x-axis labels to the time intervals\n",
    "x_labels = [str(time) for time in hh_duration_gp_before.index]\n",
    "ax.set_xticks([i + bar_width/2 for i in x])\n",
    "ax.set_xticklabels(x_labels, rotation=90, ha='right')\n",
    "\n",
    "# Set axis labels and title\n",
    "ax.set_xlabel('Time of the Day')\n",
    "ax.set_ylabel('Average Event Duration')\n",
    "ax.set_title('Average Event Duration by Plug-in Time', fontsize=15)\n",
    "\n",
    "# Add gridlines for the y-axis\n",
    "ax.yaxis.grid(True, linestyle='dotted', alpha=0.5)\n",
    "\n",
    "# Add legend\n",
    "ax.legend(loc='best')\n",
    "\n",
    "# Improve layout and spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot as an image for the paper\n",
    "plt.savefig(\"duration_behavior.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the plot (optional)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNDq6286e_WD"
   },
   "outputs": [],
   "source": [
    "# Calculate the sum of energy consumption for start times in 30-minute intervals and obtain the mean for each interval\n",
    "en_start_ev_before = before.resample(\"H\", on=\"Start Date\")[\"Energy (kWh)\"].sum()\n",
    "en_start_gp_before = en_start_ev_before.groupby(en_start_ev_before.index.time).mean()\n",
    "\n",
    "# Calculate the sum of energy consumption for end times in 30-minute intervals and obtain the mean for each interval\n",
    "en_end_ev_before = before.resample(\"H\", on=\"End Date\")[\"Energy (kWh)\"].sum()\n",
    "en_end_gp_before = en_end_ev_before.groupby(en_end_ev_before.index.time).mean()\n",
    "\n",
    "# Merge the two DataFrames on the time index\n",
    "merged_df_before = pd.merge(en_start_gp_before, en_end_gp_before, left_index=True, right_index=True, suffixes=('_start', '_end'))\n",
    "\n",
    "# Combine the mean of Start and End to represent the approximate consumption in one column\n",
    "merged_df_before[\"Approximate Energy Consumption (kWh)\"] = (merged_df_before[\"Energy (kWh)_start\"] + merged_df_before[\"Energy (kWh)_end\"]) / 2\n",
    "merged_df_before.drop(merged_df_before[[\"Energy (kWh)_start\", \"Energy (kWh)_end\"]], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "# Calculate the sum of energy consumption for start times in 30-minute intervals and obtain the mean for each interval\n",
    "en_start_ev_after = after.resample(\"H\", on=\"Start Date\")[\"Energy (kWh)\"].sum()\n",
    "en_start_gp_after = en_start_ev_after.groupby(en_start_ev_after.index.time).mean()\n",
    "\n",
    "# Calculate the sum of energy consumption for end times in 30-minute intervals and obtain the mean for each interval\n",
    "en_end_ev_after = after.resample(\"H\", on=\"End Date\")[\"Energy (kWh)\"].sum()\n",
    "en_end_gp_after = en_end_ev_after.groupby(en_end_ev_after.index.time).mean()\n",
    "\n",
    "# Merge the two DataFrames on the time index\n",
    "merged_df_after = pd.merge(en_start_gp_after, en_end_gp_after, left_index=True, right_index=True, suffixes=('_start', '_end'))\n",
    "\n",
    "# Combine the mean of Start and End to represent the approximate consumption in one column\n",
    "merged_df_after[\"Approximate Energy Consumption (kWh)\"] = (merged_df_after[\"Energy (kWh)_start\"] + merged_df_after[\"Energy (kWh)_end\"]) / 2\n",
    "merged_df_after.drop(merged_df_after[[\"Energy (kWh)_start\", \"Energy (kWh)_end\"]], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hDESA8Vmkj66"
   },
   "outputs": [],
   "source": [
    "cmap= 'RdYlGn_r'\n",
    "\n",
    "# Determine the range of values for the colorbar\n",
    "min_value = min(merged_df_before[\"Approximate Energy Consumption (kWh)\"].min(), merged_df_after[\"Approximate Energy Consumption (kWh)\"].min())\n",
    "max_value = max(merged_df_before[\"Approximate Energy Consumption (kWh)\"].max(), merged_df_after[\"Approximate Energy Consumption (kWh)\"].max())\n",
    "\n",
    "# Create a single figure with two subplots side by side\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "# Create the first heatmap\n",
    "sns.heatmap(merged_df_before, cmap=cmap, vmin=min_value, vmax=max_value, ax=ax1, cbar=False, xticklabels=False, yticklabels=True)\n",
    "ax1.set_title('Before Policy Intervention')\n",
    "\n",
    "# Create the second heatmap\n",
    "sns.heatmap(merged_df_after, cmap=cmap, vmin=min_value, vmax=max_value, ax=ax2, cbar=False, xticklabels=False, yticklabels=True)\n",
    "ax2.set_title('After Policy Intervention')\n",
    "\n",
    "# Set a common colorbar for both subplots to the right of the second heatmap\n",
    "cax = plt.axes([0.92, 0.15, 0.02, 0.7])\n",
    "cbar = fig.colorbar(ax2.collections[0], cax=cax)\n",
    "cbar.set_label('Energy Consumption (kWh)')\n",
    "\n",
    "#plt.suptitle('Average Daily Energy Consumption Heatmap')\n",
    "\n",
    "# Adjust the spacing between subplots\n",
    "plt.subplots_adjust(wspace=0.1)\n",
    "\n",
    "# Save the plot as an image for the paper\n",
    "plt.savefig(\"energy_consumption_heatmap.png\", dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Display the combined plot (optional)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PILROdO94aAT"
   },
   "source": [
    "## **4.3. Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T_QsJnkXsR3S"
   },
   "source": [
    "### **4.3.1. Data Preprocesing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2lutsjdsUvIG"
   },
   "source": [
    "#### Station_change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xc2XBZUGUrJw"
   },
   "outputs": [],
   "source": [
    "# Create non_charge_min Column\n",
    "df[\"non_charge_min\"] = df[\"Duration_min\"] - df[\"Charging_min\"]\n",
    "\n",
    "# Split to before and after\n",
    "before= df[df[\"Fee_Status\"]==0]\n",
    "after= df[df[\"Fee_Status\"]==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XDX1ee81UrJx"
   },
   "outputs": [],
   "source": [
    "# Group by 'Station Name' and 'Start Date', then counting the number of events for 'before' and 'after'\n",
    "charging_events_per_day_before = before.groupby([\"Station Name\", before[\"Start Date\"].dt.date]).size().reset_index(name='Event Count')\n",
    "charging_events_per_day_after = after.groupby([\"Station Name\", after[\"Start Date\"].dt.date]).size().reset_index(name='Event Count')\n",
    "\n",
    "# Group by 'Station Name' and calculate the average charging events per day for 'before' and 'after'\n",
    "average_events_per_day_before = charging_events_per_day_before.groupby(\"Station Name\")[\"Event Count\"].sum().reset_index(name='Average Events (Before)')\n",
    "average_events_per_day_after = charging_events_per_day_after.groupby(\"Station Name\")[\"Event Count\"].sum().reset_index(name='Average Events (After)')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kKg353BjUrJx"
   },
   "outputs": [],
   "source": [
    "# Concatenate DataFrames along columns\n",
    "result_df = pd.concat([average_events_per_day_before, average_events_per_day_after], axis=1)\n",
    "\n",
    "# Drop the duplicate Station Name column\n",
    "result_df = result_df.loc[:, ~result_df.columns.duplicated()]\n",
    "\n",
    "# Calculate the percentage change\n",
    "result_df['Percent Change'] = ((result_df['Average Events (After)'] - result_df['Average Events (Before)']) / result_df['Average Events (Before)']) * 100\n",
    "result_df['Change Ratio'] = (result_df['Average Events (After)'] / result_df['Average Events (Before)'])\n",
    "\n",
    "# Rename columns for clarity\n",
    "#result_df.columns = ['Station', 'Average Events (Before)', 'Average Events (After)', 'Percent Change']\n",
    "#result_df.columns = ['Station', 'Average Events (Before)', 'Average Events (After)', 'Change Ratio']\n",
    "\n",
    "# Display the result\n",
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6JI8mIRVAgNT"
   },
   "outputs": [],
   "source": [
    "#result_df.to_csv('change.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tGtQtd8i7U4c"
   },
   "source": [
    "#### Station_Characteristics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3tBlkX2j7ajO"
   },
   "outputs": [],
   "source": [
    "# Group by 'Station Name' and 'Start Date', then counting the number of events for 'before' and 'after'\n",
    "charging_events_per_day_before = before.groupby([\"Station Name\", before[\"Start Date\"].dt.date]).size().reset_index(name='Event Count')\n",
    "\n",
    "# Group by 'Station Name' and calculate the average charging events per day for 'before' and 'after'\n",
    "average_events_per_day_before = charging_events_per_day_before.groupby(\"Station Name\")[\"Event Count\"].mean().reset_index(name='Average Events per Day')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cAoMegxT8rWn"
   },
   "outputs": [],
   "source": [
    "freq = before.groupby([\"Station Name\", 'User Segment']).size().reset_index(name='freq')\n",
    "\n",
    "# Calculate total users for each station\n",
    "total_users = freq.groupby('Station Name')['freq'].sum()\n",
    "\n",
    "# Calculate the percentage of frequent users among total users for each station\n",
    "frequent_users_percentage = (freq[freq['User Segment'] == 'Frequent'].groupby('Station Name')['freq'].sum() / total_users) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iHeaHUNI9dvW"
   },
   "outputs": [],
   "source": [
    "# Calculate the total number of rows for each station\n",
    "station_counts = before.groupby('Station Name').size()\n",
    "\n",
    "# Calculate the number of rows where Driver Postal Code equals Postal Code for each station\n",
    "matching_counts = before[before['Driver Postal Code'] == before['Postal Code']].groupby('Station Name').size()\n",
    "\n",
    "# Calculate the percentage of matching rows for each station\n",
    "percentage_matching = (matching_counts / station_counts) * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0LTOOfxt92mQ"
   },
   "outputs": [],
   "source": [
    "# Average Duration\n",
    "avg_dur= before.groupby('Station Name')['Duration_min'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A9JO0zBH-qU7"
   },
   "outputs": [],
   "source": [
    "percentage_matching.name = 'Local Users (pct)'\n",
    "avg_dur.name = 'Average Duration (Minute)'\n",
    "average_events_per_day_before.name = 'Average Events per Day'\n",
    "frequent_users_percentage.name = 'Frequent Users (pct)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRXD3Y99-1Ye"
   },
   "outputs": [],
   "source": [
    "# Merge the Series based on 'Station Name'\n",
    "merged_data = pd.merge(percentage_matching, avg_dur, on='Station Name')\n",
    "merged_data = pd.merge(merged_data, average_events_per_day_before, on='Station Name')\n",
    "st_char = pd.merge(merged_data, frequent_users_percentage, on='Station Name')\n",
    "\n",
    "st_char"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iMlwdPKqsW_t"
   },
   "source": [
    "#### Block_Group_Level_Population_Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3AqjoQoGpp9"
   },
   "outputs": [],
   "source": [
    "def process_geojson(gdf, station):\n",
    "    # Add Features\n",
    "    gdf[\"Population Density\"] = gdf[\"B01001e1\"] / gdf[\"ALAND\"]\n",
    "    gdf[\"Median Age\"] = gdf[\"B01002e1\"]\n",
    "    gdf[\"Median Household Income\"] = gdf[\"B19013e1\"]\n",
    "    gdf[\"Vehicles per Occupied House\"] = gdf[\"B25046e1\"] / gdf[\"B25003e1\"]\n",
    "    gdf[\"Median Value of Housing Unit\"] = gdf[\"B25077e1\"]\n",
    "    gdf[\"station\"] = station\n",
    "\n",
    "    # Drop irrelevant columns\n",
    "    gdf.drop(['STATEFP', 'COUNTYFP', 'TRACTCE', 'BLKGRPCE', 'GEOID',\n",
    "              'NAMELSAD', 'MTFCC', 'FUNCSTAT', 'ALAND', 'AWATER', 'INTPTLAT',\n",
    "              'INTPTLON', 'Shape_Leng', 'Shape_Area', 'B01001e1',\n",
    "              'B01002e1', 'B08007e1', 'B08007e3', 'B08134e2', 'B08134e3', 'B08301e10',\n",
    "              'B19013e1', 'B19301e1', 'B23025e1', 'B23025e2', 'B23025e3', 'B23025e4',\n",
    "              'B25001e1', 'B25002e1', 'B25002e2', 'B25002e3', 'B25003e1', 'B25003e2',\n",
    "              'B25003e3', 'B25046e1', 'B25058e1', 'B25071e1', 'B25077e1'], axis=1, inplace=True)\n",
    "\n",
    "    return gdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4jJPdfx9JPPL"
   },
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "\n",
    "# Dictionary of GeoDataFrames\n",
    "stations = {\n",
    "    \"bryant\": gpd.read_file(\"/content/Bryant.geojson\"),\n",
    "    \"cambridge\": gpd.read_file(\"/content/Cambridge.geojson\"),\n",
    "    \"hamilton\": gpd.read_file(\"/content/Hamilton.geojson\"),\n",
    "    \"high\": gpd.read_file(\"/content/High.geojson\"),\n",
    "    \"mpl\": gpd.read_file(\"/content/MPL.geojson\"),\n",
    "    \"rinconada\": gpd.read_file(\"/content/Rinconada_Lib.geojson\"),\n",
    "    \"ted\": gpd.read_file(\"/content/Ted_thompson.geojson\"),\n",
    "    \"webster\": gpd.read_file(\"/content/Webster.geojson\"),\n",
    "}\n",
    "\n",
    "def process_all_stations(stations):\n",
    "    processed_stations = {}\n",
    "    for station, gdf in stations.items():\n",
    "        processed_gdf = process_geojson(gdf, station)\n",
    "        processed_stations[station] = processed_gdf\n",
    "    return processed_stations\n",
    "\n",
    "# Process all stations\n",
    "processed_stations = process_all_stations(stations)\n",
    "\n",
    "# Access the processed GeoDataFrames\n",
    "bryant = processed_stations[\"bryant\"]\n",
    "cambridge = processed_stations[\"cambridge\"]\n",
    "hamilton = processed_stations[\"hamilton\"]\n",
    "high = processed_stations[\"high\"]\n",
    "mpl = processed_stations[\"mpl\"]\n",
    "rinconada = processed_stations[\"rinconada\"]\n",
    "ted = processed_stations[\"ted\"]\n",
    "webster = processed_stations[\"webster\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "akpl-dR3adnD"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Using processed_stations dictionary from previous code\n",
    "processed_stations = {\n",
    "    \"bryant\": bryant,\n",
    "    \"cambridge\": cambridge,\n",
    "    \"hamilton\": hamilton,\n",
    "    \"high\": high,\n",
    "    \"mpl\": mpl,\n",
    "    \"rinconada\": rinconada,\n",
    "    \"ted\": ted,\n",
    "    \"webster\": webster,\n",
    "}\n",
    "\n",
    "# Concatenate all GeoDataFrames into a single GeoDataFrame\n",
    "all_stations_gdf = gpd.GeoDataFrame(pd.concat(processed_stations.values(), ignore_index=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "U10D7Tdm-fec"
   },
   "outputs": [],
   "source": [
    "all_stations_gdf= all_stations_gdf.drop([\"OBJECTID\", \"GEOID_Data\", \"geometry\"], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Wxs3WA3q_BU2"
   },
   "outputs": [],
   "source": [
    "pop_bg= all_stations_gdf.groupby('station').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jb8YoSmd_oEj"
   },
   "outputs": [],
   "source": [
    "pop_bg.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3rzDS7RrUHRH"
   },
   "outputs": [],
   "source": [
    "pop_bg.iloc[5, 0]= \"rinconada lib\"\n",
    "pop_bg.iloc[6, 0]= \"ted thompson\"\n",
    "pop_bg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c-27l8xwC0h6"
   },
   "source": [
    "#### POIs_Data_in_15min_Walking_Distance_From_Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v8ST4qYd9uzK"
   },
   "outputs": [],
   "source": [
    "bryant_loc= gpd.read_file(\"/content/loc_bryant.geojson\")\n",
    "cambridge_loc= gpd.read_file(\"/content/loc_cambridge.geojson\")\n",
    "hamilton_loc= gpd.read_file(\"/content/loc_hamilton.geojson\")\n",
    "high_loc= gpd.read_file(\"/content/loc_high.geojson\")\n",
    "mpl_loc= gpd.read_file(\"/content/loc_mpl.geojson\")\n",
    "rinconada_loc= gpd.read_file(\"/content/loc_rinconada.geojson\")\n",
    "ted_loc= gpd.read_file(\"/content/loc_ted.geojson\")\n",
    "webster_loc= gpd.read_file(\"/content/loc_bryant.geojson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wRabbGifCIm"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "\n",
    "# Assuming you have a list of station names\n",
    "station_names = ['bryant', 'cambridge', 'hamilton', 'high', 'mpl', 'rinconada', 'ted', 'webster']\n",
    "\n",
    "# Define a mapping of similar features\n",
    "feature_mapping = {\n",
    "    'Businesses': 'Commercial',\n",
    "    'Restaurants': 'Commercial',\n",
    "    'Parking Lot': 'Parking Resources',\n",
    "    #'Parking Lot': 'Infrastructure',\n",
    "    'Banks': 'Commercial',\n",
    "    'Recreation': 'Recreational',\n",
    "    'Transportation': 'Transportation',\n",
    "    'Parks': 'Recreational',\n",
    "    'Hotels': 'Commercial',\n",
    "    'Apartment Complex': 'Residential Complex',\n",
    "    'City Facilities': 'Public Facilities',\n",
    "    'Retirement Facility': 'Public Facilities',\n",
    "    'Road Structures': 'Infrastructure',\n",
    "    'Medical': 'Public Facilities',\n",
    "    'Places of Worship': 'Public Facilities',\n",
    "    'Schools': 'Public Facilities',\n",
    "    'Library': 'Public Facilities',\n",
    "    'Utilities': 'Infrastructure',\n",
    "    'Public Safety': 'Public Facilities',\n",
    "    'Other Public Facilities': 'Public Facilities',\n",
    "    'Community Center': 'Public Facilities',\n",
    "    'Fire': 'Public Facilities',\n",
    "    'Community Garden': 'Recreational'\n",
    "}\n",
    "\n",
    "# Create a list to store DataFrames for each station\n",
    "dfs = []\n",
    "\n",
    "# Iterate over each station and populate the DataFrame\n",
    "for station in station_names:\n",
    "    # Read the GeoJSON file for the station\n",
    "    station_loc = gpd.read_file(f\"/content/loc_{station}.geojson\")\n",
    "\n",
    "    # Apply the feature mapping to the 'GROUPNAME' column\n",
    "    station_loc['GROUPNAME'] = station_loc['GROUPNAME'].map(feature_mapping)\n",
    "\n",
    "    # Calculate value counts for modified 'GROUPNAME'\n",
    "    value_counts = station_loc['GROUPNAME'].value_counts()\n",
    "\n",
    "    # Create a DataFrame for the station and append it to the list\n",
    "    temp_df = pd.DataFrame({station: value_counts})\n",
    "    dfs.append(temp_df)\n",
    "\n",
    "# Concatenate all DataFrames in the list\n",
    "value_counts_df = pd.concat(dfs, axis=1)\n",
    "\n",
    "# Fill NaN values with 0\n",
    "value_counts_df = value_counts_df.fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8qlu-DkZfsSw"
   },
   "outputs": [],
   "source": [
    "# Transpose the DataFrame using the .T attribute\n",
    "lu_data = value_counts_df.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "824uMscUTvGq"
   },
   "outputs": [],
   "source": [
    "# Drop Infrastructure columns\n",
    "lu_data.drop(['Infrastructure', 'Transportation', 'Residential Complex'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ke09TM0nZzDS"
   },
   "outputs": [],
   "source": [
    "lu_data.reset_index(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "noNIPI_Ih91j"
   },
   "outputs": [],
   "source": [
    "lu_data.iloc[5, 0]= \"rinconada lib\"\n",
    "lu_data.iloc[6, 0]= \"ted thompson\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5QHjHhT1joQs"
   },
   "outputs": [],
   "source": [
    "lu_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BH91Ljx9A0w4"
   },
   "source": [
    "#### Final Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cysRHeUSh1sJ"
   },
   "outputs": [],
   "source": [
    "# Lowercase station names in st_char DataFrame\n",
    "st_char[\"Station Name\"] = st_char[\"Station Name\"].str.lower()\n",
    "\n",
    "# Join DataFrames on station names\n",
    "features = st_char.merge(pop_bg, how=\"inner\", left_on=\"Station Name\", right_on=\"station\").merge(lu_data, how=\"inner\", left_on=\"Station Name\", right_on=\"index\")\n",
    "\n",
    "# Drop redundant columns\n",
    "features.drop(columns=[\"station\", \"index\"], inplace=True)\n",
    "\n",
    "features.rename(columns={\"Station Name\": \"Station\"}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jBz5rm0IVQyP"
   },
   "outputs": [],
   "source": [
    "# Concatenate DataFrames along columns\n",
    "df = pd.concat([result_df, features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vCKTda1BVyIT"
   },
   "outputs": [],
   "source": [
    "df.drop(['Percent Change', 'Average Events (Before)', 'Average Events (After)', 'Station'], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x8atw6NnZHI7"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IW0hEzLXAPcq"
   },
   "outputs": [],
   "source": [
    "#df.to_csv('df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N-iG-t61CGfE"
   },
   "source": [
    "### **4.3.2. Model Building and Evaluation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frHlEqNwJJUZ"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Patch\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import LeaveOneOut, cross_val_score, GridSearchCV\n",
    "from sklearn.metrics import make_scorer, mean_squared_error, mean_absolute_error, r2_score\n",
    "import xgboost as xgb\n",
    "\n",
    "!pip install alibi\n",
    "from alibi.explainers import ALE, plot_ale\n",
    "\n",
    "!pip install shap\n",
    "import shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4AOuwtqyHKaW"
   },
   "outputs": [],
   "source": [
    "df=pd.read_csv('/content/df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2C-wtUYYHOBR"
   },
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cfpt04sECECb"
   },
   "source": [
    "#### Random Forrest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "24R_0BtxTYg5"
   },
   "outputs": [],
   "source": [
    "# Separate features (X) and target variable (y)\n",
    "X = df.drop(['Station Name', 'Change Ratio'], axis=1)\n",
    "y = df['Change Ratio']\n",
    "\n",
    "# Normalize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Random Forest\n",
    "rf_model = RandomForestRegressor(random_state=13)\n",
    "rf_param_grid = {'n_estimators': [20, 50, 100, 200],\n",
    "                 'max_depth': [None, 3, 5, 10, 20, 50, 100],\n",
    "                 'min_samples_split': [2, 3, 5]}\n",
    "rf_grid_search = GridSearchCV(rf_model, rf_param_grid, cv=5, scoring='neg_mean_squared_error')\n",
    "rf_grid_search.fit(X_scaled, y)\n",
    "rf_best_model = rf_grid_search.best_estimator_\n",
    "\n",
    "# Extract best hyperparameters for Random Forest\n",
    "best_rf_params = rf_grid_search.best_params_\n",
    "print(f\"Best Random Forest Hyperparameters: {best_rf_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-YnbdyqbQyWI"
   },
   "outputs": [],
   "source": [
    "model_score = rf_best_model.score(X_scaled, y)\n",
    "model_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pLQZW4zTxY27"
   },
   "outputs": [],
   "source": [
    "# Feature Importance Plot for Random Forest\n",
    "feature_importance = rf_best_model.feature_importances_\n",
    "feature_names = X.columns\n",
    "sorted_idx = feature_importance.argsort()\n",
    "\n",
    "# Define colors for each group\n",
    "land_use_color = 'skyblue'\n",
    "socioeconomic_color = 'lightgreen'\n",
    "station_behavioral_color = 'salmon'\n",
    "\n",
    "# Grouping features by aspect\n",
    "land_use_features = ['Commercial', 'Recreational', 'Transportation', 'Public Facilities', 'Residential Complex']\n",
    "socioeconomic_features = ['Population Density', 'Median Age', 'Income per Capita', 'Housing Owner Rate (pct)', 'Vehicles per Occupied House', 'Median Value of Housing Unit']\n",
    "station_behavioral_features = ['Local Postal Code (pct)', 'Average Duration (Minute)', 'Average Events per Day', 'Frequent Users (pct)']\n",
    "\n",
    "# Assigning colors to features based on aspect\n",
    "colors = []\n",
    "for feature in feature_names:\n",
    "    if feature in land_use_features:\n",
    "        colors.append(land_use_color)\n",
    "    elif feature in socioeconomic_features:\n",
    "        colors.append(socioeconomic_color)\n",
    "    elif feature in station_behavioral_features:\n",
    "        colors.append(station_behavioral_color)\n",
    "\n",
    "# Plotting\n",
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "bars = plt.barh(range(len(sorted_idx)), feature_importance[sorted_idx], align=\"center\", color=[colors[i] for i in sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), [feature_names[i] for i in sorted_idx], fontsize=12)\n",
    "plt.xlabel(\"Importance in Random Forest Model\", fontsize=14)\n",
    "plt.grid(axis='x', linestyle='--', alpha=0.7)\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks([])\n",
    "\n",
    "# Add labels inside bars\n",
    "for bar, label, corr_val in zip(bars, feature_names[sorted_idx], feature_importance[sorted_idx]):\n",
    "    width = bar.get_width()\n",
    "    if corr_val >= 0:\n",
    "        plt.text(+0.001, bar.get_y() + bar.get_height()/2, label, ha='left', va='center', fontsize=10)\n",
    "    else:\n",
    "        plt.text(-0.01, bar.get_y() + bar.get_height()/2, label, ha='right', va='center', fontsize=10)\n",
    "\n",
    "# Create legend\n",
    "legend_elements = [\n",
    "    Patch(facecolor=land_use_color, edgecolor='black', label='Land-Use Features'),\n",
    "    Patch(facecolor=socioeconomic_color, edgecolor='black', label='Socioeconomic Features'),\n",
    "    Patch(facecolor=station_behavioral_color, edgecolor='black', label='Station Behavior Features')\n",
    "]\n",
    "plt.legend(handles=legend_elements, loc='lower right', fontsize=12)\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save or display the plot\n",
    "#plt.savefig(\"feature_importance_plot.png\", dpi=300)  # Save the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kxn_QeoxQmRL"
   },
   "outputs": [],
   "source": [
    "# Set the number of rounds\n",
    "num_rounds = 100\n",
    "\n",
    "# Initialize lists to store variable importance scores and model scores\n",
    "variable_importance_scores = []\n",
    "model_scores = []\n",
    "\n",
    "# Run the RF for 10 rounds and average the results\n",
    "for _ in range(num_rounds):\n",
    "    # Create a Random Forest Regressor\n",
    "    rf_model = RandomForestRegressor(n_estimators=100, min_samples_split=3, max_depth=3)#, random_state=33)\n",
    "\n",
    "    # Fit the model\n",
    "    rf_model.fit(X_scaled, y)\n",
    "\n",
    "    # Get variable importances\n",
    "    importance_scores = rf_model.feature_importances_\n",
    "\n",
    "    # Get model score\n",
    "    model_score = rf_model.score(X_scaled, y)\n",
    "\n",
    "    # Store the importance scores\n",
    "    variable_importance_scores.append(importance_scores)\n",
    "    model_scores.append(model_score)\n",
    "\n",
    "# Calculate the average importance scores\n",
    "average_importance_scores = np.mean(variable_importance_scores, axis=0)\n",
    "\n",
    "# Calculate the average model score\n",
    "average_model_score = np.mean(model_scores)\n",
    "\n",
    "# Print the average importance scores\n",
    "print(\"Average Variable Importance Scores:\")\n",
    "for feature, importance in zip(X.columns, average_importance_scores):\n",
    "    print(f\"{feature}: {importance}\")\n",
    "\n",
    "# Print the average model score\n",
    "print(f\"\\nAverage Model Score: {average_model_score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NBXdRFOHkQXn"
   },
   "source": [
    "#### XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fvbkbb6OJJUZ"
   },
   "outputs": [],
   "source": [
    "X = df.drop(columns=['Change Ratio', 'Station Name', 'Unnamed: 0'])  # Drop target and non-numeric features\n",
    "y = df['Change Ratio']\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yoCwjn5tSkfZ"
   },
   "outputs": [],
   "source": [
    "# Initialize the XGBoost Regressor\n",
    "xg_reg = xgb.XGBRegressor(random_state=2024)\n",
    "\n",
    "# Define the parameter grid\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'learning_rate': [0.01, 0.1, 0.3],\n",
    "    'subsample': [0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.7, 0.8, 0.9, 1.0],\n",
    "}\n",
    "\n",
    "# Initialize Leave-One-Out Cross-Validation\n",
    "loo = LeaveOneOut()\n",
    "\n",
    "# Grid Search for XGBoost Regressor with LOO-CV\n",
    "grid_search = GridSearchCV(estimator=xg_reg, param_grid=param_grid, cv=loo, scoring='neg_mean_squared_error')\n",
    "grid_search.fit(X_scaled, y)\n",
    "\n",
    "# Get the best parameters and the best score\n",
    "best_params = grid_search.best_params_\n",
    "best_score = -grid_search.best_score_\n",
    "\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_params)\n",
    "print(f\"Best Mean Squared Error (MSE): {best_score}\")\n",
    "\n",
    "# Train the best model on the entire dataset\n",
    "best_xgb = xgb.XGBRegressor(**best_params, random_state=2024)\n",
    "best_xgb.fit(X_scaled, y)\n",
    "\n",
    "# Predictions on the same data (since we aren't testing)\n",
    "y_pred = best_xgb.predict(X_scaled)\n",
    "\n",
    "# Calculate metrics on the full dataset\n",
    "mse = mean_squared_error(y, y_pred)\n",
    "mae = mean_absolute_error(y, y_pred)\n",
    "r2 = r2_score(y, y_pred)\n",
    "\n",
    "print(\"XGBoost Metrics on Full Dataset:\")\n",
    "print(f\"Mean Squared Error (MSE): {mse}\")\n",
    "print(f\"Mean Absolute Error (MAE): {mae}\")\n",
    "print(f\"R-squared (RÂ²): {r2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jF7E7ikvxzkv"
   },
   "outputs": [],
   "source": [
    "# Define the metrics data\n",
    "data = {\n",
    "    'Model': ['Random Forest', 'XGBoost'],\n",
    "    'MSE': [0.0082, 0.0005],\n",
    "    'MAE': [0.081, 0.01],\n",
    "    'R-squared': [0.84, 0.99]\n",
    "}\n",
    "\n",
    "# Convert the data into a DataFrame\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Melt the DataFrame to long format for easier plotting\n",
    "df_melted = df.melt(id_vars='Model', var_name='Metric', value_name='Value')\n",
    "\n",
    "# Define a refined color palette\n",
    "palette = {'XGBoost': '#4caf50', 'Random Forest': '#e57373'}\n",
    "\n",
    "# Create the figure and subplots\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 6), sharey=False)\n",
    "\n",
    "# Plot MSE\n",
    "sns.barplot(x='Model', y='Value', data=df_melted[df_melted['Metric'] == 'MSE'],\n",
    "            palette=palette, ax=axes[0])\n",
    "axes[0].set_title('Mean Squared Error (MSE) - Lower is better', fontsize=14)\n",
    "axes[0].set_ylabel('Value', fontsize=12)\n",
    "axes[0].set_ylim(0, 0.01)\n",
    "axes[0].set_xlabel('Model', fontsize=12)\n",
    "\n",
    "# Plot MAE\n",
    "sns.barplot(x='Model', y='Value', data=df_melted[df_melted['Metric'] == 'MAE'],\n",
    "            palette=palette, ax=axes[1])\n",
    "axes[1].set_title('Mean Absolute Error (MAE) - Lower is better', fontsize=14)\n",
    "axes[1].set_ylabel('Value', fontsize=12)\n",
    "axes[1].set_ylim(0, 0.1)\n",
    "axes[1].set_xlabel('Model', fontsize=12)\n",
    "\n",
    "# Plot R-squared\n",
    "sns.barplot(x='Model', y='Value', data=df_melted[df_melted['Metric'] == 'R-squared'],\n",
    "            palette=palette, ax=axes[2])\n",
    "axes[2].set_title('R-squared (RÂ²) - Higher is better', fontsize=14)\n",
    "axes[2].set_ylabel('Value', fontsize=12)\n",
    "axes[2].set_ylim(0.8, 1.05)\n",
    "axes[2].set_xlabel('Model', fontsize=12)\n",
    "\n",
    "# Add overall title and adjust layout\n",
    "#fig.suptitle('Performance Metrics of Random Forest and XGBoost Models', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])  # Adjust layout to make room for the main title\n",
    "\n",
    "# Save the plot with 300 dpi\n",
    "#plt.savefig(f\"Model_eval.png\", dpi=300)\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fctXc5TV-9wy"
   },
   "outputs": [],
   "source": [
    "feature_names = X.columns\n",
    "feature_map = {f'f{I}': feature for I, feature in enumerate(feature_names)}\n",
    "for I, feature in enumerate(feature_names):\n",
    "    print(f'f{I}: {feature}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y46W59Di-522"
   },
   "outputs": [],
   "source": [
    "# Define colors for each group\n",
    "land_use_color = 'skyblue'\n",
    "socioeconomic_color = 'lightgreen'\n",
    "station_behavioral_color = 'salmon'\n",
    "\n",
    "# Grouping features by aspect\n",
    "land_use_features = ['Commercial', 'Recreational', 'Public Facilities', 'Parking Resources']\n",
    "socioeconomic_features = ['Population Density', 'Median Age', 'Median Household Income', 'Vehicles per Occupied House', 'Median Value of Housing Unit']\n",
    "station_behavioral_features = ['Local Users (pct)', 'Average Duration (Minute)', 'Average Events per Day', 'Frequent Users (pct)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P3EeBYVd_Av7"
   },
   "outputs": [],
   "source": [
    "# Extract feature importances from the model\n",
    "importances = best_xgb.feature_importances_\n",
    "\n",
    "# Map the importance to the original feature names\n",
    "feature_importances = pd.DataFrame({'Feature': feature_names, 'Importance': importances})\n",
    "\n",
    "# Sort the features by importance\n",
    "feature_importances.sort_values(by='Importance', ascending=False, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7JrFhxBF_Dzw"
   },
   "outputs": [],
   "source": [
    "# Assign colors to each feature based on its group\n",
    "feature_importances['Color'] = feature_importances['Feature'].apply(lambda x:\n",
    "    land_use_color if x in land_use_features else\n",
    "    socioeconomic_color if x in socioeconomic_features else\n",
    "    station_behavioral_color if x in station_behavioral_features else\n",
    "    'gray')  # Default color if feature not found in any group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YXw-vsmC_EiG"
   },
   "outputs": [],
   "source": [
    "# Create a palette dictionary mapping each feature to its corresponding color\n",
    "palette_dict = feature_importances.set_index('Feature')['Color'].to_dict()\n",
    "\n",
    "# Plot the feature importances with colors\n",
    "plt.figure(figsize=(10, 8), dpi=300)\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importances, palette=palette_dict)\n",
    "\n",
    "# Customize the plot\n",
    "plt.title('Feature Importances from XGBoost')\n",
    "plt.xlabel('Importance')\n",
    "plt.ylabel('Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z-Kq2McKL2pn"
   },
   "source": [
    "#### ALE_Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iUf6Lm0GLNg6"
   },
   "outputs": [],
   "source": [
    "feature_names = X.columns  # Extract feature names from original DataFrame\n",
    "xpd = pd.DataFrame(X_scaled, columns=feature_names)  # Assign feature names to the scaled DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rkh-3uipOESk"
   },
   "outputs": [],
   "source": [
    "# List of features to plot\n",
    "features = ['Commercial', 'Recreational', 'Public Facilities', 'Parking Resources', 'Population Density',\n",
    "            'Median Age', 'Median Household Income', 'Vehicles per Occupied House', 'Median Value of Housing Unit',\n",
    "            'Local Users (pct)', 'Average Duration (Minute)', 'Average Events per Day', 'Frequent Users (pct)']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2lcLao5EuU_1"
   },
   "outputs": [],
   "source": [
    "# List of features to plot\n",
    "features = ['Commercial', 'Recreational', 'Public Facilities', 'Parking Resources', 'Population Density',\n",
    "            'Median Age', 'Median Household Income', 'Vehicles per Occupied House', 'Median Value of Housing Unit',\n",
    "            'Local Users (pct)', 'Average Duration (Minute)', 'Average Events per Day', 'Frequent Users (pct)']\n",
    "\n",
    "# Initialize the ALE explainer\n",
    "ale_explainer = ALE(best_xgb.predict, feature_names=xpd.columns, target_names=['Change Ratio'])\n",
    "\n",
    "# Calculate ALE for the selected features\n",
    "ale_exp = ale_explainer.explain(xpd.values)\n",
    "\n",
    "# Customize the plotting to meet Q1 journal standards\n",
    "fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(20, 20))\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.3)  # Adjust spacing between plots\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot ALE for the specified features\n",
    "for i, feature in enumerate(features):\n",
    "    plot_ale(ale_exp, features=[feature], ax=axes[i], line_kw={'linewidth': 2.5, 'color': '#007acc'})\n",
    "\n",
    "    # Customize each subplot\n",
    "    axes[i].set_title(f'{feature}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Scaled Feature Value', fontsize=11)\n",
    "    axes[i].set_ylabel('ALE', fontsize=11)\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=10)\n",
    "    axes[i].grid(True, linestyle='--', alpha=0.6)\n",
    "    axes[i].spines['top'].set_visible(False)\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "\n",
    "# Remove any empty subplots (if features < axes)\n",
    "for j in range(len(features), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Add a general title\n",
    "fig.suptitle('Accumulated Local Effects (ALE) for Features in XGBoost Model', fontsize=16, fontweight='bold')\n",
    "#plt.tight_layout()\n",
    "# Adjust layout\n",
    "plt.tight_layout(pad=2.0)\n",
    "# Save the combined figure with 300 dpi\n",
    "#plt.savefig(\"ALE_Plots_Scaled.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3x9k-OzuwdQZ"
   },
   "outputs": [],
   "source": [
    "# Assuming you have stored the scaler used\n",
    "scaler = StandardScaler()  # Replace with your scaler object\n",
    "scaler.fit(X)  # Fit scaler on original data\n",
    "\n",
    "# Calculate ALE using the scaled data\n",
    "ale_explainer = ALE(best_xgb.predict, feature_names=xpd.columns, target_names=['Change Ratio'])\n",
    "ale_exp = ale_explainer.explain(xpd.values)\n",
    "\n",
    "# Function to convert scaled values back to original values\n",
    "def inverse_transform_scaled_values(scaled_values, feature_name):\n",
    "    feature_index = xpd.columns.get_loc(feature_name)\n",
    "    scaled_values_reshaped = scaled_values.reshape(-1, 1)\n",
    "    original_values = scaler.inverse_transform(np.hstack([np.zeros((scaled_values_reshaped.shape[0], feature_index)), scaled_values_reshaped, np.zeros((scaled_values_reshaped.shape[0], len(scaler.mean_) - feature_index - 1))]))\n",
    "    return original_values[:, feature_index]\n",
    "\n",
    "# Plot ALE for the specified features\n",
    "fig, axes = plt.subplots(nrows=5, ncols=3, figsize=(20, 20))\n",
    "fig.subplots_adjust(hspace=0.5, wspace=0.3)  # Adjust spacing between plots\n",
    "\n",
    "# Flatten axes for easy iteration\n",
    "axes = axes.flatten()\n",
    "\n",
    "# Plot ALE for each feature\n",
    "for i, feature in enumerate(features):\n",
    "    feature_index = xpd.columns.get_loc(feature)\n",
    "    ale_data = ale_exp.ale_values[feature_index]\n",
    "    feature_values_scaled = ale_exp.feature_values[feature_index]\n",
    "    feature_values_original = inverse_transform_scaled_values(feature_values_scaled, feature)\n",
    "\n",
    "    axes[i].plot(feature_values_original, ale_data, linewidth=2.5, color='#007acc')\n",
    "\n",
    "    # Customize each subplot\n",
    "    axes[i].set_title(f'{feature}', fontsize=12, fontweight='bold')\n",
    "    axes[i].set_xlabel('Feature Value', fontsize=11)\n",
    "    axes[i].set_ylabel('ALE', fontsize=11)\n",
    "    axes[i].tick_params(axis='both', which='major', labelsize=10)\n",
    "    axes[i].grid(True, linestyle='--', alpha=0.6)\n",
    "    axes[i].spines['top'].set_visible(False)\n",
    "    axes[i].spines['right'].set_visible(False)\n",
    "\n",
    "# Remove any empty subplots (if features < axes)\n",
    "for j in range(len(features), len(axes)):\n",
    "    fig.delaxes(axes[j])\n",
    "\n",
    "# Add a general title\n",
    "fig.suptitle('Accumulated Local Effects (ALE) for Various Features', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout(pad=2.0)\n",
    "# Save the combined figure with 300 dpi\n",
    "#plt.savefig(\"ALE_Plots_Combined.png\", dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M0HYv29CxQYy"
   },
   "source": [
    "#### SHAP Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "655g3h0SxSeB"
   },
   "outputs": [],
   "source": [
    "# Create a SHAP explainer for the Random Forest model\n",
    "explainer = shap.TreeExplainer(best_xgb)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(X_scaled)\n",
    "\n",
    "# Summary Plot\n",
    "shap.summary_plot(shap_values, X_scaled, feature_names=X.columns, plot_type=\"bar\", show=False)\n",
    "plt.title(\"SHAP Summary Plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nlrP_dovgLeY"
   },
   "outputs": [],
   "source": [
    "# Define colors for each group\n",
    "land_use_color = 'lightgreen'\n",
    "socioeconomic_color = 'skyblue'\n",
    "station_behavioral_color = 'salmon'\n",
    "\n",
    "# Grouping features by aspect\n",
    "land_use_features = ['Commercial', 'Recreational', 'Public Facilities', 'Parking Resources']\n",
    "socioeconomic_features = ['Population Density', 'Median Age', 'Median Household Income', 'Vehicles per Occupied House', 'Median Value of Housing Unit']\n",
    "station_behavioral_features = ['Local Users (pct)', 'Average Duration (Minute)', 'Average Events per Day', 'Frequent Users (pct)']\n",
    "\n",
    "# Create a SHAP explainer for the XGBoost model\n",
    "explainer = shap.TreeExplainer(best_xgb)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(X_scaled)\n",
    "\n",
    "# Use feature_names from X.columns\n",
    "feature_names = X.columns\n",
    "\n",
    "# Calculate mean absolute SHAP values for each feature\n",
    "mean_abs_shap_values = np.abs(shap_values).mean(axis=0)\n",
    "\n",
    "# Create a DataFrame for plotting\n",
    "shap_df = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'MeanAbsSHAP': mean_abs_shap_values\n",
    "})\n",
    "\n",
    "# Sort features by mean absolute SHAP values\n",
    "shap_df = shap_df.sort_values(by='MeanAbsSHAP', ascending=False)\n",
    "\n",
    "# Create a dictionary to map feature names to their respective colors\n",
    "feature_color_map = {}\n",
    "for feature in shap_df['Feature']:\n",
    "    if feature in land_use_features:\n",
    "        feature_color_map[feature] = land_use_color\n",
    "    elif feature in socioeconomic_features:\n",
    "        feature_color_map[feature] = socioeconomic_color\n",
    "    elif feature in station_behavioral_features:\n",
    "        feature_color_map[feature] = station_behavioral_color\n",
    "    else:\n",
    "        feature_color_map[feature] = 'grey'  # Default color for features not in any group\n",
    "\n",
    "# Assign colors based on the sorted feature names\n",
    "shap_df['Color'] = shap_df['Feature'].map(feature_color_map)\n",
    "\n",
    "# Plot the custom bar plot\n",
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "bars = plt.barh(shap_df['Feature'], shap_df['MeanAbsSHAP'], color=shap_df['Color'])\n",
    "plt.xlabel('Mean Absolute SHAP Value (Average Impact on Model Output Magnitude)')\n",
    "plt.gca().invert_yaxis()  # Invert y-axis to have the highest values on top\n",
    "\n",
    "# Add vertical ticks (grid lines)\n",
    "plt.grid(axis='x', linestyle='dotted', alpha=0.3)\n",
    "\n",
    "# Add color legend\n",
    "handles = [plt.Line2D([0], [0], color=land_use_color, lw=4),\n",
    "           plt.Line2D([0], [0], color=socioeconomic_color, lw=4),\n",
    "           plt.Line2D([0], [0], color=station_behavioral_color, lw=4)]\n",
    "labels = ['Land Use', 'Socioeconomic', 'Station Behavioral']\n",
    "plt.legend(handles, labels, title=\"Feature Groups\")\n",
    "plt.tight_layout()\n",
    "# Save the plot with 300 dpi\n",
    "#plt.savefig(\"SHAP Summary Plot with Feature Groups.png\", dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4nwX2g5JebMx"
   },
   "outputs": [],
   "source": [
    "# Create SHAP Explanation object for the beeswarm plot\n",
    "shap_explanation = shap.Explanation(\n",
    "    values=shap_values,\n",
    "    base_values=explainer.expected_value,\n",
    "    data=X_scaled,\n",
    "    feature_names=X.columns\n",
    ")\n",
    "\n",
    "\n",
    "shap.plots.bar(shap_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qV1ppU1DUhLC"
   },
   "outputs": [],
   "source": [
    "# Assuming `X` is  original DataFrame with feature names\n",
    "# If `X_scaled` is a NumPy array, use the first row from `X` for the instance data\n",
    "instance_data = X.iloc[0]  # Original DataFrame to get feature names\n",
    "\n",
    "\n",
    "# Create a DataFrame for the instance data to match SHAP Explanation format\n",
    "instance_data_df = pd.DataFrame([instance_data], columns=X.columns)\n",
    "\n",
    "# Create SHAP Explanation object for the waterfall plot\n",
    "shap_explanation = shap.Explanation(\n",
    "    values=shap_values[0],\n",
    "    base_values=explainer.expected_value,\n",
    "    data=instance_data_df.iloc[0],  # Extract first instance as Series\n",
    "    feature_names=X.columns\n",
    ")\n",
    "\n",
    "# Plot the waterfall plot\n",
    "shap.plots.waterfall(shap_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Z16wNtzn8ap"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming `X` is original DataFrame with feature names\n",
    "# Assuming `shap_values` is a list of SHAP values for each station\n",
    "# Assuming `explainer` is a SHAP explainer instance\n",
    "\n",
    "# Iterate through each station\n",
    "for station_id in range(8):  # Assuming stations are indexed from 0 to 7\n",
    "    # Create a DataFrame for the instance data to match SHAP Explanation format\n",
    "    instance_data = X.iloc[station_id]  # Get feature values for the current station\n",
    "    instance_data_df = pd.DataFrame([instance_data], columns=X.columns)\n",
    "\n",
    "    # Create SHAP Explanation object for the waterfall plot\n",
    "    shap_explanation = shap.Explanation(\n",
    "        values=shap_values[station_id],\n",
    "        base_values=explainer.expected_value,\n",
    "        data=instance_data_df.iloc[0],  # Extract current instance as Series\n",
    "        feature_names=X.columns\n",
    "    )\n",
    "\n",
    "    # Plot the waterfall plot\n",
    "    fig = plt.figure(figsize=(10, 6))\n",
    "    shap.plots.waterfall(shap_explanation)\n",
    "\n",
    "    # Save the plot\n",
    "    #plt.savefig(f'shap_waterfall_station_{station_id}.png', dpi=300)\n",
    "\n",
    "    # Show the plot (optional, but useful for debugging)\n",
    "    plt.show()\n",
    "\n",
    "    # Close the plot\n",
    "    plt.close(fig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SH-6tsG6cN9d"
   },
   "outputs": [],
   "source": [
    "# Create SHAP Explanation object for the beeswarm plot\n",
    "shap_explanation = shap.Explanation(\n",
    "    values=shap_values,\n",
    "    base_values=explainer.expected_value,\n",
    "    data=X_scaled,\n",
    "    feature_names=X.columns\n",
    ")\n",
    "\n",
    "# Plot the beeswarm plot\n",
    "shap.plots.beeswarm(shap_explanation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pslrmHtRj5pl"
   },
   "outputs": [],
   "source": [
    "# Create the SHAP summary plot\n",
    "shap.summary_plot(shap_values, X_scaled, feature_names=X.columns, plot_type=\"violin\", show=False)\n",
    "\n",
    "# Save the plot with 300 dpi\n",
    "plt.savefig(\"SHAP_Summary_Plot.png\", dpi=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ecrCa2xI7sJ_"
   },
   "outputs": [],
   "source": [
    "# Dependence Plot\n",
    "shap.dependence_plot(\"Commercial\", shap_values, X_scaled, feature_names=X.columns, interaction_index=\"Commercial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K5-exAOl_BZQ"
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "# For first station: Bryant\n",
    "i = 0\n",
    "shap.force_plot(explainer.expected_value, shap_values[i], features=X.iloc[i], feature_names=X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ututlfd3dY8B"
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "# Loop through i from 0 to 7 for all stations\n",
    "for i in range(8):\n",
    "    # Generate force plot and save as PNG\n",
    "    force_plot = shap.force_plot(explainer.expected_value, shap_values[i], features=X.iloc[i], feature_names=X.columns)\n",
    "    shap.save_html(f'force_plot_i_{i}.html', force_plot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xDWB-FgK8z0K"
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "i = 0\n",
    "shap.decision_plot(explainer.expected_value, shap_values[i], X.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKryI6XnkYQ6"
   },
   "outputs": [],
   "source": [
    "shap.initjs()\n",
    "\n",
    "# Loop through i from 0 to 7\n",
    "for i in range(8):\n",
    "    # Generate decision_plot and save as PNG\n",
    "    decision_plot = shap.decision_plot(explainer.expected_value, shap_values[i], X.columns)\n",
    "    shap.save_html(f'decision_plot_{i}.html', force_plot)\n",
    "\n",
    "# Convert HTML to PNG using imgkit\n",
    "import imgkit\n",
    "\n",
    "# Loop through i from 0 to 7\n",
    "for i in range(8):\n",
    "    shap.initjs()\n",
    "    # Convert HTML to PNG using imgkit\n",
    "    imgkit.from_file(f'decision_plot_{i}.html', f'decision_plot_{i}.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M7oX0O8rmCkP"
   },
   "source": [
    "# **5. Developing Single-Output and Multi-Output Models for Forecasting Energy Consumption and Port Availability**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "z7Y8Qe8PmCkP"
   },
   "source": [
    "## **5.1. Data Acquisition, Cleaning, and Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-E95icOCmeir"
   },
   "outputs": [],
   "source": [
    "# Check the python version of this notebook\n",
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSNIQZaUmeir"
   },
   "outputs": [],
   "source": [
    "# Install and Import necessary libraries\n",
    "# Data processing tasks\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import missingno as msno\n",
    "import json\n",
    "import random\n",
    "\n",
    "# Colab settings\n",
    "from google.colab import output\n",
    "output.enable_custom_widget_manager()\n",
    "\n",
    "# Visualization tasks\n",
    "import matplotlib.pyplot as plt\n",
    "import ipywidgets as widgets\n",
    "import matplotlib.colors as mcolors\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "import seaborn as sns\n",
    "import folium\n",
    "from folium.plugins import HeatMap\n",
    "from folium.plugins import MarkerCluster\n",
    "from branca.colormap import linear\n",
    "!pip install keplergl\n",
    "from keplergl import KeplerGl\n",
    "\n",
    "# Geocoding tasks\n",
    "!pip install pgeocode\n",
    "import pgeocode\n",
    "\n",
    "# Time-series tasks\n",
    "import datetime as dt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from prophet import Prophet\n",
    "!pip install pandas holidays\n",
    "import holidays\n",
    "import calendar\n",
    "\n",
    "\n",
    "# Machine learning tasks\n",
    "!pip install catboost\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostRegressor, Pool\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, mean_absolute_percentage_error, r2_score\n",
    "\n",
    "# Deep learning tasks\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "!pip install optuna\n",
    "import optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RdbvnjW9meis"
   },
   "outputs": [],
   "source": [
    "# Reading the Electric Vehicle (EV) charging station dataset from a remote CSV file\n",
    "# The 'low_memory' parameter is set to False to ensure efficient memory usage during loading\n",
    "EV=pd.read_csv(\"https://data.cityofpaloalto.org/datasets/194693-electric-vehicle-charging-station-usage-july-2011-dec-2020.download/\", low_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5w56ltY3meit"
   },
   "outputs": [],
   "source": [
    "# Drop irrelevant columns\n",
    "EV.drop(EV[[\"MAC Address\", \"Org Name\", \"Transaction Date (Pacific Time)\", \"GHG Savings (kg)\", \"Gasoline Savings (gallons)\", \"Address 1\", \"City\", \"EVSE ID\", \"Port Type\", \"Port Number\", \"Plug Type\", \"Plug In Event Id\", \"State/Province\", \"Country\", \"Currency\", \"County\", \"System S/N\", \"Ended By\", \"Model Number\"]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WYXBqWKNmeit"
   },
   "outputs": [],
   "source": [
    "duplicates = EV.duplicated()\n",
    "\n",
    "num_duplicates = duplicates.sum()\n",
    "print(\"Number of duplicate rows:\", num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1YS9n9Oimeiu"
   },
   "outputs": [],
   "source": [
    "# Check for duplicate rows in the dataset\n",
    "duplicates = EV.duplicated()\n",
    "\n",
    "# Calculate the number of duplicate rows\n",
    "num_duplicates = duplicates.sum()\n",
    "print(\"Number of duplicate rows:\", num_duplicates)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B4X3I5memeiu"
   },
   "outputs": [],
   "source": [
    "# Remove duplicate rows from the dataset\n",
    "EV = EV.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6lNjhyy9meiv"
   },
   "outputs": [],
   "source": [
    "# Convert 'Start Date' and 'End Date' columns to Datetime\n",
    "EV[[\"Start Date\", \"End Date\"]]= EV[[\"Start Date\", \"End Date\"]].apply(pd.to_datetime, format='%m/%d/%Y %H:%M', errors='coerce') # Invalid formats would become NaT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVfuhg6Pmeiv"
   },
   "outputs": [],
   "source": [
    "# Convert 'Total Duration (hh:mm:ss)' and 'Charging Time (hh:mm:ss)' columns to Timedelta\n",
    "EV[[\"Total Duration (hh:mm:ss)\", \"Charging Time (hh:mm:ss)\"]]= EV[[\"Total Duration (hh:mm:ss)\", \"Charging Time (hh:mm:ss)\"]].apply(pd.to_timedelta, errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VCS1QpOvmeiv"
   },
   "outputs": [],
   "source": [
    "# Creating Charging_min Column\n",
    "EV[\"Charging_min\"] = EV[\"Charging Time (hh:mm:ss)\"].dt.total_seconds() / 60\n",
    "\n",
    "# Creating Duration_min Column\n",
    "EV[\"Duration_min\"] = EV[\"Total Duration (hh:mm:ss)\"].dt.total_seconds() / 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nQCSmzVBmeix"
   },
   "outputs": [],
   "source": [
    "# Calculate missing \"End Date\" values based on \"Start Date\" and \"Total Duration (hh:mm:ss)\" when \"End Date\" is NaT\n",
    "EV['End Date'] = EV.apply(lambda row: row['Start Date'] + row['Total Duration (hh:mm:ss)'] if pd.isnull(row['End Date']) else row['End Date'], axis=1)\n",
    "\n",
    "# Fill missing values in the \"Driver Postal Code\" and \"User ID\" columns with \"Unknown\" as a placeholder\n",
    "EV = EV.fillna({\"Driver Postal Code\": \"Unknown\", \"User ID\": \"Unknown\"})\n",
    "\n",
    "# Verify the effectiveness of missing value handling by checking the remaining missing values in the dataset\n",
    "EV.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e968r32kmeix"
   },
   "outputs": [],
   "source": [
    "# Remove the \"Start Time Zone\" and \"End Time Zone\" columns as they are no longer needed in the dataset\n",
    "EV.drop(EV[[\"Total Duration (hh:mm:ss)\", \"Charging Time (hh:mm:ss)\"]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fWP4jRG_meix"
   },
   "outputs": [],
   "source": [
    "# Calculate and display the count of unique combinations of \"Start Time Zone\" and \"End Time Zone\"\n",
    "EV[[\"Start Time Zone\", \"End Time Zone\"]].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K-SYzlHNmeiy"
   },
   "outputs": [],
   "source": [
    "# Define a time delta representing the difference between UTC and Pacific Standard Time (PST)\n",
    "PST= pd.to_timedelta(\"0 days 08:00:00\")\n",
    "# Define a time delta representing the difference between UTC and Pacific Daylight Time (PDT)\n",
    "PDT= pd.to_timedelta(\"0 days 07:00:00\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z4jGcRd4meiy"
   },
   "outputs": [],
   "source": [
    "# Adjust the \"Start Date\" for events where the \"Start Time Zone\" is UTC and the \"End Time Zone\" is PST\n",
    "EV.loc[(EV[\"Start Time Zone\"]== \"UTC\") & (EV[\"End Time Zone\"]== \"PST\"), \"Start Date\"] = EV.loc[(EV[\"Start Time Zone\"]== \"UTC\") & (EV[\"End Time Zone\"]== \"PST\"), \"Start Date\"] - PST\n",
    "\n",
    "# Adjust the \"Start Date\" for events where the \"Start Time Zone\" is UTC and the \"End Time Zone\" is PDT\n",
    "EV.loc[(EV[\"Start Time Zone\"]== \"UTC\") & (EV[\"End Time Zone\"]== \"PDT\"), \"Start Date\"] = EV.loc[(EV[\"Start Time Zone\"]== \"UTC\") & (EV[\"End Time Zone\"]== \"PDT\"), \"Start Date\"] - PDT\n",
    "\n",
    "# Adjust the \"End Date\" for events where the \"End Time Zone\" is UTC by subtracting the PDT time delta\n",
    "EV.loc[EV[\"End Time Zone\"]== \"UTC\", \"End Date\"] = EV[EV[\"End Time Zone\"]== \"UTC\"][\"End Date\"] - PDT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sXBvEVSdmeiy"
   },
   "outputs": [],
   "source": [
    "# Remove the \"Start Time Zone\" and \"End Time Zone\" columns as they are no longer needed in the dataset\n",
    "EV.drop(EV[[\"Start Time Zone\",\t\"End Time Zone\"]], axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ubmfmcHZmeiz"
   },
   "outputs": [],
   "source": [
    "# Group the 'EV' dataset by the \"Station Name\" column and count the occurrences of each station name\n",
    "EV.groupby(\"Station Name\")[\"Station Name\"].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CSBmy0ofmeiz"
   },
   "outputs": [],
   "source": [
    "# Create a copy of the original dataset 'EV' to preserve the plug identification data\n",
    "EV_s = EV.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0r7thSCwXQB4"
   },
   "outputs": [],
   "source": [
    "EV_ss= EV.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "b3pTzN7aTggw"
   },
   "outputs": [],
   "source": [
    "# Assuming EV is your DataFrame containing the station names\n",
    "# Create a new column named 'port number' and extract the port number from the station names\n",
    "#EV_ss['port number'] = EV['Station Name'].str.extract(r'#\\s*(\\d+)')\n",
    "# Create a new column 'port number' that captures port numbers either with '#' or as a standalone number at the end\n",
    "EV_ss['port number'] = EV['Station Name'].str.extract(r'#\\s*(\\d+)$| (\\d+)$')[0].fillna(EV['Station Name'].str.extract(r'(\\d+)$')[0])\n",
    "# Display the DataFrame to verify the changes\n",
    "EV_ss.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ji3YPJW-Xkg7"
   },
   "outputs": [],
   "source": [
    "# Define a list of values to be removed or modified from the \"Station Name\" column\n",
    "dropping_values= ['PALO ALTO CA / ', '#', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "# Loop through each value in 'dropping_values' and apply string operations to clean 'Station Name'\n",
    "for value in dropping_values:\n",
    "  EV_ss[\"Station Name\"]= EV_ss[\"Station Name\"].str.replace(value, '').str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SoEQ6Mrmei0"
   },
   "outputs": [],
   "source": [
    "# Define a list of values to be removed or modified from the \"Station Name\" column\n",
    "dropping_values= ['PALO ALTO CA / ', '#', '1', '2', '3', '4', '5', '6', '7', '8', '9']\n",
    "\n",
    "# Loop through each value in 'dropping_values' and apply string operations to clean 'Station Name'\n",
    "for value in dropping_values:\n",
    "  EV[\"Station Name\"]= EV[\"Station Name\"].str.replace(value, '').str.strip().str.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2GEFtvwtmei0"
   },
   "outputs": [],
   "source": [
    "# Group the 'EV' dataset by the cleaned \"Station Name\" column and calculate the mean latitude and longitude values\n",
    "EV[[\"Latitude\", \"Longitude\"]]= EV.groupby(\"Station Name\")[[\"Latitude\", \"Longitude\"]].transform('mean')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3XXw43PUmei0"
   },
   "outputs": [],
   "source": [
    "# Count charging events by User ID\n",
    "users= EV[\"User ID\"].value_counts()\n",
    "users"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mzIyJkBOmei1"
   },
   "outputs": [],
   "source": [
    "# Define bins and labels for user segmentation\n",
    "bins = [0, 2, 10, 100, 2000, float('inf')]\n",
    "labels = ['Passing', 'Infrequent', 'Occasional', 'Frequent', 'Unknown']\n",
    "\n",
    "# Perform user segmentation based on event counts\n",
    "user_segments = pd.cut(users, bins=bins, labels=labels, right=False)\n",
    "\n",
    "# Count the number of users in each segment\n",
    "segment_counts = user_segments.value_counts()\n",
    "\n",
    "# Display the user segmentation results\n",
    "print(\"User Segmentation:\")\n",
    "print(segment_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hkxgb7PHmei1"
   },
   "outputs": [],
   "source": [
    "# Create a mapping dictionary from the user_segments Series\n",
    "user_segments_mapping = user_segments.to_dict()\n",
    "\n",
    "# Add a new 'User Segments' column to the EV DataFrame by mapping user_segments\n",
    "EV['User Segment'] = EV['User ID'].map(user_segments_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6teuWhIjmei3"
   },
   "outputs": [],
   "source": [
    "dur_extreme_outliers= EV[EV[\"Duration_min\"] > 3000]\n",
    "\n",
    "dur_extreme_outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CKHq3k87mei3"
   },
   "outputs": [],
   "source": [
    "# Drop extreme outliers from Duration_min column\n",
    "EV.drop(dur_extreme_outliers.index, axis=0, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cO37I-JynFDe"
   },
   "source": [
    "## **5.2. Stations Choosen for Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-CYikAxTa5uZ"
   },
   "outputs": [],
   "source": [
    "# Get distinct coordinates of stations\n",
    "final_coordinates= EV[[\"Station Name\", \"Latitude\", \"Longitude\"]].drop_duplicates()\n",
    "paper_sts = final_coordinates[final_coordinates[\"Station Name\"].isin([\"BRYANT\", \"HAMILTON\", \"CAMBRIDGE\", \"RINCONADA LIB\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qXWvCf3wa38N"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "\n",
    "# Get distinct coordinates of stations\n",
    "final_coordinates = paper_sts\n",
    "\n",
    "# Create a folium map centered on Palo Alto\n",
    "palo_alto = folium.Map(location=[37.4419, -122.143936], zoom_start=14, tiles=\"CartoDB positron\")\n",
    "\n",
    "# Create a feature group for the markers\n",
    "marker_group = folium.FeatureGroup(name='Markers')\n",
    "\n",
    "# Add markers for charging stations\n",
    "for index, row in final_coordinates.iterrows():\n",
    "    # Check if the station name is \"BRYANT\"\n",
    "    if row[\"Station Name\"] == \"BRYANT\":\n",
    "        # Use a red color icon for BRYANT\n",
    "        icon = folium.DivIcon(\n",
    "            html=f'<i class=\"fa-solid fa-charging-station fa-xl\" style=\"color: red;\"></i>')\n",
    "    else:\n",
    "        # Default color for other stations\n",
    "        icon = folium.DivIcon(\n",
    "            html=f'<i class=\"fa-solid fa-charging-station fa-xl\" style=\"color: #1e00ff;\"></i>')\n",
    "\n",
    "    # Add the marker with the correct icon and popup\n",
    "    folium.Marker([row[\"Latitude\"], row[\"Longitude\"]], icon=icon, popup=row[\"Station Name\"]).add_to(marker_group)\n",
    "\n",
    "# Add the feature group to the map\n",
    "marker_group.add_to(palo_alto)\n",
    "\n",
    "# Display the map\n",
    "palo_alto"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cUzDM0gpnt5c"
   },
   "source": [
    "## **5.3. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QSHNmga3gqDy"
   },
   "source": [
    "### **5.3.1. Selecting Bryant station!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zSqyaxUhgxaa"
   },
   "outputs": [],
   "source": [
    "EV_s = EV_ss[EV_ss[\"Station Name\"]== \"BRYANT\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ih33AfdgYxEU"
   },
   "outputs": [],
   "source": [
    "EV_s.value_counts(\"port number\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2Tvea2-4T0Md"
   },
   "outputs": [],
   "source": [
    "EV_s.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "46YYCa1vcrxF"
   },
   "source": [
    "### **5.3.2. Data Preprocessing**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y2s75EEbjRMK"
   },
   "source": [
    "To prepare our data for modeling, we need to carefully select the time duration that will be most suitable for our hourly forecasting analysis. Given the hardware constraints and the desire for a manageable dataset, we have decided to segment the 10-year dataset into a more focused timeframe: from August 2017 to the end of February 2020.\n",
    "\n",
    "The rationale behind this selection is multi-faceted. Firstly, starting from August 2017 is pivotal because it marks the point at which the city began imposing fees for events. This policy change led to significant shifts in user and station behavior, as we have extensively explored in our Exploratory Data Analysis (EDA) section. Secondly, post-August 2017, the opening of new charging stations had a negligible impact on energy consumption, and it remained relatively stable. This stability allows us to incorporate total energy consumption data into our model without concerns about fluctuations caused by new station openings, which could lead to unreliable results.\n",
    "\n",
    "Furthermore, we have chosen the end of February 2020 as our endpoint. This date is significant because it represents the period just before the outbreak of the COVID-19 pandemic, an event that had a profound influence on various behaviors and activities. Since our dataset extends only until the end of 2020, there is no subsequent data available to capture and evaluate the impact of the COVID-19 shock. Therefore, in order to better model this event, we would require data beyond 2020.\n",
    "\n",
    "In essence, our data segmentation strategy is grounded in our comprehensive understanding of the dataset, particularly the two major events that had a significant and discernible impact. Armed with this knowledge, we can confidently proceed with the data preprocessing phase as a critical precursor to our modeling efforts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uPvqGVHZEWW6"
   },
   "source": [
    "Upon a thorough examination of our dataset, we've observed that it provides us with energy consumption values for each charging event, complete with corresponding start and end times. However, our goal is to work with data at hourly intervals, which necessitates resampling. Resampling using either the start date or end date alone presents a limitation, as it captures energy consumption at just one point in time, failing to account for the energy usage throughout the entire charging event duration. To address this constraint, we will undertake a series of data engineering steps in the upcoming sections.\n",
    "\n",
    "Our first step involves calculating the energy consumption per minute for each charging event. This calculation is achieved by dividing the energy consumption of each event by its duration in minutes, yielding the average energy consumption per minute for that event. While the optimal approach would be to compute this based on the actual charging duration, we are constrained by the absence of end time information for the charging itself. Therefore, we utilize the end time of the event, recognizing that this may result in a slight discrepancy since, as our analysis has shown, events typically have a total duration approximately 20 minutes longer than the actual charging duration. Nevertheless, this approach provides us with the closest approximation of the actual energy consumption per minute during each charging event, given the data available.\n",
    "\n",
    "Subsequently, we streamline the dataset to retain only three pertinent columns for our further analysis: the start date, end date, and the calculated energy consumption per minute values. Following this, we isolate the dataset to the defined start and end dates we discussed earlier. Finally, as a preparatory step for subsequent analyses, we will display the final rows of the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1y9KpVOvv08s"
   },
   "outputs": [],
   "source": [
    "# Calculate energy consumption per minute\n",
    "EV_s[\"Energy_p_min\"] = EV_s[\"Energy (kWh)\"] / EV_s[\"Duration_min\"]\n",
    "\n",
    "# Filter the Data to Start Date, End Date, and Consumption per minute columns\n",
    "mdf = EV_s[[\"Start Date\", \"End Date\", \"Energy_p_min\", \"port number\"]]\n",
    "\n",
    "# Define the start and end date for data slicing\n",
    "start = pd.to_datetime(\"2017-07-31 00:00:00\")  # Start date after initializing payments on events\n",
    "end = pd.to_datetime(\"2020-02-29 23:59:00\")    # End date before the COVID-19 lockdowns\n",
    "\n",
    "# Slice the data for modeling, selecting events within the specified time frame\n",
    "df = mdf[(mdf[\"Start Date\"] >= start) & (mdf[\"Start Date\"] < end)]\n",
    "\n",
    "# Display the last rows of the sliced dataset for inspection\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GhN-uhCPZNt3"
   },
   "outputs": [],
   "source": [
    "df.value_counts(\"port number\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uiN84ZuKH9XS"
   },
   "source": [
    "Our subsequent task involves the creation of a new DataFrame at minute intervals, spanning from the designated start time to the end time corresponding to the specified dates earmarked for modeling. We will use the pd.date_range function to generate a time series at minute intervals between the 'start' and 'end' timestamps, inclusive. Within this new DataFrame, we will introduce a column called \"Energy Consumption\" initially populated with empty values. These empty values will be progressively filled with the appropriate consumption data in the forthcoming steps of our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SrMokrDAd78v"
   },
   "outputs": [],
   "source": [
    "# Initialize the expanded DataFrame with both 'Energy Consumption' and 'Available Ports' columns\n",
    "expanded_data = {\n",
    "    'Date': pd.date_range(start=start, end=end, freq='T'),  # 'T' stands for minute frequency\n",
    "    'Energy Consumption': 0,  # Initialize the 'Energy Consumption' column with zeros\n",
    "    'Available Ports': 6  # Initialize the 'Available Ports' column with the total number of ports\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the expanded_data dictionary\n",
    "expanded_df = pd.DataFrame(expanded_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kcmbjXUBMfH4"
   },
   "source": [
    "In this section of our data preprocessing workflow, we take a crucial step towards creating a minute-level energy consumption dataset. Our objective is to iterate through the original DataFrame, df, which contains information about charging events' start and end time, and energy consumption for each minute within those events. This is a vital step as it transforms our data into the granular format necessary for modeling and forecasting.\n",
    "\n",
    "We will iterate through each row of the original DataFrame, df. For each event, we will extract the start date, end date, and energy consumption per minute. Next, we will efficiently update the expanded_df (our minute-level DataFrame) with the calculated energy consumption values. This update will be done using vectorized operations, which enhance performance when working with large datasets. Finally, we will print the last rows of the resulting expanded_df to visualize the transformed data. This meticulous process ensures that our minute-level energy consumption dataset is accurately constructed, setting the stage for further analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AmINjDI-ivMS"
   },
   "outputs": [],
   "source": [
    "# Iterate through the df and calculate energy consumption and available ports for each interval\n",
    "for _, row in df.iterrows():\n",
    "    start_date = row['Start Date']\n",
    "    end_date = row['End Date']\n",
    "    energy = row['Energy_p_min']\n",
    "\n",
    "    # Update the expanded_df with the calculated values using vectorized operations\n",
    "    mask = (expanded_df['Date'] >= start_date) & (expanded_df['Date'] <= end_date)\n",
    "    expanded_df.loc[mask, 'Energy Consumption'] += energy\n",
    "    expanded_df.loc[mask, 'Available Ports'] -= 1\n",
    "\n",
    "# Ensure all negative values in 'Available Ports' column are replaced with zeros\n",
    "expanded_df['Available Ports'] = expanded_df['Available Ports'].clip(lower=0)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(expanded_df.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kRUjbu8Ug6V0"
   },
   "outputs": [],
   "source": [
    "expanded_df[\"Available Ports\"].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MvNTX5C8l5jM"
   },
   "outputs": [],
   "source": [
    "expanded_df.groupby(\"Available Ports\")[\"Energy Consumption\"].mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r6o96rbNNxNc"
   },
   "source": [
    "Having meticulously prepared our minute-level energy consumption dataset, we are now poised to transform it into a more suitable format for our forecasting endeavors. As previously discussed, our primary objective is to conduct energy consumption forecasting at an hourly level. To facilitate this, we will resample the minute-level data into hourly intervals, and we will refer to this new dataset as 'eph,' an abbreviation for 'Energy Per Hour.'\n",
    "\n",
    "During our data preprocessing stage, you may have observed that we chose '2017-07-31' as our start date instead of '2017-08-01'. The rationale behind this decision was to capture those charging events that might have initiated on July 31 but extended into August 1. If we had selected '2017-08-01' as our start date, we would have inadvertently excluded these events, resulting in zero values for energy consumption during the initial intervals of August 1. Therefore, this adjustment allowed us to comprehensively account for energy consumption across the entire event duration.\n",
    "\n",
    "Now that we have successfully calculated energy consumption and resampled it into hourly intervals, it is appropriate to remove the data for July 31, aligning the dataset with the defined time range from '2017-08-01' to '2020-02-29'.\n",
    "\n",
    "In conclusion, we present the 'eph' data, which represents the culmination of our data preprocessing and engineering efforts in this section. This dataset is now primed and ready for further analysis, modeling, and forecasting tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PmVnGerEb_3K"
   },
   "outputs": [],
   "source": [
    "# Resample the minute-level data to hourly intervals and calculate the sum of energy consumption\n",
    "eph = expanded_df.resample('H', on='Date').agg({\n",
    "    'Energy Consumption': 'sum',\n",
    "    'Available Ports': 'last'  # Take the last available ports count for each hour\n",
    "})\n",
    "\n",
    "# Select data starting from August 1, 2017, onward to work with relevant data\n",
    "eph = eph.loc['2017-08-01 00:00:00':]\n",
    "\n",
    "# Display the 'eph' DataFrame containing energy consumption data at hourly intervals\n",
    "eph"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7S7Di0prQRQE"
   },
   "source": [
    "With the dataset in our possession, it's advantageous to gain a visual overview of its characteristics through a line plot. However, it's important to note that due to the hourly intervals spanning two and a half years, the resulting plot may appear intricate and may not provide easily interpretable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UDAFwOYQ6OlU"
   },
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Create a line plot using Seaborn to visualize 'eph' data\n",
    "sns.lineplot(data=eph, x='Date', y='Energy Consumption')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "01RjumqRmnGV"
   },
   "outputs": [],
   "source": [
    "# Set the figure size\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Create a line plot using Seaborn to visualize 'eph' data\n",
    "sns.lineplot(data=eph, x='Date', y='Available Ports')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CBr-CoVGRKJD"
   },
   "source": [
    "Next, we will delve deeper into our dataset's distribution characteristics by employing a box plot visualization. This approach allows us to identify potential outliers within the data, offering insights into the presence of extreme values that may require special attention or preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wwYZLfcS-E1"
   },
   "outputs": [],
   "source": [
    "sns.boxplot(data=eph, x='Energy Consumption')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pmb77D_CRge0"
   },
   "source": [
    "The data engineering approach we've applied has yielded several notable outcomes. First and foremost, it has enhanced the reliability of our dataset by providing more accurate and consistent energy consumption values at hourly intervals. Additionally, this approach has effectively mitigated the presence of outliers when compared to the original EV dataset, which underwent comprehensive analysis and preprocessing in Section 2 of this project.\n",
    "\n",
    "As we previously discussed in Section 2, our overarching objective from the project's outset has been to refrain from discarding or altering the actual values within this dataset. This commitment to preserving the integrity of the data remains unchanged, as it serves a dual purpose. Not only does it enable us to maintain the fidelity of our analysis and predictions, but it also allows us to capture and investigate anomalies and peak consumption behaviors, which are valuable aspects to consider in our modeling.\n",
    "\n",
    "However, it is prudent to take a closer look at these outliers to discern whether any meaningful insights can be derived from their presence. By doing so, we can potentially gain a deeper understanding of the dataset's characteristics and inform our subsequent modeling and forecasting efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wTZMU0YATFHe"
   },
   "outputs": [],
   "source": [
    "# Create a Pandas Series to store 'Energy Consumption' values from 'eph' DataFrame\n",
    "consumption_series = pd.Series(eph['Energy Consumption'])\n",
    "\n",
    "# Calculate the first quartile (Q1), third quartile (Q3), and interquartile range (IQR)\n",
    "Q1 = consumption_series.quantile(0.25)\n",
    "Q3 = consumption_series.quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "\n",
    "# Define lower and upper thresholds for identifying outliers using the IQR method\n",
    "lower_threshold = Q1 - 1.5 * IQR\n",
    "upper_threshold = Q3 + 1.5 * IQR\n",
    "\n",
    "# Extract outliers based on the defined thresholds and store them in the 'outliers' DataFrame\n",
    "outliers = eph[(eph.values < lower_threshold) | (eph.values > upper_threshold)]\n",
    "outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jqnCHO9IT-rA"
   },
   "source": [
    "These outliers in 'Energy Consumption' represent instances of unusually high energy demand that occurred during specific timestamps. Further examination of these outliers may provide valuable insights into the factors contributing to these spikes in energy consumption.\n",
    "\n",
    "The timestamps associated with these outliers are spread across various dates in the year 2019 and early 2020.\n",
    "Interestingly all of these outliers have happend during peak hours around 10-12. also the majority of these events are from january and the last months of 2019.\n",
    "The Energy Consumption values for these outliers range from approximately 170 to 183 kWh, indicating a significant surge in energy usage during these particular hours or days.\n",
    "\n",
    "The presence of these outliers suggests that there were instances of exceptionally high energy demand during the recorded hours or days. Investigating these data points further may reveal specific events or conditions that led to these spikes in energy consumption. Such insights could be valuable for understanding unusual patterns and potentially incorporating them into predictive models if they represent recurring phenomena."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1FhDAFqMBfC9"
   },
   "source": [
    "In conclusion, this section of our analysis has proven instrumental in transforming our data into a more manageable and meaningful format for energy consumption forecasting. Through meticulous data engineering and resampling, we have successfully converted minute-level data into hourly intervals, enhancing both accuracy and consistency. Moreover, by identifying and acknowledging the presence of outliers, we have laid the foundation for a more comprehensive understanding of our dataset. It is worth noting that we have chosen to retain these outliers within our data, recognizing their potential significance in capturing high peaks and potential anomalies. This decision aligns with our overarching goal of preserving the authenticity of our dataset and ensuring that our forecasting models reflect real-world dynamics. With our data now primed and enriched, we are well-equipped to embark on the next phase of our analysis, where we will construct and evaluate predictive models for energy consumption at EV charging stations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eKSWIF_d6lPf"
   },
   "source": [
    "### **5.3.3. Feature Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ohm8HFq9dz6P"
   },
   "source": [
    "In this section, we will employ a diverse set of feature engineering techniques to augment our dataset with valuable attributes, facilitating enhanced modeling capabilities. The objective here is to capture and integrate the various behavioral patterns observed throughout the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kmuewjUxd11p"
   },
   "source": [
    "Among the features we will incorporate, first two are holidays and weekends, which we extensively explored in the EDA section. Holidays and weekends have a discernible impact on energy consumption behavior, often leading to reduced activity at charging stations and, consequently, lower energy consumption records on those specific days."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o1wbYofEeDDC"
   },
   "source": [
    "Our approach begins by introducing holidays into our dataset. To accomplish this, we will utilize the 'holidays' library. We initiate the process by creating a date range spanning from the dataset's inception to its conclusion. Subsequently, we establish an empty list to collect holiday data. We then specify the United States holidays calendar and set the years' range from 2017 to 2021. This allows us to iterate through the date range, verifying whether each date is a holiday. These Boolean values will be denoted as '1' for holidays and '0' for non-holidays. Finally, we create a 'holiday' DataFrame, which includes the designated date range and corresponding holiday indicators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AFQ_doLr6lPh"
   },
   "outputs": [],
   "source": [
    "# Create a date range for dataset\n",
    "start_date = '2017-08-01'\n",
    "end_date = '2020-03-01'\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='H')\n",
    "\n",
    "# Initialize a list to store holiday data\n",
    "holiday_data = []\n",
    "\n",
    "# Choose the holiday calendar to use (US calendar)\n",
    "us_holidays = holidays.UnitedStates(years=list(range(2017, 2021)))\n",
    "\n",
    "# Iterate through the date range and check if each date is a holiday\n",
    "for date in date_range:\n",
    "    if date in us_holidays:\n",
    "        holiday_data.append(1)  # 1 indicates a holiday\n",
    "    else:\n",
    "        holiday_data.append(0)  # 0 indicates a non-holiday\n",
    "\n",
    "\n",
    "# Create a DataFrame with the date range and holiday data\n",
    "holiday = pd.DataFrame({'Date': date_range, 'IsHoliday': holiday_data})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XtdFms0eeQ-r"
   },
   "source": [
    "Now that we have acquired the holiday data, our next step is to seamlessly integrate it with our existing dataset, 'eph,' using the 'Date' column as the common identifier. This merger will result in a new DataFrame, aptly named 'df,' which combines the holiday information with our hourly energy consumption records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z3ZZo7bk6lPi"
   },
   "outputs": [],
   "source": [
    "# Merge the original dataset with the holiday DataFrame on the 'Date' column\n",
    "df = eph.merge(holiday, on='Date', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JGoODAOve0Vk"
   },
   "source": [
    "Our subsequent task involves extracting weekend-related data from the 'Date' column within our dataset. To accomplish this, we will establish a custom function capable of identifying whether a given date falls on a weekend. This function will then be applied to our 'df' DataFrame, resulting in the creation of a new column named 'IsWeekend,' where '1' signifies a weekend and '0' designates a weekday.\n",
    "\n",
    "To conclude this step, we will examine the .tail method on our dataset, df, to observe the incorporation of these new features. This allows us to visually inspect and verify the successful addition of the 'IsHoliday and ''IsWeekend' columns and their values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jVbdLxOG6lPi"
   },
   "outputs": [],
   "source": [
    "# Function to determine if a date is a weekend\n",
    "def is_weekend(date):\n",
    "    return 1 if date.weekday() in [5, 6] else 0\n",
    "\n",
    "# Create the \"IsWeekend\" column by applying the function to each date\n",
    "df[\"IsWeekend\"] = df[\"Date\"].apply(is_weekend)\n",
    "\n",
    "# Look at the last 5 rows of df\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1JNpYlnPf8La"
   },
   "source": [
    "With the acquisition of holiday and weekend columns, our progression continues with the extraction of temporal data from our 'Date' column. This step serves to enrich our dataset with time-related features, which in turn enhances our model's capacity to discern various consumption trends, patterns, and seasonal fluctuations within the data. Concretely, we will extract and segregate the year, month, day of the month, day of the week, and hour values from the 'Date' column, assigning them to their respective designated columns. Upon completion, we will set the 'Date' column as the dataset's index, ensuring a structured and organized format for our data with the newly incorporated temporal attributes. To observe the modifications made to our dataset, we will utilize the .head method to visualize the initial rows. This comprehensive approach equips our dataset with a rich array of time-related information, empowering our subsequent modeling efforts to capture the intricate dynamics of energy consumption with precision."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "25PLuSMh1QWS"
   },
   "outputs": [],
   "source": [
    "# Extract the year from the 'Date' column and create a new 'Year' column\n",
    "df['Year'] = df['Date'].dt.year\n",
    "\n",
    "# Extract the month from the 'Date' column and create a new 'Month' column\n",
    "df['Month'] = df['Date'].dt.month\n",
    "\n",
    "# Extract the day of the month from the 'Date' column and create a new 'day_of_month' column\n",
    "df['day_of_month'] = df['Date'].dt.day\n",
    "\n",
    "# Extract the day of the week (0 for Monday, 6 for Sunday) from the 'Date' column and create a new 'day_of_week' column\n",
    "df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "\n",
    "# Extract the hour from the 'Date' column and create a new 'Hour' column\n",
    "df['Hour'] = df['Date'].dt.hour\n",
    "\n",
    "# Set the 'Date' column as the index of the DataFrame\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Display the first few rows of the updated DataFrame\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MrqUXnAvAVhR"
   },
   "source": [
    "The features you've created using exponential moving averages (EMAs) with different window sizes can be valuable for capturing various trends and patterns in your time series dataset, especially if you're interested in analyzing short-term, medium-term, and long-term trends. However, whether these features are suitable for your dataset depends on the nature of the data and the specific forecasting task.\n",
    "\n",
    "Here's a breakdown of the features you've created:\n",
    "\n",
    "ema_1_day_std and ema_1_day: These features capture short-term trends with a 1-day window. The standard deviation (ema_1_day_std) and mean (ema_1_day) of the energy consumption over the last day can provide insights into daily fluctuations.\n",
    "\n",
    "ema_7_days_std and ema_7_days: These features capture medium-term trends with a 7-day window. Similar to the 1-day features, they provide the standard deviation (ema_7_days_std) and mean (ema_7_days) of energy consumption but over a 7-day period.\n",
    "\n",
    "ema_30_days and ema_30_days_std: These features capture long-term trends with a 30-day window. They provide the mean (ema_30_days) and standard deviation (ema_30_days_std) of energy consumption over the last 30 days.\n",
    "\n",
    "These features can be valuable for various forecasting tasks:\n",
    "\n",
    "Short-term features (1-day) can help capture daily patterns, such as day-night cycles or weekday-weekend variations.\n",
    "Medium-term features (7-days) may capture weekly or weekly seasonality patterns.\n",
    "Long-term features (30-days) can help identify longer-term trends or seasonal effects.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OHfbC9HwjDgW"
   },
   "source": [
    "In our ongoing exploration of feature engineering, we have introduced several features generated through Exponential Moving Averages (EMAs) with varying window sizes. These features can be immensely valuable for capturing diverse trends and patterns within our time series dataset, particularly when seeking to analyze short-term, medium-term, and long-term trends. Let's delve into the distinct features we've created:\n",
    "\n",
    "* **'ema_3_hour' and 'ema_3_hour_std':** These features are geared toward capturing very short-term trends with a 3-hour window. 'ema_3_hour' represents the mean energy consumption over the last 3 hours, offering insights into rapid fluctuations. In contrast, 'ema_3_hour_std' denotes the standard deviation of energy consumption during the same 3-hour period, providing information about the volatility and variability within these short time intervals.\n",
    "\n",
    "* **'ema_1_day_std' and 'ema_1_day':** These features are geared toward short-term trends, employing a 1-day window. 'ema_1_day_std' denotes the standard deviation of energy consumption over the last day, providing insights into daily fluctuations. In contrast, 'ema_1_day' represents the mean energy consumption over the same 1-day period, shedding light on daily patterns.\n",
    "\n",
    "* **'ema_7_days_std' and 'ema_7_days':** Designed for capturing medium-term trends, these features utilize a 7-day window. Much like their 1-day counterparts, they yield the standard deviation ('ema_7_days_std') and mean ('ema_7_days') of energy consumption, but this time over a 7-day span. These features can uncover weekly or weekly seasonality patterns.\n",
    "\n",
    "* **'ema_30_days' and 'ema_30_days_std':** Tailored for tracking long-term trends, these features employ a 30-day window. 'ema_30_days' signifies the mean energy consumption over the preceding 30 days, aiding in identifying longer-term trends or seasonal effects. Simultaneously, 'ema_30_days_std' delivers the standard deviation of energy consumption during the same 30-day period.\n",
    "\n",
    "To ensure data integrity, we will dropped rows containing NaN values resulting from the rolling calculations, maintaining the quality of our dataset. Subsequently, we will provide an initial glimpse of the updated dataset to facilitate further exploration and analysis. These comprehensive feature engineering efforts significantly enhance our dataset's ability to capture intricate consumption patterns across different time scales, empowering our forecasting models with valuable insights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "89ahnRgh23FI"
   },
   "outputs": [],
   "source": [
    "# Calculate Rolling Standard Deviation and Exponential Moving Averages (EMA) for short, medium and long-term trends\n",
    "\n",
    "# 3 hours for very-short-term trends\n",
    "ema_window_very_short = 3\n",
    "df['ema_3_hour'] = df['Energy Consumption'].ewm(span=ema_window_very_short, adjust=False).mean()\n",
    "df['ema_3_hour_std'] = df['Energy Consumption'].ewm(span=ema_window_very_short, adjust=False).std()\n",
    "\n",
    "# 1 day for short-term trends\n",
    "ema_window_short = 24\n",
    "df['ema_1_day_std'] = df['Energy Consumption'].ewm(span=ema_window_short, adjust=False).std()\n",
    "df['ema_1_day'] = df['Energy Consumption'].ewm(span=ema_window_short, adjust=False).mean()\n",
    "\n",
    "# 7 days for medium-term trends\n",
    "ema_window_medium = 7 * 24\n",
    "df['ema_7_days_std'] = df['Energy Consumption'].ewm(span=ema_window_medium, adjust=False).std()\n",
    "df['ema_7_days'] = df['Energy Consumption'].ewm(span=ema_window_medium, adjust=False).mean()\n",
    "\n",
    "# 30 days for long-term trends\n",
    "ema_window_long = 30 * 24\n",
    "df['ema_30_days'] = df['Energy Consumption'].ewm(span=ema_window_long, adjust=False).mean()\n",
    "df['ema_30_days_std'] = df['Energy Consumption'].ewm(span=ema_window_long, adjust=False).std()\n",
    "\n",
    "# Drop rows with NaN values (resulting from rolling calculations)\n",
    "#df.dropna(inplace=True)\n",
    "\n",
    "# Display the initial rows of the updated dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74U8LfCaw3Dw"
   },
   "outputs": [],
   "source": [
    "# Calculate Rolling Standard Deviation and Exponential Moving Averages (EMA) for short, medium and long-term trends\n",
    "\n",
    "# 3 hours for very-short-term trends\n",
    "ema_window_very_short = 3\n",
    "df['ema_3_hour_port'] = df['Available Ports'].ewm(span=ema_window_very_short, adjust=False).mean()\n",
    "df['ema_3_hour_std_port'] = df['Available Ports'].ewm(span=ema_window_very_short, adjust=False).std()\n",
    "\n",
    "# 1 day for short-term trends\n",
    "ema_window_short = 24\n",
    "df['ema_1_day_std_port'] = df['Available Ports'].ewm(span=ema_window_short, adjust=False).std()\n",
    "df['ema_1_day_port'] = df['Available Ports'].ewm(span=ema_window_short, adjust=False).mean()\n",
    "\n",
    "# 7 days for medium-term trends\n",
    "ema_window_medium = 7 * 24\n",
    "df['ema_7_days_std_port'] = df['Available Ports'].ewm(span=ema_window_medium, adjust=False).std()\n",
    "df['ema_7_days_port'] = df['Available Ports'].ewm(span=ema_window_medium, adjust=False).mean()\n",
    "\n",
    "# 30 days for long-term trends\n",
    "ema_window_long = 30 * 24\n",
    "df['ema_30_days_port'] = df['Available Ports'].ewm(span=ema_window_long, adjust=False).mean()\n",
    "df['ema_30_days_std_port'] = df['Available Ports'].ewm(span=ema_window_long, adjust=False).std()\n",
    "\n",
    "# Drop rows with NaN values (resulting from rolling calculations)\n",
    "df.dropna(inplace=True)\n",
    "\n",
    "# Display the initial rows of the updated dataset\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FNt9cMoEwnKS"
   },
   "outputs": [],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "q9vN6ix1x-N3"
   },
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qFJtnT_7xJSZ"
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"final.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jrw7jPbej5q-"
   },
   "source": [
    "#### Doing the Same Process for 3 Test Stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Agm7TJOlkOhB"
   },
   "outputs": [],
   "source": [
    "EV_3= EV_ss.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YRFMip-7kMxK"
   },
   "outputs": [],
   "source": [
    "EV_ham = EV_3[EV_3[\"Station Name\"]== \"HAMILTON\"]\n",
    "EV_cam = EV_3[EV_3[\"Station Name\"]== \"CAMBRIDGE\"]\n",
    "EV_rin = EV_3[EV_3[\"Station Name\"]== \"RINCONADA LIB\"]\n",
    "EV_bry = EV_3[EV_3[\"Station Name\"]== \"BRYANT\"]\n",
    "EV_bry_covid = EV_3[EV_3[\"Station Name\"]== \"BRYANT\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJeKpj2kj9s9"
   },
   "source": [
    "##### **Data Preprocessing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iRDJWKEuj9s-"
   },
   "outputs": [],
   "source": [
    "# Calculate energy consumption per minute\n",
    "EV_ham[\"Energy_p_min\"] = EV_ham[\"Energy (kWh)\"] / EV_ham[\"Duration_min\"]\n",
    "EV_cam[\"Energy_p_min\"] = EV_cam[\"Energy (kWh)\"] / EV_cam[\"Duration_min\"]\n",
    "EV_rin[\"Energy_p_min\"] = EV_rin[\"Energy (kWh)\"] / EV_rin[\"Duration_min\"]\n",
    "EV_bry[\"Energy_p_min\"] = EV_bry[\"Energy (kWh)\"] / EV_bry[\"Duration_min\"]\n",
    "\n",
    "# Filter the Data to Start Date, End Date, and Consumption per minute columns\n",
    "mdf_EV_ham = EV_ham[[\"Start Date\", \"End Date\", \"Energy_p_min\", \"port number\"]]\n",
    "mdf_EV_cam = EV_cam[[\"Start Date\", \"End Date\", \"Energy_p_min\", \"port number\"]]\n",
    "mdf_EV_rin = EV_rin[[\"Start Date\", \"End Date\", \"Energy_p_min\", \"port number\"]]\n",
    "mdf_EV_bry = EV_bry[[\"Start Date\", \"End Date\", \"Energy_p_min\", \"port number\"]]\n",
    "\n",
    "# Define the start and end date for data slicing\n",
    "start = pd.to_datetime(\"2019-10-25 00:00:00\")  # Start date for the test data + one month of rolling window\n",
    "end = pd.to_datetime(\"2020-07-01 00:00:00\")    # End date of the available data for test stations\n",
    "\n",
    "# Slice the data for modeling, selecting events within the specified time frame\n",
    "df_ham = mdf_EV_ham[(mdf_EV_ham[\"Start Date\"] >= start) & (mdf_EV_ham[\"Start Date\"] < end)]\n",
    "df_cam = mdf_EV_cam[(mdf_EV_cam[\"Start Date\"] >= start) & (mdf_EV_cam[\"Start Date\"] < end)]\n",
    "df_rin = mdf_EV_rin[(mdf_EV_rin[\"Start Date\"] >= start) & (mdf_EV_rin[\"Start Date\"] < end)]\n",
    "df_bry = mdf_EV_bry[(mdf_EV_bry[\"Start Date\"] >= start) & (mdf_EV_bry[\"Start Date\"] < end)]\n",
    "\n",
    "# Display the last rows of the sliced dataset for inspection\n",
    "#df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GnHxHb-gCCTw"
   },
   "outputs": [],
   "source": [
    "# Initialize the expanded DataFrame with both 'Energy Consumption' and 'Available Ports' columns\n",
    "expanded_data_ham = {\n",
    "    'Date': pd.date_range(start=start, end=end, freq='min'),\n",
    "    'Energy Consumption': 0,  # Initialize the 'Energy Consumption' column with zeros\n",
    "    'Available Ports': 2  # Initialize the 'Available Ports' column with the total number of ports\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the expanded_data dictionary\n",
    "expanded_df_ham = pd.DataFrame(expanded_data_ham)\n",
    "\n",
    "# Initialize the expanded DataFrame with both 'Energy Consumption' and 'Available Ports' columns\n",
    "expanded_data_cam = {\n",
    "    'Date': pd.date_range(start=start, end=end, freq='min'),\n",
    "    'Energy Consumption': 0,  # Initialize the 'Energy Consumption' column with zeros\n",
    "    'Available Ports': 5  # Initialize the 'Available Ports' column with the total number of ports\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the expanded_data dictionary\n",
    "expanded_df_cam = pd.DataFrame(expanded_data_cam)\n",
    "\n",
    "# Initialize the expanded DataFrame with both 'Energy Consumption' and 'Available Ports' columns\n",
    "expanded_data_rin = {\n",
    "    'Date': pd.date_range(start=start, end=end, freq='min'),\n",
    "    'Energy Consumption': 0,  # Initialize the 'Energy Consumption' column with zeros\n",
    "    'Available Ports': 3  # Initialize the 'Available Ports' column with the total number of ports\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the expanded_data dictionary\n",
    "expanded_df_rin = pd.DataFrame(expanded_data_rin)\n",
    "\n",
    "# Initialize the expanded DataFrame with both 'Energy Consumption' and 'Available Ports' columns\n",
    "expanded_data_bry = {\n",
    "    'Date': pd.date_range(start=start, end=end, freq='min'),\n",
    "    'Energy Consumption': 0,  # Initialize the 'Energy Consumption' column with zeros\n",
    "    'Available Ports': 6  # Initialize the 'Available Ports' column with the total number of ports\n",
    "}\n",
    "\n",
    "# Create a DataFrame from the expanded_data dictionary\n",
    "expanded_df_bry = pd.DataFrame(expanded_data_bry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TwUhFg4hD0E6"
   },
   "outputs": [],
   "source": [
    "# Iterate through the df and calculate energy consumption and available ports for each interval\n",
    "for _, row in df_ham.iterrows():\n",
    "    start_date = row['Start Date']\n",
    "    end_date = row['End Date']\n",
    "    energy = row['Energy_p_min']\n",
    "\n",
    "    # Update the expanded_df with the calculated values using vectorized operations\n",
    "    mask = (expanded_df_ham['Date'] >= start_date) & (expanded_df_ham['Date'] <= end_date)\n",
    "    expanded_df_ham.loc[mask, 'Energy Consumption'] += energy\n",
    "    expanded_df_ham.loc[mask, 'Available Ports'] -= 1\n",
    "\n",
    "# Ensure all negative values in 'Available Ports' column are replaced with zeros\n",
    "expanded_df_ham['Available Ports'] = expanded_df_ham['Available Ports'].clip(lower=0)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(expanded_df_ham.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V8VnGLTtDzbQ"
   },
   "outputs": [],
   "source": [
    "# Iterate through the df and calculate energy consumption and available ports for each interval\n",
    "for _, row in df_cam.iterrows():\n",
    "    start_date = row['Start Date']\n",
    "    end_date = row['End Date']\n",
    "    energy = row['Energy_p_min']\n",
    "\n",
    "    # Update the expanded_df with the calculated values using vectorized operations\n",
    "    mask = (expanded_df_cam['Date'] >= start_date) & (expanded_df_cam['Date'] <= end_date)\n",
    "    expanded_df_cam.loc[mask, 'Energy Consumption'] += energy\n",
    "    expanded_df_cam.loc[mask, 'Available Ports'] -= 1\n",
    "\n",
    "# Ensure all negative values in 'Available Ports' column are replaced with zeros\n",
    "expanded_df_cam['Available Ports'] = expanded_df_cam['Available Ports'].clip(lower=0)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(expanded_df_cam.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wamJ2hjWDyyG"
   },
   "outputs": [],
   "source": [
    "# Iterate through the df and calculate energy consumption and available ports for each interval\n",
    "for _, row in df_rin.iterrows():\n",
    "    start_date = row['Start Date']\n",
    "    end_date = row['End Date']\n",
    "    energy = row['Energy_p_min']\n",
    "\n",
    "    # Update the expanded_df with the calculated values using vectorized operations\n",
    "    mask = (expanded_df_rin['Date'] >= start_date) & (expanded_df_rin['Date'] <= end_date)\n",
    "    expanded_df_rin.loc[mask, 'Energy Consumption'] += energy\n",
    "    expanded_df_rin.loc[mask, 'Available Ports'] -= 1\n",
    "\n",
    "# Ensure all negative values in 'Available Ports' column are replaced with zeros\n",
    "expanded_df_rin['Available Ports'] = expanded_df_rin['Available Ports'].clip(lower=0)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(expanded_df_rin.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d6j7KXTmj9s_"
   },
   "outputs": [],
   "source": [
    "# Iterate through the df and calculate energy consumption and available ports for each interval\n",
    "for _, row in df_bry.iterrows():\n",
    "    start_date = row['Start Date']\n",
    "    end_date = row['End Date']\n",
    "    energy = row['Energy_p_min']\n",
    "\n",
    "    # Update the expanded_df with the calculated values using vectorized operations\n",
    "    mask = (expanded_df_bry['Date'] >= start_date) & (expanded_df_bry['Date'] <= end_date)\n",
    "    expanded_df_bry.loc[mask, 'Energy Consumption'] += energy\n",
    "    expanded_df_bry.loc[mask, 'Available Ports'] -= 1\n",
    "\n",
    "# Ensure all negative values in 'Available Ports' column are replaced with zeros\n",
    "expanded_df_bry['Available Ports'] = expanded_df_bry['Available Ports'].clip(lower=0)\n",
    "\n",
    "# Print the resulting DataFrame\n",
    "print(expanded_df_bry.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_uCUfFmG0C0"
   },
   "outputs": [],
   "source": [
    "# Resample the minute-level data to hourly intervals and calculate the sum of energy consumption\n",
    "eph_ham = expanded_df_ham.resample('H', on='Date').agg({\n",
    "    'Energy Consumption': 'sum',\n",
    "    'Available Ports': 'last'  # Take the last available ports count for each hour\n",
    "})\n",
    "\n",
    "# Select data to work with relevant data\n",
    "eph_ham = eph_ham.loc[:'2020-06-30 23:00:00']\n",
    "\n",
    "# Display the 'eph' DataFrame containing energy consumption data at hourly intervals\n",
    "eph_ham"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "74TyEWAUGz58"
   },
   "outputs": [],
   "source": [
    "# Resample the minute-level data to hourly intervals and calculate the sum of energy consumption\n",
    "eph_cam = expanded_df_cam.resample('H', on='Date').agg({\n",
    "    'Energy Consumption': 'sum',\n",
    "    'Available Ports': 'last'  # Take the last available ports count for each hour\n",
    "})\n",
    "\n",
    "# Select data to work with relevant data\n",
    "eph_cam = eph_cam.loc[:'2020-06-30 23:00:00']\n",
    "\n",
    "# Display the 'eph' DataFrame containing energy consumption data at hourly intervals\n",
    "eph_cam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1Xse5_qSGzyI"
   },
   "outputs": [],
   "source": [
    "# Resample the minute-level data to hourly intervals and calculate the sum of energy consumption\n",
    "eph_rin = expanded_df_rin.resample('H', on='Date').agg({\n",
    "    'Energy Consumption': 'sum',\n",
    "    'Available Ports': 'last'  # Take the last available ports count for each hour\n",
    "})\n",
    "\n",
    "# Select data to work with relevant data\n",
    "eph_rin = eph_rin.loc[:'2020-06-30 23:00:00']\n",
    "\n",
    "# Display the 'eph' DataFrame containing energy consumption data at hourly intervals\n",
    "eph_rin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KU7GGON5j9s_"
   },
   "outputs": [],
   "source": [
    "# Resample the minute-level data to hourly intervals and calculate the sum of energy consumption\n",
    "eph_bry = expanded_df_bry.resample('H', on='Date').agg({\n",
    "    'Energy Consumption': 'sum',\n",
    "    'Available Ports': 'last'  # Take the last available ports count for each hour\n",
    "})\n",
    "\n",
    "# Select data to work with relevant data\n",
    "eph_bry = eph_bry.loc[:'2020-06-30 23:00:00']\n",
    "\n",
    "# Display the 'eph' DataFrame containing energy consumption data at hourly intervals\n",
    "eph_bry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ytJbhJD3j9tA"
   },
   "source": [
    "##### **Feature Engineering**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kaQh4cc1j9tB"
   },
   "outputs": [],
   "source": [
    "# Create a date range for dataset\n",
    "start_date = '2019-10-25'\n",
    "end_date = '2020-07-01'\n",
    "date_range = pd.date_range(start=start_date, end=end_date, freq='h')\n",
    "\n",
    "# Initialize a list to store holiday data\n",
    "holiday_data = []\n",
    "\n",
    "# Choose the holiday calendar to use (US calendar)\n",
    "us_holidays = holidays.UnitedStates(years=list(range(2019, 2021)))\n",
    "\n",
    "# Iterate through the date range and check if each date is a holiday\n",
    "for date in date_range:\n",
    "    if date in us_holidays:\n",
    "        holiday_data.append(1)  # 1 indicates a holiday\n",
    "    else:\n",
    "        holiday_data.append(0)  # 0 indicates a non-holiday\n",
    "\n",
    "\n",
    "# Create a DataFrame with the date range and holiday data\n",
    "holiday = pd.DataFrame({'Date': date_range, 'IsHoliday': holiday_data})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yV7PlW2-j9tB"
   },
   "outputs": [],
   "source": [
    "# Merge the original dataset with the holiday DataFrame on the 'Date' column\n",
    "df_ham = eph_ham.merge(holiday, on='Date', how='left')\n",
    "df_cam = eph_cam.merge(holiday, on='Date', how='left')\n",
    "df_rin = eph_rin.merge(holiday, on='Date', how='left')\n",
    "df_bry = eph_bry.merge(holiday, on='Date', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MdracAyKj9tB"
   },
   "outputs": [],
   "source": [
    "# Function to determine if a date is a weekend\n",
    "def is_weekend(date):\n",
    "    return 1 if date.weekday() in [5, 6] else 0\n",
    "\n",
    "# Create the \"IsWeekend\" column by applying the function to each date\n",
    "df_ham[\"IsWeekend\"] = df_ham[\"Date\"].apply(is_weekend)\n",
    "df_cam[\"IsWeekend\"] = df_cam[\"Date\"].apply(is_weekend)\n",
    "df_rin[\"IsWeekend\"] = df_rin[\"Date\"].apply(is_weekend)\n",
    "df_bry[\"IsWeekend\"] = df_bry[\"Date\"].apply(is_weekend)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MXySgR67j9tB"
   },
   "outputs": [],
   "source": [
    "# Define a function to perform the operations on each DataFrame\n",
    "def process_dataframe(df):\n",
    "    # Extract year, month, day, day of the week, and hour from 'Date' column\n",
    "    df['Year'] = df['Date'].dt.year\n",
    "    df['Month'] = df['Date'].dt.month\n",
    "    df['day_of_month'] = df['Date'].dt.day\n",
    "    df['day_of_week'] = df['Date'].dt.dayofweek\n",
    "    df['Hour'] = df['Date'].dt.hour\n",
    "\n",
    "    # Set the 'Date' column as the index of the DataFrame\n",
    "    df.set_index('Date', inplace=True)\n",
    "\n",
    "    # Return the modified DataFrame\n",
    "    return df\n",
    "\n",
    "# Apply the function to each of your dataframes\n",
    "df_ham = process_dataframe(df_ham)\n",
    "df_cam = process_dataframe(df_cam)\n",
    "df_rin = process_dataframe(df_rin)\n",
    "df_bry = process_dataframe(df_bry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2P8XdvvjecmL"
   },
   "outputs": [],
   "source": [
    "# Define a function to calculate the rolling standard deviation and exponential moving averages\n",
    "def calculate_ema_and_std(df, scaler_columns):\n",
    "    # Define the window sizes for very short, short, medium, and long-term trends\n",
    "    ema_window_very_short = 3               # 3 hours\n",
    "    ema_window_short = 24                   # 1 day\n",
    "    ema_window_medium = 7 * 24              # 7 days\n",
    "    ema_window_long = 30 * 24               # 30 days\n",
    "\n",
    "    # Calculate EMA and standard deviation for 'Energy Consumption'\n",
    "    df['ema_3_hour'] = df['Energy Consumption'].ewm(span=ema_window_very_short, adjust=False).mean()\n",
    "    df['ema_3_hour_std'] = df['Energy Consumption'].ewm(span=ema_window_very_short, adjust=False).std()\n",
    "    df['ema_1_day'] = df['Energy Consumption'].ewm(span=ema_window_short, adjust=False).mean()\n",
    "    df['ema_1_day_std'] = df['Energy Consumption'].ewm(span=ema_window_short, adjust=False).std()\n",
    "    df['ema_7_days'] = df['Energy Consumption'].ewm(span=ema_window_medium, adjust=False).mean()\n",
    "    df['ema_7_days_std'] = df['Energy Consumption'].ewm(span=ema_window_medium, adjust=False).std()\n",
    "    df['ema_30_days'] = df['Energy Consumption'].ewm(span=ema_window_long, adjust=False).mean()\n",
    "    df['ema_30_days_std'] = df['Energy Consumption'].ewm(span=ema_window_long, adjust=False).std()\n",
    "\n",
    "    # Calculate EMA and standard deviation for 'Available Ports'\n",
    "    df['ema_3_hour_port'] = df['Available Ports'].ewm(span=ema_window_very_short, adjust=False).mean()\n",
    "    df['ema_3_hour_std_port'] = df['Available Ports'].ewm(span=ema_window_very_short, adjust=False).std()\n",
    "    df['ema_1_day_port'] = df['Available Ports'].ewm(span=ema_window_short, adjust=False).mean()\n",
    "    df['ema_1_day_std_port'] = df['Available Ports'].ewm(span=ema_window_short, adjust=False).std()\n",
    "    df['ema_7_days_port'] = df['Available Ports'].ewm(span=ema_window_medium, adjust=False).mean()\n",
    "    df['ema_7_days_std_port'] = df['Available Ports'].ewm(span=ema_window_medium, adjust=False).std()\n",
    "    df['ema_30_days_port'] = df['Available Ports'].ewm(span=ema_window_long, adjust=False).mean()\n",
    "    df['ema_30_days_std_port'] = df['Available Ports'].ewm(span=ema_window_long, adjust=False).std()\n",
    "\n",
    "    # Drop rows with NaN values generated from rolling calculations\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    # Reorder columns to match the feature_scaler columns\n",
    "    df = df[scaler_columns]\n",
    "\n",
    "    # Return the processed DataFrame\n",
    "    return df\n",
    "\n",
    "# Apply the function to each DataFrame with the correct column order\n",
    "scaler_columns = ['Energy Consumption', 'Available Ports', 'IsHoliday', 'IsWeekend', 'Year', 'Month', 'day_of_month', 'day_of_week', 'Hour',\n",
    "                  'ema_3_hour', 'ema_3_hour_std', 'ema_1_day_std', 'ema_1_day', 'ema_7_days_std', 'ema_7_days',\n",
    "                  'ema_30_days', 'ema_30_days_std', 'ema_3_hour_port', 'ema_3_hour_std_port', 'ema_1_day_std_port',\n",
    "                  'ema_1_day_port', 'ema_7_days_std_port', 'ema_7_days_port', 'ema_30_days_port', 'ema_30_days_std_port']\n",
    "\n",
    "df_ham = calculate_ema_and_std(df_ham, scaler_columns)\n",
    "df_cam = calculate_ema_and_std(df_cam, scaler_columns)\n",
    "df_rin = calculate_ema_and_std(df_rin, scaler_columns)\n",
    "df_bry = calculate_ema_and_std(df_bry, scaler_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nNBpb9QXRx8z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Given test start date\n",
    "test_start_date = pd.Timestamp(\"2019-11-27 16:00\")\n",
    "\n",
    "# Calculate the start date by subtracting 168 hours\n",
    "start_date_for_dfs = test_start_date - pd.Timedelta(hours=168)\n",
    "\n",
    "# Display the calculated start date\n",
    "start_date_for_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VTe-Kmt9Kkp_"
   },
   "outputs": [],
   "source": [
    "# To handle the sequencing for the fututre RNN data preprocessing\n",
    "df_ham = df_ham[df_ham.index >= start_date_for_dfs].reset_index()\n",
    "df_cam = df_cam[df_cam.index >= start_date_for_dfs].reset_index()\n",
    "df_rin = df_rin[df_rin.index >= start_date_for_dfs].reset_index()\n",
    "df_bry = df_bry[df_bry.index >= start_date_for_dfs].reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3oidaIbCj9tC"
   },
   "outputs": [],
   "source": [
    "df_ham.to_csv(\"/content/drive/MyDrive/EV_pred/df_ham.csv\")\n",
    "df_cam.to_csv(\"/content/drive/MyDrive/EV_pred/df_cam.csv\")\n",
    "df_rin.to_csv(\"/content/drive/MyDrive/EV_pred/df_rin.csv\")\n",
    "df_bry.to_csv(\"/content/drive/MyDrive/EV_pred/df_bry.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Mp5GcvYZmCY-"
   },
   "source": [
    "### **5.3.4. Data Splitting and Normalization**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1q_XDsOXmdA_"
   },
   "source": [
    "Now that the feature engineering section is complete, we're ready to transition into the next section, which comprises two essential steps: splitting the dataset and normalizing the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q1VIyv4vY9Cn"
   },
   "source": [
    "In preparation for our modeling efforts, we'll delve into the critical task of splitting our dataset into train, validation, and test sets. While the detailed usage of validation data will be elaborated upon in the subsequent \"Building Model\" section, it's important to note that this step is pivotal for model assessment and fine-tuning.\n",
    "\n",
    "Given the time series nature of our dataset, a random split would not be prudent, as it would disregard the temporal and seasonal aspects inherent in the data, potentially leading to inaccurate results. To preserve the temporal order, we will adopt a straightforward approach by allocating the initial 70% of the data as the training set, the subsequent 20% as the validation set, and the final 10% as the test set. Subsequently, we will define the feature (X) and target (y) data for each of these sets, with the objective of forecasting the target values (y) - Energy Consumption - based on the features (X). This systematic approach ensures that our model can effectively learn from historical patterns and validate its performance against unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ekbu9EuAQPeV"
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"/content/drive/MyDrive/final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IqgENRi7W276"
   },
   "outputs": [],
   "source": [
    "df_ham= pd.read_csv(\"/content/drive/MyDrive/EV_pred/df_ham.csv\")\n",
    "df_cam= pd.read_csv(\"/content/drive/MyDrive/EV_pred/df_cam.csv\")\n",
    "df_rin= pd.read_csv(\"/content/drive/MyDrive/EV_pred/df_rin.csv\")\n",
    "df_bry= pd.read_csv(\"/content/drive/MyDrive/EV_pred/df_bry.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DoYuhuql_o1d"
   },
   "outputs": [],
   "source": [
    "# Determine the sizes of the training, validation, and test sets based on specified percentages\n",
    "train_size = int(len(df) * 0.7)\n",
    "val_size = int(len(df) * 0.2)\n",
    "test_size = len(df) - train_size - val_size\n",
    "\n",
    "# Split the dataset into training, validation, and test sets based on the calculated sizes\n",
    "train_data= df[:train_size]\n",
    "val_data= df[train_size:train_size+val_size]\n",
    "test_data= df[train_size+val_size:]\n",
    "\n",
    "# Prepare the features (X) and target (y) data for the training, validation, and test sets\n",
    "X_train= train_data.drop([\"Date\", \"Energy Consumption\", \"Available Ports\", 'ema_3_hour_port',\n",
    "       'ema_3_hour_std_port', 'ema_1_day_std_port', 'ema_1_day_port',\n",
    "       'ema_7_days_std_port', 'ema_7_days_port', 'ema_30_days_port',\n",
    "       'ema_30_days_std_port'], axis=1)\n",
    "y_train= train_data[\"Energy Consumption\"]\n",
    "X_val= val_data.drop([\"Date\", \"Energy Consumption\", \"Available Ports\", 'ema_3_hour_port',\n",
    "       'ema_3_hour_std_port', 'ema_1_day_std_port', 'ema_1_day_port',\n",
    "       'ema_7_days_std_port', 'ema_7_days_port', 'ema_30_days_port',\n",
    "       'ema_30_days_std_port'], axis=1)\n",
    "y_val= val_data[\"Energy Consumption\"]\n",
    "X_test= test_data.drop([\"Date\", \"Energy Consumption\", \"Available Ports\", 'ema_3_hour_port',\n",
    "       'ema_3_hour_std_port', 'ema_1_day_std_port', 'ema_1_day_port',\n",
    "       'ema_7_days_std_port', 'ema_7_days_port', 'ema_30_days_port',\n",
    "       'ema_30_days_std_port'], axis=1)\n",
    "y_test= test_data[\"Energy Consumption\"]\n",
    "\n",
    "# Save y_test_dates with associated dates for model evaluation phase\n",
    "y_test_dates= y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pRyX9I8qVA9z"
   },
   "outputs": [],
   "source": [
    "# Determine the sizes of the training, validation, and test sets based on specified percentages\n",
    "train_size = int(len(df) * 0.7)\n",
    "val_size = int(len(df) * 0.2)\n",
    "test_size = len(df) - train_size - val_size\n",
    "\n",
    "# Split the dataset into training, validation, and test sets based on the calculated sizes\n",
    "train_data= df[:train_size]\n",
    "val_data= df[train_size:train_size+val_size]\n",
    "test_data= df[train_size+val_size:]\n",
    "\n",
    "# Prepare the features (X) and target (y) data for the training, validation, and test sets\n",
    "X_train_port= train_data.drop([\"Date\", \"Energy Consumption\", \"Available Ports\", 'ema_3_hour',\n",
    "       'ema_3_hour_std', 'ema_1_day_std', 'ema_1_day', 'ema_7_days_std',\n",
    "       'ema_7_days', 'ema_30_days', 'ema_30_days_std'], axis=1)\n",
    "y_train_port= train_data[\"Available Ports\"]\n",
    "X_val_port= val_data.drop([\"Date\", \"Energy Consumption\", \"Available Ports\", 'ema_3_hour',\n",
    "       'ema_3_hour_std', 'ema_1_day_std', 'ema_1_day', 'ema_7_days_std',\n",
    "       'ema_7_days', 'ema_30_days', 'ema_30_days_std'], axis=1)\n",
    "y_val_port= val_data[\"Available Ports\"]\n",
    "X_test_port= test_data.drop([\"Date\", \"Energy Consumption\", \"Available Ports\", 'ema_3_hour',\n",
    "       'ema_3_hour_std', 'ema_1_day_std', 'ema_1_day', 'ema_7_days_std',\n",
    "       'ema_7_days', 'ema_30_days', 'ema_30_days_std'], axis=1)\n",
    "y_test_port= test_data[\"Available Ports\"]\n",
    "\n",
    "# Save y_test_dates with associated dates for model evaluation phase\n",
    "y_test_dates_port= y_test_port"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FUVb1WsQzDER"
   },
   "outputs": [],
   "source": [
    "# Prepare the features (X) and target (y) data for the training, validation, and test sets\n",
    "X_train_multi= train_data.drop([\"Date\", \"Energy Consumption\", \"Available Ports\"], axis=1)\n",
    "X_val_multi= val_data.drop([\"Date\", \"Energy Consumption\", \"Available Ports\"], axis=1)\n",
    "#X_test_multi= test_data.drop([\"Date\", \"Energy Consumption\", \"Available Ports\"], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SYCte-Zo5rv"
   },
   "source": [
    "Let's examine the shapes of these datasets to gain a deeper understanding of their dimensions and sizes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wPyev8gsX1xr"
   },
   "outputs": [],
   "source": [
    "# Display the shapes of the datasets\n",
    "print('Train Data Shape:', X_train.shape, y_train.shape)\n",
    "print('Validation Data Shape:', X_val.shape, y_val.shape)\n",
    "print('Test Data Shape:', X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a6jkWRGGVp8V"
   },
   "outputs": [],
   "source": [
    "# Display the shapes of the datasets\n",
    "print('Train Data Shape Ports:', X_train_port.shape, y_train_port.shape)\n",
    "print('Validation Data Shape Ports:', X_val_port.shape, y_val_port.shape)\n",
    "print('Test Data Shape Ports:', X_test_port.shape, y_test_port.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sJhNSH53Yhhy"
   },
   "source": [
    "Our last crucial preparatory step before constructing our model involves data normalizationâ€”a fundamental practice that ensures all features contribute evenly during model training and leads to improved model convergence and performance. Normalization scales the data to a consistent range, preventing certain features from dominating the learning process and enhancing the model's ability to generalize.\n",
    "\n",
    "To accomplish this, we'll employ Scikit-Learn's MinMaxScaler, which rescales features to fall within a specified range, typically [0, 1]. This scaling ensures that the data retains its relative proportions while being transformed to a uniform range suitable for machine learning algorithms.\n",
    "\n",
    "> A critical aspect of this process is that we fit the normalization parameters separately for both the feature matrix (X) and the target variable (y) using only the training data. Subsequently, we apply the same transformation to the validation and test datasets. This meticulous separation of training, validation, and test sets during normalization is vital to prevent data leakageâ€”ensuring that our model learns from historical patterns without being influenced by unseen data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0oQmFBuibLA0"
   },
   "outputs": [],
   "source": [
    "# Initialize separate MinMaxScaler for feature and target variables\n",
    "feature_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the feature scaler on the training data and transform the train, val, and test features\n",
    "X_train_normalized = feature_scaler.fit_transform(X_train)\n",
    "X_val_normalized = feature_scaler.transform(X_val)\n",
    "X_test_normalized = feature_scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qDTH2PM_V7zH"
   },
   "outputs": [],
   "source": [
    "# Initialize separate MinMaxScaler for feature and target variables\n",
    "feature_scaler = MinMaxScaler()\n",
    "\n",
    "# Fit the feature scaler on the training data and transform the train, val, and test features\n",
    "X_train_normalized_port = feature_scaler.fit_transform(X_train_port)\n",
    "X_val_normalized_port = feature_scaler.transform(X_val_port)\n",
    "X_test_normalized_port = feature_scaler.transform(X_test_port)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TWEpBt5ZaG8u"
   },
   "source": [
    "With the completion of data normalization, we now possess six essential datasets, each meticulously normalized to ensure consistent feature scaling. These datasets include X_train_normalized, y_train_normalized, X_val_normalized, y_val_normalized, X_test_normalized, and y_test_normalized. These normalized representations of our data are primed for model training, enabling us to embark on the subsequent phases of model development, evaluation, and forecasting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2jQKf0fxsNK3"
   },
   "source": [
    "## **5.4. Model Building**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GcdDrsWw_beb"
   },
   "source": [
    "### **5.4.1. Sequencing Data For RNN Models**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1W6p4oLPep4y"
   },
   "source": [
    "Recurrent Neural Networks (RNNs), notably LSTM (Long Short-Term Memory) and GRU (Gated Recurrent Unit), represent a category of deep learning models tailored for handling sequential data, making them particularly well-suited for tasks like time series forecasting. These models process input sequences incrementally, accounting for both the current input and the prior hidden state, which enables them to capture temporal dependencies effectively.\n",
    "\n",
    "The concept of data sequencing entails structuring and representing data in a sequential or temporal fashion. This process arranges data observations in a specific order, often dictated by a temporal or sequence-related attribute, to capture inherent temporal patterns. When dealing with time series data, sequential data, or any dataset where the order of observations is meaningful, sequencing becomes crucial for uncovering valuable insights.\n",
    "\n",
    "In our context, our primary goal is to unveil and model the intricate temporal dependencies and sequential patterns inherently present in the Energy Consumption data. By focusing exclusively on the sequential aspects of the data, we empower our LSTM and GRU models to excel at capturing and comprehending the underlying time series or sequential patterns governing the phenomenon under study. These models are explicitly designed for handling sequential information, rendering them highly suitable for tasks such as time series forecasting, natural language processing, and speech recognition, where decoding sequential intricacies is paramount. By refraining from incorporating additional feature-engineered data, we prioritize the purity of the sequential information, enabling the models to distill and leverage the intrinsic patterns within the data, thereby yielding more effective and interpretable results.\n",
    "\n",
    "In selecting a sequence length of 168 hours, we refer to the need to look back at the previous week's energy consumption data to inform predictions. However, this sequencing necessitates access to the past 168 data points for our datasets. To ensure we don't lose any data points during this sequencing process, we adopt a strategy of appending last 168 data points from the training set to the beginning of the validation set and also repeat the process for the last 168 data points of the validation set to the beginning of the test set. This approach guarantees that, post-sequencing, we retain all data points in our validation and test sets, mitigating any potential loss of information due to sequencing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZqQ3j_Li0PG"
   },
   "outputs": [],
   "source": [
    "# Get the last 168 rows (last week) from y_train_normalized\n",
    "last_168_rows_y_train = y_train[-168:]\n",
    "\n",
    "# Concatenate last_168_rows_y_train with y_val_normalized\n",
    "y_val_with_last_168 = np.concatenate((last_168_rows_y_train, y_val), axis=0)\n",
    "\n",
    "# Do the same for y_test_normalized\n",
    "last_168_rows_y_val = y_val[-168:]\n",
    "y_test_with_last_168 = np.concatenate((last_168_rows_y_val, y_test), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P-MqyaLvfoHd"
   },
   "outputs": [],
   "source": [
    "# Get the last 168 rows (last week) from y_train_normalized\n",
    "last_168_rows_y_train_port = y_train_port[-168:]\n",
    "\n",
    "# Concatenate last_168_rows_y_train with y_val_normalized\n",
    "y_val_with_last_168_port = np.concatenate((last_168_rows_y_train_port, y_val_port), axis=0)\n",
    "\n",
    "# Do the same for y_test_normalized\n",
    "last_168_rows_y_val_port = y_val_port[-168:]\n",
    "y_test_with_last_168_port = np.concatenate((last_168_rows_y_val_port, y_test_port), axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tzhFOq8joDO"
   },
   "source": [
    "With data integrity ensured for our models, we are ready to embark on the data sequencing process. To achieve this, we will create a sequencing function that takes the data and a specified \"look-back\" number as inputs. This function will yield sequenced data in two parts: X and y arrays.\n",
    "\n",
    "In this context, X will represent the past values of energy consumption for the defined sequence length, equivalent to the previous week's data. Meanwhile, y will denote the energy consumption value that follows this past week's sequence. This organization allows us to frame our predictive modeling problem effectively, as we seek to forecast energy consumption based on historical patterns observed in the past week's data.\n",
    "\n",
    "We will apply our sequencing function to the normalized energy consumption data for the train, validation, and test sets. This process will yield the respective X and y values for each of these datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uBGqg9uTfXrp"
   },
   "outputs": [],
   "source": [
    "# Create sequences of data\n",
    "def create_sequences(data, look_back):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:i+look_back])\n",
    "        y.append(data[i+look_back])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "look_back = 24*7  # Number of hours to look back\n",
    "X_train_seq, y_train_seq = create_sequences(y_train, look_back)\n",
    "X_val_seq, y_val_seq = create_sequences(y_val_with_last_168, look_back)\n",
    "X_test_seq, y_test_seq = create_sequences(y_test_with_last_168, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HqUd0hzNf08V"
   },
   "outputs": [],
   "source": [
    "# Create sequences of data\n",
    "def create_sequences(data, look_back):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - look_back):\n",
    "        X.append(data[i:i+look_back])\n",
    "        y.append(data[i+look_back])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "look_back = 24*7  # Number of hours to look back\n",
    "X_train_seq_port, y_train_seq_port = create_sequences(y_train_port, look_back)\n",
    "X_val_seq_port, y_val_seq_port = create_sequences(y_val_with_last_168_port, look_back)\n",
    "X_test_seq_port, y_test_seq_port = create_sequences(y_test_with_last_168_port, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D3dKHVrUi8E4"
   },
   "outputs": [],
   "source": [
    "x_seq_shape_train = X_train_normalized[168:]\n",
    "x_seq_shape_train_port = X_train_normalized_port[168:]\n",
    "#x_seq_shape_train_multi = X_train_normalized_multi[168:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MrrGM0HaeAM2"
   },
   "outputs": [],
   "source": [
    "# Combine sequences of energy consumption with additional features\n",
    "X_train_combined = np.concatenate((X_train_seq, x_seq_shape_train), axis=1)\n",
    "X_val_combined = np.concatenate((X_val_seq, X_val_normalized), axis=1)\n",
    "X_test_combined = np.concatenate((X_test_seq, X_test_normalized), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nyBvcLaStios"
   },
   "outputs": [],
   "source": [
    "# Combine sequences of ports with additional features\n",
    "X_train_combined_port = np.concatenate((X_train_seq_port, x_seq_shape_train_port), axis=1)\n",
    "X_val_combined_port = np.concatenate((X_val_seq_port, X_val_normalized_port), axis=1)\n",
    "X_test_combined_port = np.concatenate((X_test_seq_port, X_test_normalized_port), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kkW3PWTXlZAz"
   },
   "source": [
    "We also should check for the availability of a Graphics Processing Unit (GPU) and configures the computing device accordingly. In machine learning and deep learning tasks, GPUs can significantly accelerate the training and inference processes due to their parallel processing capabilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FYgOBOlri0gy"
   },
   "outputs": [],
   "source": [
    "# Check if a GPU is available, and if so, uset it; otherwise, use the CPU\n",
    "device= torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nJ_C5oVUlulP"
   },
   "source": [
    "The conditional statement assigns the device to \"cuda\" if a GPU is available (i.e., torch.cuda.is_available() is True). Otherwise, it assigns the device to \"cpu\". This ensures that the code can adapt to the available hardware resources, utilizing the GPU when possible to accelerate computations and falling back to the CPU when a GPU is not available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SDmpfVjQlyxe"
   },
   "source": [
    "The final data preparation step before diving into deep learning involves converting our data into PyTorch tensors. This conversion is essential because deep learning frameworks like PyTorch are optimized to work with tensors, and they provide efficient GPU acceleration for tensor operations. By converting our data into tensors, we ensure compatibility with the PyTorch framework. Additionally, we aim to allocate these tensors to the available computing device, preferably a GPU. By transferring our data to the GPU when possible, we harness its computational power to expedite model training and enhance overall efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TdkTmoVlwIS0"
   },
   "outputs": [],
   "source": [
    "# Ensure input data has the correct shape\n",
    "X_train_seq = torch.tensor(X_train_combined, dtype=torch.float32).unsqueeze(-1)  # (batch_size, seq_length, 1)\n",
    "X_val_seq = torch.tensor(X_val_combined, dtype=torch.float32).unsqueeze(-1)      # (batch_size, seq_length, 1)\n",
    "X_test_seq = torch.tensor(X_test_combined, dtype=torch.float32).unsqueeze(-1)    # (batch_size, seq_length, 1)\n",
    "y_train_seq = torch.tensor(y_train_seq, dtype=torch.float32).unsqueeze(1) # Reshape y tensors to match model output shape\n",
    "y_val_seq = torch.tensor(y_val_seq, dtype=torch.float32).unsqueeze(1) # Reshape y tensors to match model output shape\n",
    "y_test_seq = torch.tensor(y_test_seq, dtype=torch.float32).unsqueeze(1) # Reshape y tensors to match model output shape\n",
    "\n",
    "print(f'X_train_seq shape: {X_train_seq.shape}')  # (num_samples, seq_length, 1)\n",
    "print(f'X_val_seq shape: {X_val_seq.shape}')      # (num_samples, seq_length, 1)\n",
    "print(f'X_test_seq shape: {X_test_seq.shape}')    # (num_samples, seq_length, 1)\n",
    "print(f'y_train_seq shape: {y_train_seq.shape}')  # (num_samples,) # Shape: (num_samples, 1)\n",
    "print(f'y_val_seq shape: {y_val_seq.shape}')      # (num_samples,) # Shape: (num_samples, 1)\n",
    "print(f'y_test_seq shape: {y_test_seq.shape}')    # (num_samples,) # Shape: (num_samples, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1MjOsChcwLr-"
   },
   "outputs": [],
   "source": [
    "# Ensure input data has the correct shape\n",
    "X_train_seq_port = torch.tensor(X_train_combined_port, dtype=torch.float32).unsqueeze(-1)  # (batch_size, seq_length, 1)\n",
    "X_val_seq_port = torch.tensor(X_val_combined_port, dtype=torch.float32).unsqueeze(-1)      # (batch_size, seq_length, 1)\n",
    "X_test_seq_port = torch.tensor(X_test_combined_port, dtype=torch.float32).unsqueeze(-1)    # (batch_size, seq_length, 1)\n",
    "y_train_seq_port = torch.tensor(y_train_seq_port, dtype=torch.float32)\n",
    "y_val_seq_port = torch.tensor(y_val_seq_port, dtype=torch.float32)\n",
    "y_test_seq_port = torch.tensor(y_test_seq_port, dtype=torch.float32)\n",
    "\n",
    "print(f'X_train_seq_port shape: {X_train_seq_port.shape}')  # (num_samples, seq_length, 1)\n",
    "print(f'X_val_seq_port shape: {X_val_seq_port.shape}')      # (num_samples, seq_length, 1)\n",
    "print(f'X_test_seq_port shape: {X_test_seq_port.shape}')    # (num_samples, seq_length, 1)\n",
    "print(f'y_train_seq_port shape: {y_train_seq_port.shape}')  # (num_samples,)\n",
    "print(f'y_val_seq_port shape: {y_val_seq_port.shape}')      # (num_samples,)\n",
    "print(f'y_test_seq_port shape: {y_test_seq_port.shape}')    # (num_samples,)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nxOOFu7PABqn"
   },
   "source": [
    "These shapes represent the dimensions of our data tensors, and they indicate the number of samples, sequence length (168 in this case, corresponding to a week's worth of data), and the number of features (1 in this scenario, as we are dealing with univariate time series data). These dimensions are crucial for configuring our deep learning models correctly and ensuring that they can effectively learn and make predictions based on the provided data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2p51MOWZT0tt"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, accuracy_score\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O1g1WGinj6Vw"
   },
   "outputs": [],
   "source": [
    "# Set random seeds for reproducibility\n",
    "def set_seed(seed):\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "\n",
    "set_seed(42)  # Set the seed for reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8t0eR3n6AB6g"
   },
   "source": [
    "### **5.4.2. LSTM Regression Model (Forcasting Energy Consumption)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OynTfDQi1abC"
   },
   "outputs": [],
   "source": [
    "# Define the LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Take the last time step's output\n",
    "        return out\n",
    "\n",
    "# Define hyperparameters and device\n",
    "batch_size = 32\n",
    "input_size = 1  # Single feature, reshape to (batch_size, sequence_length, 1)\n",
    "num_epochs = 50\n",
    "patience = 10  # Early stopping patience\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_seq)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_seq, y_val_seq)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Optuna study for LSTM hyperparameter tuning\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "\n",
    "# Objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Define hyperparameters for tuning\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 64)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 3)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "\n",
    "    # Initialize model with the trial's hyperparameters\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, dropout_prob).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loop with DataLoader\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_targets).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Early stopping mechanism\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save best model state\n",
    "            torch.save(model.state_dict(), 'best_lstm_model.pth')\n",
    "\n",
    "            # Save best hyperparameters\n",
    "            best_lstm_params = {\n",
    "                'hidden_size': hidden_size,\n",
    "                'num_layers': num_layers,\n",
    "                'dropout_prob': dropout_prob,\n",
    "                'learning_rate': learning_rate\n",
    "            }\n",
    "            with open('best_lstm_params.json', 'w') as f:\n",
    "                json.dump(best_lstm_params, f)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# Run Optuna optimization\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Retrieve and print the best hyperparameters and validation loss\n",
    "best_lstm_params = study.best_params\n",
    "best_lstm_val_loss = study.best_value\n",
    "print(f\"Best Hyperparameters: {best_lstm_params}\")\n",
    "print(f\"Best Validation Loss: {best_lstm_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uL2WJjqgD09Q"
   },
   "source": [
    "Best Hyperparameters: {'hidden_size': 30, 'num_layers': 3, 'dropout_prob': 0.2727780074568463, 'learning_rate': 0.002919379110578439}\n",
    "Best Validation Loss: 0.2765738972475831"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o37_eQVoeSLf"
   },
   "outputs": [],
   "source": [
    "# Define the LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Take the last time step's output\n",
    "        return out\n",
    "\n",
    "# Define hyperparameters and device\n",
    "batch_size = 32\n",
    "input_size = 1  # Single feature, reshape to (batch_size, sequence_length, 1)\n",
    "num_epochs = 50\n",
    "patience = 10  # Early stopping patience\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create data loaders for training and validation\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_seq)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_seq, y_val_seq)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eZuMIV-GEq7d"
   },
   "outputs": [],
   "source": [
    "# LSTM model class\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_prob):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Use the last time step's output\n",
    "        return out\n",
    "\n",
    "# Load the best hyperparameters from JSON\n",
    "#with open('best_lstm_params.json', 'r') as f:\n",
    "#    best_lstm_params = json.load(f)\n",
    "best_lstm_params = {\n",
    "    'hidden_size': 30,\n",
    "    'num_layers': 3,\n",
    "    'dropout_prob': 0.2727780074568463,\n",
    "    'learning_rate': 0.002919379110578439,\n",
    "}\n",
    "\n",
    "# Create the best LSTM model using loaded hyperparameters\n",
    "input_size = 1  # as per your data shape (batch_size, seq_length, 1)\n",
    "best_lstm_model = LSTMModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size=best_lstm_params['hidden_size'],\n",
    "    num_layers=best_lstm_params['num_layers'],\n",
    "    dropout_prob=best_lstm_params['dropout_prob']\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hOgcGd9nEtvs"
   },
   "outputs": [],
   "source": [
    "# Combine training and validation data\n",
    "X_combined = torch.cat((X_train_seq, X_val_seq), dim=0)\n",
    "y_combined = torch.cat((y_train_seq, y_val_seq), dim=0)\n",
    "\n",
    "# Create combined dataset and dataloader\n",
    "combined_dataset = TensorDataset(X_combined, y_combined)\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define loss function and optimizer with the best learning rate\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(best_lstm_model.parameters(), lr=best_lstm_params['learning_rate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Io-VCRfuEw4d"
   },
   "outputs": [],
   "source": [
    "# Final training on combined data\n",
    "num_epochs = 200\n",
    "for epoch in range(num_epochs):\n",
    "    best_lstm_model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(combined_loader), total=len(combined_loader), desc=f'Epoch {epoch+1}/{num_epochs}', ncols=100)\n",
    "\n",
    "    for i, (inputs, targets) in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_lstm_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=running_loss/(i+1))\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(combined_loader):.4f}')\n",
    "\n",
    "# Save the final model's state dictionary\n",
    "torch.save(best_lstm_model.state_dict(), 'final_lstm_model.pth')\n",
    "print(\"Final model training complete and saved as 'final_lstm_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6crL2CD5Ih0s"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "O4r1ePuQGkec"
   },
   "outputs": [],
   "source": [
    "save_path = '/content/drive/MyDrive/EV_pred/final_lstm_model.pth'\n",
    "torch.save(best_lstm_model.state_dict(), save_path)\n",
    "print(f\"Final model training complete and saved as '{save_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wEk0zJeSUPYN"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "# Load test dataset and DataLoader\n",
    "test_dataset = TensorDataset(X_test_seq, y_test_seq)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the path where the model is saved on Google Drive\n",
    "model_path = '/content/drive/MyDrive/EV_pred/final_lstm_model.pth'\n",
    "\n",
    "# Load the saved model's state dictionary\n",
    "best_lstm_model.load_state_dict(torch.load(model_path, weights_only=False))\n",
    "best_lstm_model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# Testing phase\n",
    "test_predictions = []\n",
    "test_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "\n",
    "        test_outputs = best_lstm_model(test_inputs)\n",
    "        test_predictions.append(test_outputs.cpu().numpy())\n",
    "        test_true_labels.append(test_labels.cpu().numpy())\n",
    "\n",
    "# Concatenate predictions and true labels for metrics calculation\n",
    "test_predictions = np.concatenate(test_predictions)\n",
    "test_true_labels = np.concatenate(test_true_labels)\n",
    "\n",
    "# Define MAPE function\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "test_mse = mean_squared_error(test_true_labels, test_predictions)\n",
    "test_mae = mean_absolute_error(test_true_labels, test_predictions)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test MSE for LSTM: {test_mse:.4f}\")\n",
    "print(f\"Test MAE for LSTM: {test_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QB1w2DTslfyf"
   },
   "source": [
    "### **5.4.3. LSTM Classification Model (Forcasting Available Ports)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "61yqVoOlctKu"
   },
   "outputs": [],
   "source": [
    "# Create LSTM model class for classification\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_prob, output_size):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = self.fc(out[:, -1, :])  # Use the output of the last time step\n",
    "        return out\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 32\n",
    "input_size = 1  # Update based on actual feature dimension\n",
    "num_epochs = 50\n",
    "patience = 10\n",
    "output_size = 7  # Number of classes\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_seq_port, y_train_seq_port)\n",
    "val_dataset = TensorDataset(X_val_seq_port, y_val_seq_port)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "#study = optuna.create_study(direction='maximize')  # Maximize F1 score\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 64)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 3)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "\n",
    "    # Create the model with suggested hyperparameters\n",
    "    model = LSTMModel(input_size, hidden_size, num_layers, dropout_prob, output_size).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_f1 = float('-inf')  # Initialize best F1 score\n",
    "    patience_counter = 0  # Initialize patience counter\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device).long()  # Move to GPU if available\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_targets_list = []\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_targets = val_targets.to(device).long()  # Move to GPU if available\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_predictions.extend(torch.argmax(val_outputs, dim=1).cpu().numpy())\n",
    "                val_targets_list.extend(val_targets.cpu().numpy())\n",
    "\n",
    "        val_f1 = f1_score(val_targets_list, val_predictions, average='weighted')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "\n",
    "            # Save the best model's state dictionary\n",
    "            torch.save(model.state_dict(), 'best_lstm_model_port.pth')\n",
    "\n",
    "            # Save the best hyperparameters\n",
    "            best_lstm_params_port = {\n",
    "                'hidden_size': hidden_size,\n",
    "                'num_layers': num_layers,\n",
    "                'dropout_prob': dropout_prob,\n",
    "                'learning_rate': learning_rate\n",
    "            }\n",
    "\n",
    "            with open('best_lstm_params_port.json', 'w') as f:\n",
    "                json.dump(best_lstm_params_port, f)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    # Return the best F1 score as the objective to maximize\n",
    "    return best_val_f1\n",
    "\n",
    "# Perform hyperparameter optimization and set the number of trials\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Print the best hyperparameters and best validation F1 score\n",
    "best_lstm_params_port = study.best_params\n",
    "best_lstm_val_f1_port = study.best_value\n",
    "print(f\"Best Hyperparameters: {best_lstm_params_port}\")\n",
    "print(f\"Best Validation F1 Score: {best_lstm_val_f1_port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wk2WlztFMUX2"
   },
   "source": [
    "Best Hyperparameters: {'hidden_size': 30, 'num_layers': 2, 'dropout_prob': 0.3736932106048628, 'learning_rate': 0.0044071234124586165}\n",
    "Best Validation F1 Score: 0.9969152530343797"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vjqW0_cVT8Ot"
   },
   "outputs": [],
   "source": [
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_classes):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True, dropout=0.5)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out[:, -1, :])  # Use the last output\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AAj5QWwqdTa7"
   },
   "outputs": [],
   "source": [
    "# Assuming you have your best hyperparameters from the Optuna study\n",
    "best_params = {\n",
    "    'hidden_size': 30,\n",
    "    'num_layers': 2,\n",
    "    'dropout_prob': 0.3736932106048628,\n",
    "    'learning_rate': 0.0044071234124586165,\n",
    "    'output_size': 7  # Number of classes\n",
    "}\n",
    "\n",
    "# Load the best hyperparameters from JSON\n",
    "#with open('best_lstm_params_port.json', 'r') as f:\n",
    "#    best_params = json.load(f)\n",
    "\n",
    "\n",
    "# Create the model with the best hyperparameters\n",
    "model = LSTMModel(\n",
    "    input_size=input_size,  # Adjusted for the number of input features\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    num_layers=best_params['num_layers'],\n",
    "    dropout_prob=best_params['dropout_prob'],\n",
    "    output_size=best_params['output_size']  # Number of classes\n",
    ").to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "# Combine training and validation datasets\n",
    "combined_X_train_val = torch.cat((X_train_seq_port, X_val_seq_port), dim=0)\n",
    "combined_y_train_val = torch.cat((y_train_seq_port, y_val_seq_port), dim=0)\n",
    "\n",
    "# Create DataLoader for the combined dataset\n",
    "train_dataset_combined = TensorDataset(combined_X_train_val, combined_y_train_val)\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=32, shuffle=False)\n",
    "\n",
    "# Train the model on the combined dataset\n",
    "num_epochs = 50  # Define your number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0  # Initialize running loss\n",
    "    progress_bar = tqdm(enumerate(train_loader_combined), total=len(train_loader_combined), desc=f'Epoch {epoch + 1}/{num_epochs}', ncols=100)\n",
    "\n",
    "    for i, (inputs, targets) in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device).long()  # Ensure targets are long\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()  # Update running loss\n",
    "        progress_bar.set_postfix(loss=running_loss / (i + 1))  # Update progress bar with the average loss\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader_combined):.4f}')\n",
    "\n",
    "# Save the final model's state dictionary\n",
    "torch.save(model.state_dict(), 'final_lstm_classification_model.pth')\n",
    "print(\"Training complete and model saved as 'final_lstm_classification_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vN0X8XmiHnxM"
   },
   "outputs": [],
   "source": [
    "save_path = '/content/drive/MyDrive/EV_pred/final_lstm_classification_model.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Final model training complete and saved as '{save_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fZofbQHtd5nG"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "# Define the test dataset\n",
    "test_dataset = TensorDataset(X_test_seq_port, y_test_seq_port)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the path where the model is saved on Google Drive\n",
    "model_path = '/content/drive/MyDrive/EV_pred/final_lstm_classification_model.pth'\n",
    "\n",
    "# Load the saved model's state dictionary\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_predictions_classification = []\n",
    "test_true_labels_classification = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "\n",
    "        # Ensure test_inputs have the correct dimensions\n",
    "        if test_inputs.dim() == 2:  # If it's (batch_size, sequence_length)\n",
    "            test_inputs = test_inputs.unsqueeze(-1)  # Add input_size dimension if necessary\n",
    "\n",
    "        test_outputs = model(test_inputs)\n",
    "\n",
    "        test_predictions_classification.append(torch.argmax(test_outputs, dim=1).cpu().numpy())\n",
    "        test_true_labels_classification.append(test_labels.cpu().numpy())\n",
    "\n",
    "# Concatenate predictions and true labels\n",
    "test_predictions_classification = np.concatenate(test_predictions_classification)\n",
    "test_true_labels_classification = np.concatenate(test_true_labels_classification)\n",
    "\n",
    "# Calculate metrics\n",
    "test_f1 = f1_score(test_true_labels_classification, test_predictions_classification, average='weighted')\n",
    "test_accuracy = accuracy_score(test_true_labels_classification, test_predictions_classification)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test F1 Score for LSTM: {test_f1:.4f}\")\n",
    "print(f\"Test Accuracy for LSTM: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8MzLAFJFK-bx"
   },
   "source": [
    "### **5.4.4. GRU Regression Model (Forcasting Energy Consumption)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HxxEWdb_N1WM"
   },
   "outputs": [],
   "source": [
    "# Define the GRU model class\n",
    "class GRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_prob):\n",
    "        super(GRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Hyperparameters and data loaders\n",
    "batch_size = 32\n",
    "input_size = 1\n",
    "num_epochs = 50\n",
    "patience = 10\n",
    "\n",
    "train_dataset = TensorDataset(X_train_seq, y_train_seq)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_seq, y_val_seq)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the Optuna study\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "\n",
    "# Define the objective function\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 64)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 3)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "\n",
    "    # Create model, loss, and optimizer\n",
    "    model = GRUModel(input_size, hidden_size, num_layers, dropout_prob).to(device)\n",
    "    criterion = nn.MSELoss().to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loop with DataLoader\n",
    "        model.eval()\n",
    "        val_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_inputs, val_targets = val_inputs.to(device), val_targets.to(device)\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_loss += criterion(val_outputs, val_targets).item()\n",
    "        val_loss /= len(val_loader)\n",
    "\n",
    "        # Early stopping mechanism\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "            # Save the best model\n",
    "            torch.save(model.state_dict(), 'best_gru_model.pth')\n",
    "\n",
    "            # Save best hyperparameters\n",
    "            best_gru_params = {\n",
    "                'hidden_size': hidden_size,\n",
    "                'num_layers': num_layers,\n",
    "                'dropout_prob': dropout_prob,\n",
    "                'learning_rate': learning_rate\n",
    "            }\n",
    "            with open('best_gru_params.json', 'w') as f:\n",
    "                json.dump(best_gru_params, f)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    return best_val_loss\n",
    "\n",
    "# Perform hyperparameter optimization\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Display best results\n",
    "best_gru_params = study.best_params\n",
    "best_gru_val_loss = study.best_value\n",
    "print(f\"Best Hyperparameters: {best_gru_params}\")\n",
    "print(f\"Best Validation Loss: {best_gru_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VCoFLAN09OFn"
   },
   "source": [
    "Best Hyperparameters: {'hidden_size': 50, 'num_layers': 3, 'dropout_prob': 0.3627137988853174, 'learning_rate': 0.0010389678366861522}\n",
    "Best Validation Loss: 0.27841983961177547"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HYjcWXcBQ0_b"
   },
   "outputs": [],
   "source": [
    "# Load the best hyperparameters from JSON\n",
    "#with open('best_gru_params.json', 'r') as f:\n",
    "#    best_gru_params = json.load(f)\n",
    "best_gru_params={\n",
    "    'hidden_size': 50,\n",
    "    'num_layers': 3,\n",
    "    'dropout_prob': 0.3627137988853174,\n",
    "    'learning_rate': 0.0010389678366861522,\n",
    "}\n",
    "# Create the best GRU model using loaded hyperparameters\n",
    "input_size = 1  # as per your data shape (batch_size, seq_length, 1)\n",
    "best_gru_model = GRUModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size=best_gru_params['hidden_size'],\n",
    "    num_layers=best_gru_params['num_layers'],\n",
    "    dropout_prob=best_gru_params['dropout_prob']\n",
    ").to(device)\n",
    "\n",
    "# Combine training and validation data\n",
    "X_combined = torch.cat((X_train_seq, X_val_seq), dim=0)\n",
    "y_combined = torch.cat((y_train_seq, y_val_seq), dim=0)\n",
    "\n",
    "# Create combined dataset and dataloader\n",
    "combined_dataset = TensorDataset(X_combined, y_combined)\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define loss function and optimizer with the best learning rate\n",
    "criterion = nn.MSELoss().to(device)\n",
    "optimizer = optim.Adam(best_gru_model.parameters(), lr=best_gru_params['learning_rate'])\n",
    "\n",
    "# Final training on combined data\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    best_gru_model.train()\n",
    "    running_loss = 0.0\n",
    "    progress_bar = tqdm(enumerate(combined_loader), total=len(combined_loader), desc=f'Epoch {epoch+1}/{num_epochs}', ncols=100)\n",
    "\n",
    "    for i, (inputs, targets) in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = best_gru_model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        progress_bar.set_postfix(loss=running_loss/(i+1))\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(combined_loader):.4f}')\n",
    "\n",
    "# Save the final model's state dictionary\n",
    "torch.save(best_gru_model.state_dict(), 'final_gru_model.pth')\n",
    "print(\"Final model training complete and saved as 'final_gru_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdrA6xypzgDX"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zvtgHbnozgDY"
   },
   "outputs": [],
   "source": [
    "save_path = '/content/drive/MyDrive/EV_pred/final_gru_model.pth'\n",
    "torch.save(best_gru_model.state_dict(), save_path)\n",
    "print(f\"Final model training complete and saved as '{save_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oQR-0ErrRFGi"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "# Load test dataset and DataLoader\n",
    "test_dataset = TensorDataset(X_test_seq, y_test_seq)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Define the path where the model is saved on Google Drive\n",
    "model_path = '/content/drive/MyDrive/EV_pred/final_gru_model.pth'\n",
    "\n",
    "# Reload the best model (optional step if you want to confirm loading)\n",
    "best_gru_model.load_state_dict(torch.load(model_path))\n",
    "best_gru_model.eval()\n",
    "\n",
    "# Testing phase\n",
    "test_predictions = []\n",
    "test_true_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "\n",
    "        test_outputs = best_gru_model(test_inputs)\n",
    "        test_predictions.append(test_outputs.cpu().numpy())\n",
    "        test_true_labels.append(test_labels.cpu().numpy())\n",
    "\n",
    "# Concatenate predictions and true labels for metrics calculation\n",
    "test_predictions = np.concatenate(test_predictions)\n",
    "test_true_labels = np.concatenate(test_true_labels)\n",
    "\n",
    "# Define MAPE function\n",
    "def mean_absolute_percentage_error(y_true, y_pred):\n",
    "    return np.mean(np.abs((y_true - y_pred) / y_true)) * 100\n",
    "\n",
    "# Calculate evaluation metrics\n",
    "test_mse = mean_squared_error(test_true_labels, test_predictions)\n",
    "test_mae = mean_absolute_error(test_true_labels, test_predictions)\n",
    "test_mape = mean_absolute_percentage_error(test_true_labels, test_predictions)\n",
    "\n",
    "# Print results\n",
    "print(f\"Test MSE for GRU: {test_mse:.4f}\")\n",
    "print(f\"Test MAE for GRU: {test_mae:.4f}\")\n",
    "print(f\"Test MAPE for GRU: {test_mape:.4f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEo2lD2tsdLQ"
   },
   "source": [
    "### **5.4.5. GRU Classification Model (Forcasting Available Ports)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93kmXGCmA9b5"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from sklearn.metrics import f1_score\n",
    "import optuna\n",
    "import json\n",
    "\n",
    "# Create GRU model class for classification\n",
    "class GRUClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_prob, output_size):\n",
    "        super(GRUClassifier, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = self.fc(out[:, -1, :])  # Use the output of the last time step\n",
    "        return out\n",
    "\n",
    "# Define hyperparameters\n",
    "batch_size = 32\n",
    "input_size = 1  # Update based on actual feature dimension\n",
    "num_epochs = 50\n",
    "patience = 10\n",
    "output_size = 7  # Number of classes\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = TensorDataset(X_train_seq_port, y_train_seq_port)\n",
    "val_dataset = TensorDataset(X_val_seq_port, y_val_seq_port)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Create a study object and optimize the objective function\n",
    "study = optuna.create_study(direction='maximize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_size = trial.suggest_int('hidden_size', 16, 64)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 3)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "\n",
    "    # Create the model with suggested hyperparameters\n",
    "    model = GRUClassifier(input_size, hidden_size, num_layers, dropout_prob, output_size).to(device)\n",
    "\n",
    "    # Define loss function and optimizer\n",
    "    criterion = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_f1 = float('-inf')  # Initialize best F1 score\n",
    "    patience_counter = 0  # Initialize patience counter\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets in train_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device).long()  # Move to GPU if available\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate on the validation set\n",
    "        model.eval()\n",
    "        val_predictions = []\n",
    "        val_targets_list = []\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets in val_loader:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_targets = val_targets.to(device).long()  # Move to GPU if available\n",
    "                val_outputs = model(val_inputs)\n",
    "                val_predictions.extend(torch.argmax(val_outputs, dim=1).cpu().numpy())\n",
    "                val_targets_list.extend(val_targets.cpu().numpy())\n",
    "\n",
    "        val_f1 = f1_score(val_targets_list, val_predictions, average='weighted')\n",
    "\n",
    "        # Check for early stopping\n",
    "        if val_f1 > best_val_f1:\n",
    "            best_val_f1 = val_f1\n",
    "            patience_counter = 0\n",
    "\n",
    "            # Save the best model's state dictionary\n",
    "            torch.save(model.state_dict(), 'best_gru_model_port.pth')\n",
    "\n",
    "            # Save the best hyperparameters\n",
    "            best_gru_params_port = {\n",
    "                'hidden_size': hidden_size,\n",
    "                'num_layers': num_layers,\n",
    "                'dropout_prob': dropout_prob,\n",
    "                'learning_rate': learning_rate\n",
    "            }\n",
    "\n",
    "            with open('best_gru_params_port.json', 'w') as f:\n",
    "                json.dump(best_gru_params_port, f)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    # Return the best F1 score as the objective to maximize\n",
    "    return best_val_f1\n",
    "\n",
    "# Perform hyperparameter optimization and set the number of trials\n",
    "study.optimize(objective, n_trials=10)\n",
    "\n",
    "# Print the best hyperparameters and best validation F1 score\n",
    "best_gru_params_port = study.best_params\n",
    "best_gru_val_f1_port = study.best_value\n",
    "print(f\"Best Hyperparameters: {best_gru_params_port}\")\n",
    "print(f\"Best Validation F1 Score: {best_gru_val_f1_port}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p17Y10UL_bi-"
   },
   "source": [
    "Best Hyperparameters: {'hidden_size': 56, 'num_layers': 2, 'dropout_prob': 0.17272998688284025, 'learning_rate': 0.001842211053435804}\n",
    "Best Validation F1 Score: 0.995576326486205"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qM8SYjadqi-O"
   },
   "outputs": [],
   "source": [
    "# Load the best hyperparameters (assuming they were saved earlier from an Optuna study)\n",
    "#with open('best_gru_params_port.json', 'r') as f:\n",
    "#    best_params = json.load(f)\n",
    "\n",
    "# Assuming you have your best hyperparameters from the Optuna study\n",
    "best_params = {\n",
    "    'hidden_size': 56,\n",
    "    'num_layers': 2,\n",
    "    'dropout_prob': 0.17272998688284025,\n",
    "    'learning_rate': 0.001842211053435804,\n",
    "    'output_size': 7  # Number of classes\n",
    "}\n",
    "\n",
    "# Create the model with the best hyperparameters\n",
    "model = GRUClassifier(\n",
    "    num_layers= best_params['num_layers'],\n",
    "    dropout_prob= best_params['dropout_prob'],\n",
    "    input_size=input_size,  # Adjust based on input feature dimensions\n",
    "    hidden_size=best_params['hidden_size'],\n",
    "    output_size=best_params['output_size']  # Number of classes\n",
    ").to(device)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "# Combine training and validation datasets\n",
    "combined_X_train_val = torch.cat((X_train_seq_port, X_val_seq_port), dim=0)\n",
    "combined_y_train_val = torch.cat((y_train_seq_port, y_val_seq_port), dim=0)\n",
    "\n",
    "# Create DataLoader for the combined dataset\n",
    "train_dataset_combined = TensorDataset(combined_X_train_val, combined_y_train_val)\n",
    "train_loader_combined = DataLoader(train_dataset_combined, batch_size=32, shuffle=False)\n",
    "\n",
    "# Train the model on the combined dataset\n",
    "num_epochs = 50  # Define your number of epochs\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0  # Initialize running loss\n",
    "    progress_bar = tqdm(enumerate(train_loader_combined), total=len(train_loader_combined), desc=f'Epoch {epoch + 1}/{num_epochs}', ncols=100)\n",
    "\n",
    "    for i, (inputs, targets) in progress_bar:\n",
    "        inputs, targets = inputs.to(device), targets.to(device).long()  # Ensure targets are long\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()  # Update running loss\n",
    "        progress_bar.set_postfix(loss=running_loss / (i + 1))  # Update progress bar with the average loss\n",
    "\n",
    "    print(f'Epoch {epoch + 1}, Loss: {running_loss / len(train_loader_combined):.4f}')\n",
    "\n",
    "# Save the final model's state dictionary\n",
    "torch.save(model.state_dict(), 'final_gru_classification_model.pth')\n",
    "print(\"Training complete and model saved as 'final_gru_classification_model.pth'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RgIjLa-srCV8"
   },
   "outputs": [],
   "source": [
    "save_path = '/content/drive/MyDrive/EV_pred/final_gru_classification_model.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Final model training complete and saved as '{save_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VKxYjG61p4rj"
   },
   "outputs": [],
   "source": [
    "# Testing\n",
    "# Define the test dataset\n",
    "test_dataset = TensorDataset(X_test_seq_port, y_test_seq_port)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Define the path where the model is saved on Google Drive\n",
    "model_path = '/content/drive/MyDrive/EV_pred/final_gru_classification_model.pth'\n",
    "\n",
    "# Load the saved model's state dictionary\n",
    "model.load_state_dict(torch.load(model_path, weights_only=True))\n",
    "\n",
    "# Evaluate the model on the test data\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "test_predictions_classification = []\n",
    "test_true_labels_classification = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels in test_loader:\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_labels = test_labels.to(device)\n",
    "\n",
    "        # Ensure test_inputs have the correct dimensions\n",
    "        if test_inputs.dim() == 2:  # If it's (batch_size, sequence_length)\n",
    "            test_inputs = test_inputs.unsqueeze(-1)  # Add input_size dimension if necessary\n",
    "\n",
    "        test_outputs = model(test_inputs)\n",
    "\n",
    "        test_predictions_classification.append(torch.argmax(test_outputs, dim=1).cpu().numpy())\n",
    "        test_true_labels_classification.append(test_labels.cpu().numpy())\n",
    "\n",
    "# Concatenate predictions and true labels\n",
    "test_predictions_classification = np.concatenate(test_predictions_classification)\n",
    "test_true_labels_classification = np.concatenate(test_true_labels_classification)\n",
    "\n",
    "# Calculate metrics\n",
    "test_f1 = f1_score(test_true_labels_classification, test_predictions_classification, average='weighted')\n",
    "test_accuracy = accuracy_score(test_true_labels_classification, test_predictions_classification)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test F1 Score for GRU: {test_f1:.4f}\")\n",
    "print(f\"Test Accuracy for GRU: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1oqc37NAl3ii"
   },
   "source": [
    "### **5.4.6. Informer Regression Model (Forcasting Energy Consumption)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7upGWZh3mBj7"
   },
   "outputs": [],
   "source": [
    "!pip install pytorch_forecasting pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bIpIi5mlmA4Z"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pytorch_forecasting import TemporalFusionTransformer, TimeSeriesDataSet\n",
    "from pytorch_forecasting.metrics import MAE\n",
    "from pytorch_lightning import Trainer\n",
    "from pytorch_lightning.callbacks import EarlyStopping\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from pytorch_lightning import seed_everything\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "piIKj3Nqmhof"
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"/content/drive/MyDrive/final.csv\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JJh61YpMmjgw"
   },
   "outputs": [],
   "source": [
    "df= df[['Date', 'Energy Consumption', 'IsHoliday',\n",
    "       'IsWeekend', 'Year', 'Month', 'day_of_month', 'day_of_week', 'Hour',\n",
    "       'ema_3_hour', 'ema_3_hour_std', 'ema_1_day_std', 'ema_1_day',\n",
    "       'ema_7_days_std', 'ema_7_days', 'ema_30_days', 'ema_30_days_std']]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G-RoCX9Xl6An"
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed_everything(42)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wSCOOjbenkut"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Assuming 'df' contains all columns (features + target)\n",
    "# 'X' should contain all features, and 'y' should contain the target ('Energy Consumption')\n",
    "X = df.drop(columns=['Energy Consumption'])  # Features\n",
    "y = df['Energy Consumption']  # Target\n",
    "\n",
    "# Split data into train, validation, and test (keeping the temporal order)\n",
    "train_size = int(0.7 * len(df))\n",
    "val_size = int(0.2 * len(df))\n",
    "test_size = len(df) - train_size - val_size\n",
    "\n",
    "train_X, val_X, test_X = X.iloc[:train_size], X.iloc[train_size:train_size+val_size], X.iloc[train_size+val_size:]\n",
    "train_y, val_y, test_y = y.iloc[:train_size], y.iloc[train_size:train_size+val_size], y.iloc[train_size+val_size:]\n",
    "\n",
    "# Normalize the features\n",
    "scaler = MinMaxScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "val_X_scaled = scaler.transform(val_X)\n",
    "test_X_scaled = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OMSVND1Kmum7"
   },
   "outputs": [],
   "source": [
    "# Custom Dataset Class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data, input_window, output_window):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        self.input_window = input_window\n",
    "        self.output_window = output_window\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data) - self.input_window - self.output_window\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Use array slicing instead of .iloc\n",
    "        X = self.X_data[idx:idx + self.input_window]\n",
    "        y = self.y_data[idx + self.input_window:idx + self.input_window + self.output_window]\n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)\n",
    "\n",
    "# Informer Model Adjustments\n",
    "class Informer(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, num_layers, n_heads, dropout):\n",
    "        super(Informer, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.output_size = output_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Encoder Layer with batch_first=True\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_size, nhead=n_heads, dropout=dropout, batch_first=True),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Linear layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Pass input through the first layer\n",
    "        x = self.fc1(x)\n",
    "\n",
    "        # Encoder Layer\n",
    "        x = self.encoder(x)\n",
    "\n",
    "        # Output layer\n",
    "        x = self.fc_out(x)\n",
    "\n",
    "        # We only want the last output in the sequence (for forecasting)\n",
    "        return x[:, -1, :]  # Assuming the model predicts the last time step\n",
    "\n",
    "# Create dataset objects for training, validation, and testing\n",
    "n_input = 24*7  # Number of past hours to consider\n",
    "n_output = 1  # Forecast the next 24 hours\n",
    "\n",
    "train_dataset = TimeSeriesDataset(train_X_scaled, train_y, n_input, n_output)\n",
    "val_dataset = TimeSeriesDataset(val_X_scaled, val_y, n_input, n_output)\n",
    "test_dataset = TimeSeriesDataset(test_X_scaled, test_y, n_input, n_output)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LgtNLLA9nvTO"
   },
   "outputs": [],
   "source": [
    "import optuna\n",
    "from optuna.samplers import TPESampler\n",
    "\n",
    "# Objective function for Optuna optimization\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    n_heads = trial.suggest_int('n_heads', 2, 6, step=2)  # Only even values\n",
    "    hidden_size = trial.suggest_int('hidden_size', n_heads * 4, n_heads * 16, step=n_heads * 4)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 3)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "\n",
    "    num_epochs = 50  # Maximum number of epochs to train\n",
    "    patience = 10    # Early stopping patience\n",
    "\n",
    "    # Initialize the model\n",
    "    model = Informer(input_size=train_X_scaled.shape[1],  # Number of features\n",
    "                     output_size=1, hidden_size=hidden_size,\n",
    "                     num_layers=num_layers, n_heads=n_heads, dropout=dropout).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Training loop with early stopping on validation loss\n",
    "    model.train()\n",
    "    val_loss = 0.0\n",
    "    best_val_loss = float('inf')\n",
    "    no_improvement_count = 0  # Tracks epochs without improvement\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Average validation loss for this epoch\n",
    "        epoch_val_loss = epoch_loss / len(val_loader)\n",
    "\n",
    "        # Early stopping logic\n",
    "        if epoch_val_loss < best_val_loss:\n",
    "            best_val_loss = epoch_val_loss\n",
    "            no_improvement_count = 0  # Reset count if there's improvement\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        if no_improvement_count >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    # Return best validation loss as the objective\n",
    "    return best_val_loss\n",
    "\n",
    "# Run Optuna optimization with TPESampler\n",
    "study = optuna.create_study(direction='minimize', sampler=TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "# Get the best hyperparameters\n",
    "best_params = study.best_params\n",
    "print(\"Best hyperparameters: \", best_params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9eRdoUNWABOb"
   },
   "source": [
    "Best hyperparameters:  {'n_heads': 6, 'hidden_size': 96, 'learning_rate': 0.00039156358814491297, 'num_layers': 3, 'dropout': 0.10257406393714368}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1DCU2H2RrLYR"
   },
   "outputs": [],
   "source": [
    "# Combine train and validation datasets\n",
    "train_val_X = np.concatenate([train_X_scaled, val_X_scaled], axis=0)\n",
    "train_val_y = np.concatenate([train_y, val_y], axis=0)\n",
    "\n",
    "# Create combined dataset for training\n",
    "train_val_dataset = TimeSeriesDataset(train_val_X, train_val_y, n_input, n_output)\n",
    "train_val_loader = DataLoader(train_val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Initialize the final model with the best hyperparameters\n",
    "model = Informer(input_size=train_X_scaled.shape[1],  # Number of features\n",
    "                 output_size=1, hidden_size=best_params['hidden_size'],\n",
    "                 num_layers=best_params['num_layers'], n_heads=best_params['n_heads'], dropout=best_params['dropout']).to(device)\n",
    "\n",
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "# Final training loop with the best number of epochs\n",
    "num_epochs = 100  # Use the best number of epochs found by Optuna\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_val_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch + 1}/{num_epochs}], Loss: {train_loss / len(train_val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AkWHNnhst6Xg"
   },
   "outputs": [],
   "source": [
    "save_path = '/content/drive/MyDrive/EV_pred/final_informer_regression_model.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Final model training complete and saved as '{save_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pfum0aaht49M"
   },
   "outputs": [],
   "source": [
    "model_path = '/content/drive/MyDrive/EV_pred/final_informer_regression_model.pth'\n",
    "#model.load_state_dict(torch.load(model_path))\n",
    "model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzuyHPihrVKj"
   },
   "outputs": [],
   "source": [
    "# Evaluate final model on test set\n",
    "model.eval()\n",
    "test_loss = 0.0\n",
    "predictions = []\n",
    "true_values = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        test_loss += loss.item()\n",
    "        predictions.append(y_pred.cpu().numpy())\n",
    "        true_values.append(y_batch.cpu().numpy())\n",
    "\n",
    "# Convert predictions and true values to 1D arrays\n",
    "predictions = np.concatenate(predictions, axis=0)\n",
    "true_values = np.concatenate(true_values, axis=0)\n",
    "\n",
    "# Calculate MSE and MAE\n",
    "mse = mean_squared_error(true_values, predictions)\n",
    "mae = mean_absolute_error(true_values, predictions)\n",
    "\n",
    "print(f\"Test MSE: {mse}\")\n",
    "print(f\"Test MAE: {mae}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n6sIJA7CEFKu"
   },
   "outputs": [],
   "source": [
    "# Plot true vs predicted values\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(true_values, label='True Energy Consumption', color='blue')\n",
    "plt.plot(predictions, label='Predicted Energy Consumption', color='orange')\n",
    "plt.legend()\n",
    "plt.title('True vs Predicted Energy Consumption')\n",
    "plt.xlabel('Time Step')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ecQ0Gz0Vq7TS"
   },
   "source": [
    "### **5.4.7. Informer Classification Model (Forcasting Available Ports)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LoWX-fhntA8w"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import optuna\n",
    "from optuna.samplers import TPESampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LWO4tpUUtGso"
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"/content/drive/MyDrive/final.csv\")\n",
    "\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QScktFi4tGso"
   },
   "outputs": [],
   "source": [
    "df= df[['Date', 'Available Ports', 'IsHoliday',\n",
    "       'IsWeekend', 'Year', 'Month', 'day_of_month', 'day_of_week', 'Hour',\n",
    "       'ema_3_hour_port', 'ema_3_hour_std_port', 'ema_1_day_std_port', 'ema_1_day_port',\n",
    "       'ema_7_days_std_port', 'ema_7_days_port', 'ema_30_days_port', 'ema_30_days_std_port']]\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P2aeP4_fMSGI"
   },
   "outputs": [],
   "source": [
    "df['Available Ports'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0BFv1SDKKb8j"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print(\"CUDA is not available.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "60wYPAz3KkVL"
   },
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryBACQmPLHx4"
   },
   "outputs": [],
   "source": [
    "from pytorch_lightning import seed_everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4XY5yuDKtGso"
   },
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "seed_everything(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r9HKu7hAtXG-"
   },
   "outputs": [],
   "source": [
    "# Preprocessing\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df.set_index('Date', inplace=True)\n",
    "\n",
    "# Convert the target to categorical if it isnâ€™t already (for CrossEntropyLoss)\n",
    "label_encoder = LabelEncoder()\n",
    "df['Available Ports'] = label_encoder.fit_transform(df['Available Ports'])\n",
    "\n",
    "# Splitting features and target\n",
    "X = df.drop(columns=['Available Ports'])  # Features\n",
    "y = df['Available Ports']  # Target (categorical)\n",
    "\n",
    "# Train-validation-test split (keeping temporal order)\n",
    "train_size = int(0.7 * len(df))\n",
    "val_size = int(0.2 * len(df))\n",
    "test_size = len(df) - train_size - val_size\n",
    "\n",
    "train_X, val_X, test_X = X.iloc[:train_size], X.iloc[train_size:train_size+val_size], X.iloc[train_size+val_size:]\n",
    "train_y, val_y, test_y = y.iloc[:train_size], y.iloc[train_size:train_size+val_size], y.iloc[train_size+val_size:]\n",
    "\n",
    "# Normalize features\n",
    "scaler = MinMaxScaler()\n",
    "train_X_scaled = scaler.fit_transform(train_X)\n",
    "val_X_scaled = scaler.transform(val_X)\n",
    "test_X_scaled = scaler.transform(test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "93shud2dta7g"
   },
   "outputs": [],
   "source": [
    "# Custom Dataset Class for Classification\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data, input_window, output_window):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        self.input_window = input_window\n",
    "        self.output_window = output_window\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data) - self.input_window - self.output_window\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X_data[idx:idx + self.input_window]\n",
    "        y = self.y_data.iloc[idx + self.input_window]  # Use .iloc for positional indexing\n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n",
    "\n",
    "# Informer Model for Classification\n",
    "class Informer(nn.Module):\n",
    "    def __init__(self, input_size, num_classes, hidden_size, num_layers, n_heads, dropout):\n",
    "        super(Informer, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.num_classes = num_classes\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = dropout\n",
    "\n",
    "        # Encoder\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=hidden_size, nhead=n_heads, dropout=dropout, batch_first=True),\n",
    "            num_layers=num_layers\n",
    "        )\n",
    "\n",
    "        # Fully connected layers\n",
    "        self.fc1 = nn.Linear(input_size, hidden_size)\n",
    "        self.fc_out = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fc1(x)\n",
    "        x = self.encoder(x)\n",
    "        x = self.fc_out(x)\n",
    "        return x[:, -1, :]  # Class prediction for the last time step\n",
    "\n",
    "# Dataset parameters\n",
    "n_input = 24 * 7  # Number of past hours to consider\n",
    "n_output = 1  # Predicting next time step\n",
    "\n",
    "# Create datasets and loaders\n",
    "train_dataset = TimeSeriesDataset(train_X_scaled, train_y, n_input, n_output)\n",
    "val_dataset = TimeSeriesDataset(val_X_scaled, val_y, n_input, n_output)\n",
    "test_dataset = TimeSeriesDataset(test_X_scaled, test_y, n_input, n_output)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "22fb2G7ytfOw"
   },
   "outputs": [],
   "source": [
    "# Optuna objective function for classification with early stopping\n",
    "def objective(trial):\n",
    "    # Hyperparameters to tune\n",
    "    n_heads = trial.suggest_int('n_heads', 2, 6, step=2)\n",
    "    hidden_size = trial.suggest_int('hidden_size', n_heads * 4, n_heads * 16, step=n_heads * 4)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-3)  # Adjusted learning rate range\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 3)\n",
    "    dropout = trial.suggest_float('dropout', 0.1, 0.5)\n",
    "\n",
    "    # Ensure fresh random seed for reproducibility in each trial\n",
    "    torch.manual_seed(42 + trial.number)\n",
    "\n",
    "    # Model initialization with the current trial parameters\n",
    "    model = Informer(input_size=train_X_scaled.shape[1], num_classes=7,\n",
    "                     hidden_size=hidden_size, num_layers=num_layers,\n",
    "                     n_heads=n_heads, dropout=dropout).to(device)\n",
    "\n",
    "    # Loss and optimizer\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    # Re-create the datasets and data loaders in each trial\n",
    "    train_dataset = TimeSeriesDataset(train_X_scaled, train_y, n_input, n_output)\n",
    "    val_dataset = TimeSeriesDataset(val_X_scaled, val_y, n_input, n_output)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "    best_val_accuracy = 0.0\n",
    "    patience = 10  # Increase patience for early stopping\n",
    "    patience_counter = 0  # Counter to track epochs without improvement\n",
    "\n",
    "    # Training loop with validation accuracy tracking for Optuna\n",
    "    model.train()\n",
    "    for epoch in range(50):  # Consider reducing the fixed epochs for testing purposes\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "\n",
    "            # Compute loss\n",
    "            loss = criterion(y_pred, y_batch)\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "            # Backpropagation\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Calculate validation accuracy\n",
    "        model.eval()\n",
    "        val_true = []\n",
    "        val_pred = []\n",
    "        with torch.no_grad():\n",
    "            for X_batch, y_batch in val_loader:\n",
    "                X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "                y_pred = model(X_batch)\n",
    "                y_pred_labels = y_pred.argmax(dim=1)\n",
    "                val_true.extend(y_batch.cpu().numpy())\n",
    "                val_pred.extend(y_pred_labels.cpu().numpy())\n",
    "\n",
    "        val_accuracy = accuracy_score(val_true, val_pred)\n",
    "\n",
    "        # Check if validation accuracy improved\n",
    "        if val_accuracy > best_val_accuracy:\n",
    "            best_val_accuracy = val_accuracy\n",
    "            patience_counter = 0  # Reset the counter if there is an improvement\n",
    "        else:\n",
    "            patience_counter += 1  # Increment the counter if no improvement\n",
    "\n",
    "        # Early stopping check\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "            break\n",
    "\n",
    "        model.train()\n",
    "\n",
    "    return best_val_accuracy\n",
    "\n",
    "# Run Optuna optimization\n",
    "study = optuna.create_study(direction='maximize', sampler=TPESampler(seed=42))\n",
    "study.optimize(objective, n_trials=10)\n",
    "best_params = study.best_params\n",
    "\n",
    "# Display the best parameters found\n",
    "print(f\"Best hyperparameters: {best_params}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEVjNKOR6GZ9"
   },
   "outputs": [],
   "source": [
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, X_data, y_data, input_window, output_window):\n",
    "        self.X_data = X_data\n",
    "        self.y_data = y_data\n",
    "        self.input_window = input_window\n",
    "        self.output_window = output_window\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_data) - self.input_window - self.output_window\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X_data[idx:idx + self.input_window]\n",
    "        # Adjusted indexing for y to support both pandas Series and numpy arrays\n",
    "        if isinstance(self.y_data, np.ndarray):\n",
    "            y = self.y_data[idx + self.input_window]\n",
    "        else:\n",
    "            y = self.y_data.iloc[idx + self.input_window]\n",
    "        return torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rr1qpPxvtmI8"
   },
   "outputs": [],
   "source": [
    "# Final model with best hyperparameters\n",
    "model = Informer(input_size=train_X_scaled.shape[1], num_classes=len(label_encoder.classes_),\n",
    "                 hidden_size=best_params['hidden_size'], num_layers=best_params['num_layers'],\n",
    "                 n_heads=best_params['n_heads'], dropout=best_params['dropout']).to(device)\n",
    "\n",
    "# Final training on combined train + validation data\n",
    "train_val_X = np.concatenate([train_X_scaled, val_X_scaled], axis=0)\n",
    "train_val_y = np.concatenate([train_y, val_y], axis=0)\n",
    "train_val_dataset = TimeSeriesDataset(train_val_X, train_val_y, n_input, n_output)\n",
    "train_val_loader = DataLoader(train_val_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=best_params['learning_rate'])\n",
    "\n",
    "for epoch in range(50):\n",
    "    model.train()\n",
    "    train_loss = 0.0\n",
    "    for X_batch, y_batch in train_val_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_batch)\n",
    "        loss = criterion(y_pred, y_batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    print(f\"Epoch [{epoch + 1}/50], Loss: {train_loss / len(train_val_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kmgnJUM3JB1-"
   },
   "outputs": [],
   "source": [
    "save_path = '/content/drive/MyDrive/EV_pred/final_informer_classification_model.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Final model training complete and saved as '{save_path}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-AoldyqJB1_"
   },
   "outputs": [],
   "source": [
    "model_path = '/content/drive/MyDrive/EV_pred/final_informer_classification_model.pth'\n",
    "#model.load_state_dict(torch.load(model_path))\n",
    "model.load_state_dict(torch.load(model_path, map_location=device, weights_only=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JhPfFiAJq7TT"
   },
   "outputs": [],
   "source": [
    "# Evaluate model on test set\n",
    "model.eval()\n",
    "test_true = []\n",
    "test_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in test_loader:\n",
    "        X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "        y_pred = model(X_batch)\n",
    "        y_pred_labels = y_pred.argmax(dim=1)\n",
    "        test_true.extend(y_batch.cpu().numpy())\n",
    "        test_pred.extend(y_pred_labels.cpu().numpy())\n",
    "\n",
    "# Calculate metrics\n",
    "test_f1 = f1_score(test_true, test_pred, average='weighted')\n",
    "test_accuracy = accuracy_score(test_true, test_pred)\n",
    "\n",
    "print(f\"Test F1 Score: {test_f1}\")\n",
    "print(f\"Test Accuracy: {test_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "31jdc4TXyFyB"
   },
   "source": [
    "### **5.4.8. Data Preprocessing for Training Multi Output Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sQ37TEn44MZ0"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "#!pip install optuna\n",
    "import optuna\n",
    "import json\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, accuracy_score\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "CKckiumb4Moi"
   },
   "outputs": [],
   "source": [
    "df= pd.read_csv(\"/content/drive/MyDrive/final.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "jilP7pbL4Moi"
   },
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "pQfRCq-S35G1"
   },
   "outputs": [],
   "source": [
    "# Assume `df` is your dataframe containing the dataset\n",
    "\n",
    "# Split dataset into training, validation, and test sets\n",
    "train_size = int(len(df) * 0.7)\n",
    "val_size = int(len(df) * 0.2)\n",
    "test_size = len(df) - train_size - val_size\n",
    "\n",
    "train_data = df[:train_size]\n",
    "val_data = df[train_size:train_size + val_size]\n",
    "test_data = df[train_size + val_size:]\n",
    "\n",
    "# Prepare the feature sets for training, validation, and test sets\n",
    "X_train_multi = train_data.drop([\"Date\", \"Energy Consumption\", \"Available Ports\"], axis=1)\n",
    "X_val_multi = val_data.drop([\"Date\", \"Energy Consumption\", \"Available Ports\"], axis=1)\n",
    "X_test_multi = test_data.drop([\"Date\", \"Energy Consumption\", \"Available Ports\"], axis=1)\n",
    "\n",
    "# Normalize the feature sets\n",
    "feature_scaler = MinMaxScaler()\n",
    "X_train_normalized_multi = feature_scaler.fit_transform(X_train_multi)\n",
    "X_val_normalized_multi = feature_scaler.transform(X_val_multi)\n",
    "X_test_normalized_multi = feature_scaler.transform(X_test_multi)\n",
    "\n",
    "# Prepare the target sets for energy consumption and available ports\n",
    "y_train_energy = train_data[\"Energy Consumption\"]\n",
    "y_val_energy = val_data[\"Energy Consumption\"]\n",
    "y_test_energy = test_data[\"Energy Consumption\"]\n",
    "\n",
    "y_train_ports = train_data[\"Available Ports\"]\n",
    "y_val_ports = val_data[\"Available Ports\"]\n",
    "y_test_ports = test_data[\"Available Ports\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "JYebti0FEqd-"
   },
   "outputs": [],
   "source": [
    "y_test_dates= test_data[[\"Date\", \"Energy Consumption\"]]\n",
    "y_test_dates= y_test_dates.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "gMuZBbLsE3MF"
   },
   "outputs": [],
   "source": [
    "y_test_dates_ports= test_data[[\"Date\", \"Available Ports\"]]\n",
    "y_test_dates_ports= y_test_dates_ports.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true
    },
    "id": "1OAFh66LMwPA"
   },
   "outputs": [],
   "source": [
    "# Get the last 168 rows (last week) from y_train_normalized\n",
    "last_168_rows_y_train_energy = y_train_energy[-168:]\n",
    "last_168_rows_y_train_ports = y_train_ports[-168:]\n",
    "\n",
    "# Concatenate last_168_rows_y_train with y_val_normalized\n",
    "y_val_energy_with_last_168 = np.concatenate((last_168_rows_y_train_energy, y_val_energy), axis=0)\n",
    "y_val_ports_with_last_168 = np.concatenate((last_168_rows_y_train_ports, y_val_ports), axis=0)\n",
    "\n",
    "# Do the same for y_test_normalized\n",
    "last_168_rows_y_val_energy = y_val_energy[-168:]\n",
    "last_168_rows_y_val_ports = y_val_ports[-168:]\n",
    "\n",
    "y_test_energy_with_last_168 = np.concatenate((last_168_rows_y_val_energy, y_test_energy), axis=0)\n",
    "y_test_ports_with_last_168 = np.concatenate((last_168_rows_y_val_ports, y_test_ports), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzoqRrRY5FGZ"
   },
   "outputs": [],
   "source": [
    "# Function to create sequences for multi-target data\n",
    "def create_sequences_multi(y_energy, y_ports, look_back):\n",
    "    X_energy, y_energy_seq, y_ports_seq = [], [], []\n",
    "    for i in range(len(y_energy) - look_back):\n",
    "        X_energy.append(y_energy[i:i + look_back])\n",
    "        y_energy_seq.append(y_energy[i + look_back])\n",
    "        y_ports_seq.append(y_ports[i + look_back])\n",
    "    return np.array(X_energy), np.array(y_energy_seq), np.array(y_ports_seq)\n",
    "\n",
    "look_back = 24 * 7  # Number of hours to look back\n",
    "\n",
    "# Create sequences for training, validation, and test sets\n",
    "X_train_seq_multi, y_train_seq_energy, y_train_seq_ports = create_sequences_multi(y_train_energy, y_train_ports, look_back)\n",
    "X_val_seq_multi, y_val_seq_energy, y_val_seq_ports = create_sequences_multi(y_val_energy_with_last_168, y_val_ports_with_last_168, look_back)\n",
    "X_test_seq_multi, y_test_seq_energy, y_test_seq_ports = create_sequences_multi(y_test_energy_with_last_168, y_test_ports_with_last_168, look_back)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cANmobC1MwPB"
   },
   "outputs": [],
   "source": [
    "# The sequence features will have the shape (number of sequences, sequence length, number of features)\n",
    "# Extract the corresponding feature data excluding the initial look_back period\n",
    "x_seq_shape_train = X_train_normalized_multi[look_back:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-w3HzXZMwPB"
   },
   "outputs": [],
   "source": [
    "# Combine sequences of both targets with additional features\n",
    "X_train_combined_multi = np.concatenate((X_train_seq_multi, x_seq_shape_train), axis=1)\n",
    "X_val_combined_multi = np.concatenate((X_val_seq_multi, X_val_normalized_multi), axis=1)\n",
    "X_test_combined_multi = np.concatenate((X_test_seq_multi, X_test_normalized_multi), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Mrx-jt6-6kG5"
   },
   "outputs": [],
   "source": [
    "# Convert to PyTorch tensors\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Convert targets to PyTorch tensors with appropriate types\n",
    "X_train_combined_multi = torch.tensor(X_train_combined_multi, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "X_val_combined_multi = torch.tensor(X_val_combined_multi, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "X_test_combined_multi = torch.tensor(X_test_combined_multi, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "\n",
    "y_train_seq_energy = torch.tensor(y_train_seq_energy, dtype=torch.float32).unsqueeze(-1).to(device)  # Add this\n",
    "y_val_seq_energy = torch.tensor(y_val_seq_energy, dtype=torch.float32).unsqueeze(-1).to(device)      # Add this\n",
    "y_test_seq_energy = torch.tensor(y_test_seq_energy, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "\n",
    "y_train_seq_ports = torch.tensor(y_train_seq_ports, dtype=torch.long).to(device)  # Change to long\n",
    "y_val_seq_ports = torch.tensor(y_val_seq_ports, dtype=torch.long).to(device)      # Change to long\n",
    "y_test_seq_ports = torch.tensor(y_test_seq_ports, dtype=torch.long).to(device)    # Change to long\n",
    "\n",
    "# Print the shapes of the tensors\n",
    "print(f'X_train_combined_multi shape: {X_train_combined_multi.shape}')\n",
    "print(f'X_val_combined_multi shape: {X_val_combined_multi.shape}')\n",
    "print(f'X_test_combined_multi shape: {X_test_combined_multi.shape}')\n",
    "print(f'y_train_seq_energy shape: {y_train_seq_energy.shape}')\n",
    "print(f'y_val_seq_energy shape: {y_val_seq_energy.shape}')\n",
    "print(f'y_test_seq_energy shape: {y_test_seq_energy.shape}')\n",
    "print(f'y_train_seq_ports shape: {y_train_seq_ports.shape}')\n",
    "print(f'y_val_seq_ports shape: {y_val_seq_ports.shape}')\n",
    "print(f'y_test_seq_ports shape: {y_test_seq_ports.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JYSNSD_2s79E"
   },
   "outputs": [],
   "source": [
    "df_ham= pd.read_csv(\"/content/drive/MyDrive/EV_pred/df_ham.csv\")\n",
    "df_cam= pd.read_csv(\"/content/drive/MyDrive/EV_pred/df_cam.csv\")\n",
    "df_rin= pd.read_csv(\"/content/drive/MyDrive/EV_pred/df_rin.csv\")\n",
    "df_bry= pd.read_csv(\"/content/drive/MyDrive/EV_pred/df_bry.csv\")\n",
    "\n",
    "df_ham = df_ham.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "df_cam = df_cam.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "df_rin = df_rin.drop(columns=['Unnamed: 0'], errors='ignore')\n",
    "df_bry = df_bry.drop(columns=['Unnamed: 0'], errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "E5qEAodCCsCk"
   },
   "outputs": [],
   "source": [
    "# Define the dataframes and their corresponding names\n",
    "dataframes = [df_ham, df_cam, df_rin, df_bry]\n",
    "names = ['df_ham', 'df_cam', 'df_rin', 'df_bry']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uSEWN2y4yxCE"
   },
   "outputs": [],
   "source": [
    "def slice_to_date(dataframes, names, end_date):\n",
    "    \"\"\"\n",
    "    Slices multiple dataframes to include only rows up to a specified end date.\n",
    "\n",
    "    Parameters:\n",
    "        dataframes (list): List of dataframes to process.\n",
    "        names (list): List of names corresponding to the dataframes.\n",
    "        end_date (str): The end date (inclusive) up to which the data will be sliced.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of DataFrames with meaningful names as keys.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Ensure the end date is in datetime format\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    for df, name in zip(dataframes, names):\n",
    "        # Ensure the 'Date' column is in datetime format\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "        # Slice the dataframe up to and including the end date\n",
    "        sliced_df = df[df['Date'] <= end_date]\n",
    "\n",
    "        # Save the sliced dataframe in the results dictionary\n",
    "        results[f\"{name}_sliced_to_{end_date.strftime('%Y%m%d')}\"] = sliced_df\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define the end date for slicing\n",
    "end_date = \"2020-02-29 23:00:00\"\n",
    "\n",
    "# Apply the function to slice the DataFrames\n",
    "sliced_results = slice_to_date(dataframes, names, end_date)\n",
    "\n",
    "# Save each sliced dataframe with meaningful names\n",
    "df_ham_sliced_to_20200229 = sliced_results['df_ham_sliced_to_20200229']\n",
    "df_cam_sliced_to_20200229 = sliced_results['df_cam_sliced_to_20200229']\n",
    "df_rin_sliced_to_20200229 = sliced_results['df_rin_sliced_to_20200229']\n",
    "df_bry_sliced_to_20200229 = sliced_results['df_bry_sliced_to_20200229']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nYXkGIxG-dU5"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def slice_to_date_range(dataframes, names, start_date, end_date):\n",
    "    \"\"\"\n",
    "    Slices multiple dataframes to include only rows within a specified date range.\n",
    "\n",
    "    Parameters:\n",
    "        dataframes (list): List of dataframes to process.\n",
    "        names (list): List of names corresponding to the dataframes.\n",
    "        start_date (str): The start date (inclusive) of the date range.\n",
    "        end_date (str): The end date (inclusive) of the date range.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of DataFrames with meaningful names as keys.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    # Ensure the start and end dates are in datetime format\n",
    "    start_date = pd.to_datetime(start_date)\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "\n",
    "    for df, name in zip(dataframes, names):\n",
    "        # Ensure the 'Date' column is in datetime format\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "        # Slice the dataframe within the date range (inclusive)\n",
    "        sliced_df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]\n",
    "\n",
    "        # Save the sliced dataframe in the results dictionary\n",
    "        results[f\"{name}_sliced_from_{start_date.strftime('%Y%m%d')}_to_{end_date.strftime('%Y%m%d')}\"] = sliced_df\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define the start and end dates for slicing\n",
    "start_date = \"2020-03-17 00:00:00\"\n",
    "end_date = \"2020-06-30 23:00:00\"\n",
    "\n",
    "# Apply the function to slice the DataFrames\n",
    "sliced_results = slice_to_date_range(dataframes, names, start_date, end_date)\n",
    "\n",
    "# Save each sliced dataframe with meaningful names\n",
    "df_ham_sliced_from_20200317_to_20200630 = sliced_results['df_ham_sliced_from_20200317_to_20200630']\n",
    "df_cam_sliced_from_20200317_to_20200630 = sliced_results['df_cam_sliced_from_20200317_to_20200630']\n",
    "df_rin_sliced_from_20200317_to_20200630 = sliced_results['df_rin_sliced_from_20200317_to_20200630']\n",
    "df_bry_sliced_from_20200317_to_20200630 = sliced_results['df_bry_sliced_from_20200317_to_20200630']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LzXpWp7u_h7q"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming the dataframe df_bry_sliced_from_20200317_to_20200630 exists and has a column 'Energy Consumption'\n",
    "\n",
    "# Plotting the 'Energy Consumption' column\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(df_bry_sliced_from_20200317_to_20200630['Date'], df_bry_sliced_from_20200317_to_20200630['Energy Consumption'], label='Energy Consumption', color='b')\n",
    "\n",
    "# Adding labels and title\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Energy Consumption')\n",
    "plt.title('Energy Consumption Over Time (2020-03-17 to 2020-06-30)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hb_pxAcOwFC0"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def select_random_two_weeks(df, start_date, end_date, seed=None):\n",
    "    \"\"\"\n",
    "    Selects a random two-week period (14 days) within the specified date range of the dataframe.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The input dataframe with a 'Date' column.\n",
    "        start_date (str): Start date of the filter range (inclusive).\n",
    "        end_date (str): End date of the filter range (inclusive).\n",
    "        seed (int): Seed for reproducibility. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe slice containing the selected random two-week period.\n",
    "    \"\"\"\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "\n",
    "    # Ensure the 'Date' column is in datetime format\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "\n",
    "    # Filter dataframe within the date range\n",
    "    filtered_df = df[(df['Date'] >= start_date) & (df['Date'] <= end_date)]\n",
    "\n",
    "    # Check if there are enough days to select a 2-week period\n",
    "    if len(filtered_df) < 14:\n",
    "        raise ValueError(\"Not enough data in the specified date range for a two-week selection.\")\n",
    "\n",
    "    # Calculate the random start date for the two-week period\n",
    "    max_start_date = filtered_df['Date'].max() - pd.Timedelta(days=14)  # Ensure room for 14 days\n",
    "    random_start = filtered_df['Date'].min() + pd.to_timedelta(\n",
    "        np.random.randint(0, (max_start_date - filtered_df['Date'].min()).days + 1), unit='D'\n",
    "    )\n",
    "    random_end = random_start + pd.Timedelta(days=14)  # Includes the start date, resulting in 14 days\n",
    "\n",
    "    # Return the two-week period\n",
    "    return filtered_df[(filtered_df['Date'] >= random_start) & (filtered_df['Date'] <= random_end)]\n",
    "\n",
    "def process_dataframes_with_named_outputs(dataframes, names, early_lockdown_range, partial_reopenings_range, seed=None):\n",
    "    \"\"\"\n",
    "    Processes multiple dataframes to extract random two-week periods during Early Lockdown and Partial Reopenings.\n",
    "    Each sample is stored in a separate dataframe with a specific naming convention.\n",
    "\n",
    "    Parameters:\n",
    "        dataframes (list): List of dataframes to process.\n",
    "        names (list): List of names corresponding to the dataframes.\n",
    "        early_lockdown_range (tuple): Start and end dates for the Early Lockdown phase.\n",
    "        partial_reopenings_range (tuple): Start and end dates for the Partial Reopenings phase.\n",
    "        seed (int): Seed for reproducibility. Defaults to None.\n",
    "\n",
    "    Returns:\n",
    "        dict: Dictionary of DataFrames with meaningful names as keys.\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "\n",
    "    for df, name in zip(dataframes, names):\n",
    "        # Select Early Lockdown sample\n",
    "        early_lockdown_sample = select_random_two_weeks(\n",
    "            df, early_lockdown_range[0], early_lockdown_range[1], seed=seed\n",
    "        )\n",
    "        # Select Partial Reopenings sample\n",
    "        partial_reopenings_sample = select_random_two_weeks(\n",
    "            df, partial_reopenings_range[0], partial_reopenings_range[1], seed=seed\n",
    "        )\n",
    "\n",
    "        # Assign results to the appropriate names\n",
    "        results[f\"{name}_covid_early_lockdown_range\"] = early_lockdown_sample\n",
    "        results[f\"{name}_covid_partial_reopenings_range\"] = partial_reopenings_sample\n",
    "\n",
    "    return results\n",
    "\n",
    "# List of dataframes and their corresponding names\n",
    "dataframes = [df_ham, df_cam, df_rin, df_bry]\n",
    "names = ['df_ham', 'df_cam', 'df_rin', 'df_bry']\n",
    "\n",
    "# Define the date ranges\n",
    "early_lockdown_range = ('2020-03-17', '2020-05-31')\n",
    "partial_reopenings_range = ('2020-06-01', '2020-06-30') #'2020-11-30')\n",
    "\n",
    "# Apply the function with a seed for reproducibility\n",
    "results = process_dataframes_with_named_outputs(dataframes, names, early_lockdown_range, partial_reopenings_range, seed=13)\n",
    "\n",
    "# Save each sample as a separate dataframe\n",
    "df_ham_covid_early_lockdown_range = results['df_ham_covid_early_lockdown_range']\n",
    "df_ham_covid_partial_reopenings_range = results['df_ham_covid_partial_reopenings_range']\n",
    "df_cam_covid_early_lockdown_range = results['df_cam_covid_early_lockdown_range']\n",
    "df_cam_covid_partial_reopenings_range = results['df_cam_covid_partial_reopenings_range']\n",
    "df_rin_covid_early_lockdown_range = results['df_rin_covid_early_lockdown_range']\n",
    "df_rin_covid_partial_reopenings_range = results['df_rin_covid_partial_reopenings_range']\n",
    "df_bry_covid_early_lockdown_range = results['df_bry_covid_early_lockdown_range']\n",
    "df_bry_covid_partial_reopenings_range = results['df_bry_covid_partial_reopenings_range']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1s3C41mtZiLV"
   },
   "outputs": [],
   "source": [
    "# Function to process a DataFrame for LSTM testing\n",
    "def prepare_lstm_data(df, feature_scaler, look_back):\n",
    "    # Drop unwanted columns and normalize features\n",
    "    X_test_multi = df.drop([\"Date\", \"Energy Consumption\", \"Available Ports\"], axis=1)\n",
    "    X_test_normalized_multi = feature_scaler.transform(X_test_multi)\n",
    "\n",
    "    # Slice the normalized features to match the sequence length\n",
    "    X_test_normalized_multi = X_test_normalized_multi[look_back:]  # Remove the first 'look_back' rows\n",
    "\n",
    "    # Extract targets\n",
    "    y_test_energy = df[\"Energy Consumption\"].values\n",
    "    y_test_ports = df[\"Available Ports\"].values\n",
    "    y_test_dates = df[[\"Date\", \"Energy Consumption\"]].set_index(\"Date\")\n",
    "    y_test_dates_ports = df[[\"Date\", \"Available Ports\"]].set_index(\"Date\")\n",
    "\n",
    "    # Create sequences\n",
    "    X_test_seq_multi, y_test_seq_energy, y_test_seq_ports = create_sequences_multi(\n",
    "        y_test_energy, y_test_ports, look_back\n",
    "    )\n",
    "\n",
    "    # Combine sequences with additional features\n",
    "    X_test_combined_multi = np.concatenate((X_test_seq_multi, X_test_normalized_multi), axis=1)\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_test_combined_multi = torch.tensor(X_test_combined_multi, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "    y_test_seq_energy = torch.tensor(y_test_seq_energy, dtype=torch.float32).unsqueeze(-1).to(device)\n",
    "    y_test_seq_ports = torch.tensor(y_test_seq_ports, dtype=torch.long).to(device)  # Ports as long tensor\n",
    "\n",
    "    return X_test_combined_multi, y_test_seq_energy, y_test_seq_ports\n",
    "\n",
    "# Apply the function to each test DataFrame for each station\n",
    "X_test_ham_sliced_to_20200229, y_test_energy_ham_sliced_to_20200229, y_test_ports_ham_sliced_to_20200229 = prepare_lstm_data(df_ham_sliced_to_20200229, feature_scaler, look_back)\n",
    "X_test_cam_sliced_to_20200229, y_test_energy_cam_sliced_to_20200229, y_test_ports_cam_sliced_to_20200229 = prepare_lstm_data(df_cam_sliced_to_20200229, feature_scaler, look_back)\n",
    "X_test_rin_sliced_to_20200229, y_test_energy_rin_sliced_to_20200229, y_test_ports_rin_sliced_to_20200229 = prepare_lstm_data(df_rin_sliced_to_20200229, feature_scaler, look_back)\n",
    "X_test_bry_sliced_to_20200229, y_test_energy_bry_sliced_to_20200229, y_test_ports_bry_sliced_to_20200229 = prepare_lstm_data(df_bry_sliced_to_20200229, feature_scaler, look_back)\n",
    "\n",
    "X_test_ham_covid_early_lockdown_range, y_test_energy_ham_covid_early_lockdown_range, y_test_ports_ham_covid_early_lockdown_range = prepare_lstm_data(df_ham_covid_early_lockdown_range, feature_scaler, look_back)\n",
    "X_test_cam_covid_early_lockdown_range, y_test_energy_cam_covid_early_lockdown_range, y_test_ports_cam_covid_early_lockdown_range = prepare_lstm_data(df_cam_covid_early_lockdown_range, feature_scaler, look_back)\n",
    "X_test_rin_covid_early_lockdown_range, y_test_energy_rin_covid_early_lockdown_range, y_test_ports_rin_covid_early_lockdown_range = prepare_lstm_data(df_rin_covid_early_lockdown_range, feature_scaler, look_back)\n",
    "X_test_bry_covid_early_lockdown_range, y_test_energy_bry_covid_early_lockdown_range, y_test_ports_bry_covid_early_lockdown_range = prepare_lstm_data(df_bry_covid_early_lockdown_range, feature_scaler, look_back)\n",
    "\n",
    "X_test_ham_covid_partial_reopenings_range, y_test_energy_ham_covid_partial_reopenings_range, y_test_ports_ham_covid_partial_reopenings_range = prepare_lstm_data(df_ham_covid_partial_reopenings_range, feature_scaler, look_back)\n",
    "X_test_cam_covid_partial_reopenings_range, y_test_energy_cam_covid_partial_reopenings_range, y_test_ports_cam_covid_partial_reopenings_range = prepare_lstm_data(df_cam_covid_partial_reopenings_range, feature_scaler, look_back)\n",
    "X_test_rin_covid_partial_reopenings_range, y_test_energy_rin_covid_partial_reopenings_range, y_test_ports_rin_covid_partial_reopenings_range = prepare_lstm_data(df_rin_covid_partial_reopenings_range, feature_scaler, look_back)\n",
    "X_test_bry_covid_partial_reopenings_range, y_test_energy_bry_covid_partial_reopenings_range, y_test_ports_bry_covid_partial_reopenings_range = prepare_lstm_data(df_bry_covid_partial_reopenings_range, feature_scaler, look_back)\n",
    "\n",
    "X_test_df_bry_sliced_from_20200317_to_20200630, y_test_energy_df_bry_sliced_from_20200317_to_20200630, y_test_ports_df_bry_sliced_from_20200317_to_20200630 = prepare_lstm_data(df_bry_sliced_from_20200317_to_20200630, feature_scaler, look_back)\n",
    "# Print shapes for verification\n",
    "print(f'X_test_ham_sliced_to_20200229 shape: {X_test_ham_sliced_to_20200229.shape}, y_test_energy_ham_sliced_to_20200229 shape: {y_test_energy_ham_sliced_to_20200229.shape}, y_test_ports_ham_sliced_to_20200229 shape: {y_test_ports_ham_sliced_to_20200229.shape}')\n",
    "print(f'X_test_cam_sliced_to_20200229 shape: {X_test_cam_sliced_to_20200229.shape}, y_test_energy_cam_sliced_to_20200229 shape: {y_test_energy_cam_sliced_to_20200229.shape}, y_test_ports_cam_sliced_to_20200229 shape: {y_test_ports_cam_sliced_to_20200229.shape}')\n",
    "print(f'X_test_rin_sliced_to_20200229 shape: {X_test_rin_sliced_to_20200229.shape}, y_test_energy_rin_sliced_to_20200229 shape: {y_test_energy_rin_sliced_to_20200229.shape}, y_test_ports_rin_sliced_to_20200229 shape: {y_test_ports_rin_sliced_to_20200229.shape}')\n",
    "print(f'X_test_bry_sliced_to_20200229 shape: {X_test_bry_sliced_to_20200229.shape}, y_test_energy_bry_sliced_to_20200229 shape: {y_test_energy_bry_sliced_to_20200229.shape}, y_test_ports_bry_sliced_to_20200229 shape: {y_test_ports_bry_sliced_to_20200229.shape}')\n",
    "\n",
    "print(f'X_test_ham_covid_early_lockdown_range shape: {X_test_ham_covid_early_lockdown_range.shape}, y_test_energy_ham_covid_early_lockdown_range shape: {y_test_energy_ham_covid_early_lockdown_range.shape}, y_test_ports_ham_covid_early_lockdown_range shape: {y_test_ports_ham_covid_early_lockdown_range.shape}')\n",
    "print(f'X_test_cam_covid_early_lockdown_range shape: {X_test_cam_covid_early_lockdown_range.shape}, y_test_energy_cam_covid_early_lockdown_range shape: {y_test_energy_cam_covid_early_lockdown_range.shape}, y_test_ports_cam_covid_early_lockdown_range shape: {y_test_ports_cam_covid_early_lockdown_range.shape}')\n",
    "print(f'X_test_rin_covid_early_lockdown_range shape: {X_test_rin_covid_early_lockdown_range.shape}, y_test_energy_rin_covid_early_lockdown_range shape: {y_test_energy_rin_covid_early_lockdown_range.shape}, y_test_ports_rin_covid_early_lockdown_range shape: {y_test_ports_rin_covid_early_lockdown_range.shape}')\n",
    "print(f'X_test_bry_covid_early_lockdown_range shape: {X_test_bry_covid_early_lockdown_range.shape}, y_test_energy_bry_covid_early_lockdown_range shape: {y_test_energy_bry_covid_early_lockdown_range.shape}, y_test_ports_bry_covid_early_lockdown_range shape: {y_test_ports_bry_covid_early_lockdown_range.shape}')\n",
    "\n",
    "print(f'X_test_ham_covid_partial_reopenings_range shape: {X_test_ham_covid_partial_reopenings_range.shape}, y_test_energy_ham_covid_partial_reopenings_range shape: {y_test_energy_ham_covid_partial_reopenings_range.shape}, y_test_ports_ham_covid_partial_reopenings_range shape: {y_test_ports_ham_covid_partial_reopenings_range.shape}')\n",
    "print(f'X_test_cam_covid_partial_reopenings_range shape: {X_test_cam_covid_partial_reopenings_range.shape}, y_test_energy_cam_covid_partial_reopenings_range shape: {y_test_energy_cam_covid_partial_reopenings_range.shape}, y_test_ports_cam_covid_partial_reopenings_range shape: {y_test_ports_cam_covid_partial_reopenings_range.shape}')\n",
    "print(f'X_test_rin_covid_partial_reopenings_range shape: {X_test_rin_covid_partial_reopenings_range.shape}, y_test_energy_rin_covid_partial_reopenings_range shape: {y_test_energy_rin_covid_partial_reopenings_range.shape}, y_test_ports_rin_covid_partial_reopenings_range shape: {y_test_ports_rin_covid_partial_reopenings_range.shape}')\n",
    "print(f'X_test_bry_covid_partial_reopenings_range shape: {X_test_bry_covid_partial_reopenings_range.shape}, y_test_energy_bry_covid_partial_reopenings_range shape: {y_test_energy_bry_covid_partial_reopenings_range.shape}, y_test_ports_bry_covid_partial_reopenings_range shape: {y_test_ports_bry_covid_partial_reopenings_range.shape}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5sQYLHYG3Mjc"
   },
   "source": [
    "### **5.4.9. Multi Output GRU Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rLb7fmfx7XY5"
   },
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class MultiOutputTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, targets_regression, targets_classification):\n",
    "        self.sequences = sequences\n",
    "        self.targets_regression = targets_regression\n",
    "        self.targets_classification = targets_classification\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets_regression[idx], self.targets_classification[idx]\n",
    "\n",
    "# GRU Model\n",
    "class MultiOutputGRUModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_prob, output_size_classification):\n",
    "        super(MultiOutputGRUModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc_regression = nn.Linear(hidden_size, 1)\n",
    "        self.fc_classification = nn.Linear(hidden_size, output_size_classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.gru(x, h0)\n",
    "        out = out[:, -1, :]\n",
    "        regression_output = self.fc_regression(out)\n",
    "        classification_output = self.fc_classification(out)\n",
    "        return regression_output, classification_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "enp0OXk25qYl"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters and Configurations\n",
    "batch_size = 32\n",
    "input_size = 1  # Adjust if you have more features\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "output_size_classification = 7  # Number of classes (0 to 6)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = MultiOutputTimeSeriesDataset(X_train_combined_multi, y_train_seq_energy, y_train_seq_ports)\n",
    "val_dataset = MultiOutputTimeSeriesDataset(X_val_combined_multi, y_val_seq_energy, y_val_seq_ports)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Optuna Study for Hyperparameter Optimization\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 64)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 3)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "\n",
    "    # Create the model with suggested hyperparameters\n",
    "    model = MultiOutputGRUModel(input_size, hidden_size, num_layers, dropout_prob, output_size_classification).to(device)\n",
    "\n",
    "    # Define loss functions and optimizer\n",
    "    criterion_regression = nn.MSELoss().to(device)\n",
    "    criterion_classification = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets_regression, targets_classification in train_loader:\n",
    "            inputs, targets_regression, targets_classification = inputs.to(device), targets_regression.to(device), targets_classification.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs_regression, outputs_classification = model(inputs)\n",
    "            #loss_regression = criterion_regression(outputs_regression, targets_regression.unsqueeze(-1))\n",
    "            loss_regression = criterion_regression(outputs_regression, targets_regression)\n",
    "            loss_classification = criterion_classification(outputs_classification, targets_classification)\n",
    "            loss = loss_regression + loss_classification\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_regression_loss = 0\n",
    "        val_classification_loss = 0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets_regression, val_targets_classification in val_loader:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_targets_regression = val_targets_regression.to(device)\n",
    "                val_targets_classification = val_targets_classification.to(device)\n",
    "                val_outputs_regression, val_outputs_classification = model(val_inputs)\n",
    "                #val_regression_loss += criterion_regression(val_outputs_regression, val_targets_regression.unsqueeze(-1)).item()\n",
    "                val_regression_loss += criterion_regression(val_outputs_regression, val_targets_regression).item()\n",
    "                val_classification_loss += criterion_classification(val_outputs_classification, val_targets_classification).item()\n",
    "                val_predictions.extend(torch.argmax(val_outputs_classification, dim=1).cpu().numpy())\n",
    "                val_targets.extend(val_targets_classification.cpu().numpy())\n",
    "\n",
    "        # Normalize the losses by the number of validation samples\n",
    "        val_regression_loss /= len(val_loader)\n",
    "        val_classification_loss /= len(val_loader)\n",
    "        val_loss = val_regression_loss + val_classification_loss  # Total validation loss\n",
    "\n",
    "        # Print validation losses for tracking\n",
    "        print(f'Epoch {epoch+1}, Val Regression Loss: {val_regression_loss:.4f}, Val Classification Loss: {val_classification_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "\n",
    "            # Save the best model's state dictionary\n",
    "            torch.save(model.state_dict(), 'best_multioutput_gru_model.pth')\n",
    "\n",
    "            # Save the best hyperparameters\n",
    "            best_params = {\n",
    "                'hidden_size': hidden_size,\n",
    "                'num_layers': num_layers,\n",
    "                'dropout_prob': dropout_prob,\n",
    "                'learning_rate': learning_rate\n",
    "            }\n",
    "\n",
    "            with open('best_multioutput_gru_params.json', 'w') as f:\n",
    "                json.dump(best_params, f)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    # Return the combined validation loss as the objective to minimize\n",
    "    return val_loss\n",
    "\n",
    "# Perform hyperparameter optimization\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_val_loss = study.best_value\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bSx3C3_xYdQ5"
   },
   "source": [
    "Best Hyperparameters: {'hidden_size': 56, 'num_layers': 2, 'dropout_prob': 0.31034939184612553, 'learning_rate': 0.002000716164292076}\n",
    "Best Validation Loss: 0.45263606131496564\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XUvsd89Adr9z"
   },
   "outputs": [],
   "source": [
    "# Load the best hyperparameters\n",
    "#with open('best_multioutput_lstm_params.json', 'r') as f:\n",
    "#    best_params = json.load(f)\n",
    "best_mo_gru_params={\n",
    "    'hidden_size': 56,\n",
    "    'num_layers': 2,\n",
    "    'dropout_prob': 0.31034939184612553,\n",
    "    'learning_rate': 0.002000716164292076,\n",
    "}\n",
    "input_size=1\n",
    "output_size_classification= 7 # Number of classes (0 to 6)\n",
    "\n",
    "\n",
    "# Create the model with the best hyperparameters\n",
    "model = MultiOutputGRUModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size= best_mo_gru_params['hidden_size'],\n",
    "    num_layers= best_mo_gru_params['num_layers'],\n",
    "    dropout_prob= best_mo_gru_params['dropout_prob'],\n",
    "    output_size_classification= output_size_classification\n",
    ").to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L_FMhkMk-NQM"
   },
   "outputs": [],
   "source": [
    "# Load the best hyperparameters\n",
    "#with open('best_multioutput_lstm_params.json', 'r') as f:\n",
    "#    best_params = json.load(f)\n",
    "best_mo_gru_params={\n",
    "    'hidden_size': 56,\n",
    "    'num_layers': 2,\n",
    "    'dropout_prob': 0.31034939184612553,\n",
    "    'learning_rate': 0.002000716164292076,\n",
    "}\n",
    "input_size=1\n",
    "output_size_classification= 7 # Number of classes (0 to 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Combine training and validation data\n",
    "X_combined = torch.cat((X_train_combined_multi, X_val_combined_multi), dim=0)\n",
    "y_combined_regression = torch.cat((y_train_seq_energy, y_val_seq_energy), dim=0)\n",
    "y_combined_classification = torch.cat((y_train_seq_ports, y_val_seq_ports), dim=0)\n",
    "\n",
    "# Create combined dataset and dataloader\n",
    "combined_dataset = MultiOutputTimeSeriesDataset(X_combined, y_combined_regression, y_combined_classification)\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Create the model with the best hyperparameters\n",
    "model = MultiOutputGRUModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size= best_mo_gru_params['hidden_size'],\n",
    "    num_layers= best_mo_gru_params['num_layers'],\n",
    "    dropout_prob= best_mo_gru_params['dropout_prob'],\n",
    "    output_size_classification= output_size_classification\n",
    ").to(device)\n",
    "\n",
    "# Define loss functions and optimizer\n",
    "criterion_regression = nn.MSELoss().to(device)\n",
    "criterion_classification = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_mo_gru_params['learning_rate'])\n",
    "\n",
    "# Train the model on the combined dataset\n",
    "num_epochs = 100\n",
    "training_log = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0  # Initialize running loss\n",
    "    progress_bar = tqdm(enumerate(combined_loader), total=len(combined_loader), desc=f'Epoch {epoch+1}/{num_epochs}', ncols=100)\n",
    "\n",
    "    for i, (inputs, targets_regression, targets_classification) in progress_bar:\n",
    "        inputs, targets_regression, targets_classification = inputs.to(device), targets_regression.to(device), targets_classification.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs_regression, outputs_classification = model(inputs)\n",
    "        #loss_regression = criterion_regression(outputs_regression, targets_regression.unsqueeze(-1))\n",
    "        loss_regression = criterion_regression(outputs_regression, targets_regression)\n",
    "        loss_classification = criterion_classification(outputs_classification, targets_classification)\n",
    "        loss = loss_regression + loss_classification\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()  # Update running loss\n",
    "        progress_bar.set_postfix(loss=running_loss/(i+1))  # Update progress bar with the average loss\n",
    "\n",
    "    average_loss = running_loss / len(combined_loader)\n",
    "    training_log.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'loss': average_loss\n",
    "    })\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(combined_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0UscoqXfAk59"
   },
   "outputs": [],
   "source": [
    "# Save the final model's state dictionary\n",
    "save_path = '/content/drive/MyDrive/EV_pred/final_multioutput_gru_model.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Final model training complete and saved as '{save_path}'\")\n",
    "\n",
    "# Save the training log\n",
    "csv_path = '/content/drive/MyDrive/EV_pred/training_log_multioutput_gru.csv'\n",
    "pd.DataFrame(training_log).to_csv(csv_path, index=False)\n",
    "print(\"Training log saved to 'training_log_multioutput_gru.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6rsP44mT913n"
   },
   "outputs": [],
   "source": [
    "# Define the test dataset\n",
    "test_dataset = MultiOutputTimeSeriesDataset(X_test_combined_multi, y_test_seq_energy, y_test_seq_ports)\n",
    "# Define the test DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wcHOxSai913o"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, accuracy_score\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the path where the model is saved on Google Drive\n",
    "model_path = '/content/drive/MyDrive/EV_pred/final_multioutput_gru_model.pth'\n",
    "# Load the saved model's state dictionary\n",
    "#model.load_state_dict(torch.load(model_path))\n",
    "model.load_state_dict(torch.load(model_path, map_location=torch.device('cpu') ))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Define lists to hold predictions and true labels\n",
    "test_predictions_regression = []\n",
    "test_predictions_classification = []\n",
    "test_true_labels_classification = []\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels_regression, test_labels_classification in test_loader:\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_labels_regression = test_labels_regression.to(device)\n",
    "        test_labels_classification = test_labels_classification.to(device)\n",
    "\n",
    "        test_outputs_regression, test_outputs_classification = model(test_inputs)\n",
    "\n",
    "        test_predictions_regression.append(test_outputs_regression.cpu().numpy())\n",
    "        test_predictions_classification.append(torch.argmax(test_outputs_classification, dim=1).cpu().numpy())\n",
    "        test_true_labels_classification.append(test_labels_classification.cpu().numpy())\n",
    "\n",
    "# Concatenate predictions and true labels\n",
    "test_predictions_regression = np.concatenate(test_predictions_regression)\n",
    "test_predictions_classification = np.concatenate(test_predictions_classification)\n",
    "test_true_labels_classification = np.concatenate(test_true_labels_classification)\n",
    "\n",
    "# Calculate metrics\n",
    "test_mse = mean_squared_error(y_test_seq_energy.cpu().numpy(), test_predictions_regression)\n",
    "test_mae = mean_absolute_error(y_test_seq_energy.cpu().numpy(), test_predictions_regression)\n",
    "test_f1 = f1_score(test_true_labels_classification, test_predictions_classification, average='weighted')\n",
    "test_accuracy = accuracy_score(test_true_labels_classification, test_predictions_classification)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE for MO_GRU: {test_mse:.4f}\")\n",
    "print(f\"Test MAE for MO_GRU: {test_mae:.4f}\")\n",
    "print(f\"Test F1 Score for MO_GRU: {test_f1:.4f}\")\n",
    "print(f\"Test Accuracy for MO_GRU: {test_accuracy:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9d-RHyyK1_5L"
   },
   "source": [
    "### **5.4.10. Multi Output LSTM Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bjvf0DtQhbNy"
   },
   "outputs": [],
   "source": [
    "# Custom dataset class\n",
    "class MultiOutputTimeSeriesDataset(Dataset):\n",
    "    def __init__(self, sequences, targets_regression, targets_classification):\n",
    "        self.sequences = sequences\n",
    "        self.targets_regression = targets_regression\n",
    "        self.targets_classification = targets_classification\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sequences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sequences[idx], self.targets_regression[idx], self.targets_classification[idx]\n",
    "\n",
    "# Multi-output LSTM model class\n",
    "class MultiOutputLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, dropout_prob, output_size_classification):\n",
    "        super(MultiOutputLSTMModel, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        self.num_layers = num_layers\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, dropout=dropout_prob)\n",
    "        self.fc_regression = nn.Linear(hidden_size, 1)\n",
    "        self.fc_classification = nn.Linear(hidden_size, output_size_classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(x.device)\n",
    "        out, _ = self.lstm(x, (h0, c0))\n",
    "        out = out[:, -1, :]\n",
    "        regression_output = self.fc_regression(out)\n",
    "        classification_output = self.fc_classification(out)\n",
    "        return regression_output, classification_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v-9-0_CvFkZY"
   },
   "outputs": [],
   "source": [
    "# Hyperparameters and Configurations\n",
    "batch_size = 32\n",
    "input_size = 1  # Adjust if you have more features\n",
    "num_epochs = 100\n",
    "patience = 10\n",
    "output_size_classification = 7  # Number of classes (0 to 6)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create data loaders\n",
    "train_dataset = MultiOutputTimeSeriesDataset(X_train_combined_multi, y_train_seq_energy, y_train_seq_ports)\n",
    "val_dataset = MultiOutputTimeSeriesDataset(X_val_combined_multi, y_val_seq_energy, y_val_seq_ports)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Optuna Study for Hyperparameter Optimization\n",
    "study = optuna.create_study(direction='minimize', sampler=optuna.samplers.TPESampler(seed=42))\n",
    "\n",
    "# Define the objective function for Optuna\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    hidden_size = trial.suggest_int('hidden_size', 32, 64)\n",
    "    num_layers = trial.suggest_int('num_layers', 2, 3)\n",
    "    dropout_prob = trial.suggest_float('dropout_prob', 0.1, 0.5)\n",
    "    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-2)\n",
    "\n",
    "    # Create the model with suggested hyperparameters\n",
    "    model = MultiOutputLSTMModel(input_size, hidden_size, num_layers, dropout_prob, output_size_classification).to(device)\n",
    "\n",
    "    # Define loss functions and optimizer\n",
    "    criterion_regression = nn.MSELoss().to(device)\n",
    "    criterion_classification = nn.CrossEntropyLoss().to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for inputs, targets_regression, targets_classification in train_loader:\n",
    "            inputs, targets_regression, targets_classification = inputs.to(device), targets_regression.to(device), targets_classification.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs_regression, outputs_classification = model(inputs)\n",
    "            loss_regression = criterion_regression(outputs_regression, targets_regression)\n",
    "            loss_classification = criterion_classification(outputs_classification, targets_classification)\n",
    "            loss = loss_regression + loss_classification\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Validation loop\n",
    "        model.eval()\n",
    "        val_regression_loss = 0\n",
    "        val_classification_loss = 0\n",
    "        val_predictions = []\n",
    "        val_targets = []\n",
    "        with torch.no_grad():\n",
    "            for val_inputs, val_targets_regression, val_targets_classification in val_loader:\n",
    "                val_inputs = val_inputs.to(device)\n",
    "                val_targets_regression = val_targets_regression.to(device)\n",
    "                val_targets_classification = val_targets_classification.to(device)\n",
    "                val_outputs_regression, val_outputs_classification = model(val_inputs)\n",
    "                val_regression_loss += criterion_regression(val_outputs_regression, val_targets_regression).item()\n",
    "                val_classification_loss += criterion_classification(val_outputs_classification, val_targets_classification).item()\n",
    "                val_predictions.extend(torch.argmax(val_outputs_classification, dim=1).cpu().numpy())\n",
    "                val_targets.extend(val_targets_classification.cpu().numpy())\n",
    "\n",
    "        # Normalize the losses by the number of validation samples\n",
    "        val_regression_loss /= len(val_loader)\n",
    "        val_classification_loss /= len(val_loader)\n",
    "        val_loss = val_regression_loss + val_classification_loss  # Total validation loss\n",
    "\n",
    "        # Print validation losses for tracking\n",
    "        print(f'Epoch {epoch+1}, Val Regression Loss: {val_regression_loss:.4f}, Val Classification Loss: {val_classification_loss:.4f}, Val Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Early stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            patience_counter = 0\n",
    "\n",
    "            # Save the best model's state dictionary\n",
    "            torch.save(model.state_dict(), 'best_multioutput_lstm_model.pth')\n",
    "\n",
    "            # Save the best hyperparameters\n",
    "            best_params = {\n",
    "                'hidden_size': hidden_size,\n",
    "                'num_layers': num_layers,\n",
    "                'dropout_prob': dropout_prob,\n",
    "                'learning_rate': learning_rate\n",
    "            }\n",
    "\n",
    "            with open('best_multioutput_lstm_params.json', 'w') as f:\n",
    "                json.dump(best_params, f)\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                break\n",
    "\n",
    "    # Return the combined validation loss as the objective to minimize\n",
    "    return val_loss\n",
    "\n",
    "# Perform hyperparameter optimization\n",
    "study.optimize(objective, n_trials=50)\n",
    "\n",
    "best_params = study.best_params\n",
    "best_val_loss = study.best_value\n",
    "print(f\"Best Hyperparameters: {best_params}\")\n",
    "print(f\"Best Validation Loss: {best_val_loss}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cugsVlCOzyTB"
   },
   "source": [
    "Best Hyperparameters: {'hidden_size': 64, 'num_layers': 2, 'dropout_prob': 0.2975636226379064, 'learning_rate': 0.000544701286921017}\n",
    "Best Validation Loss: 6.452369216672132\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9NdavkYvh4EP"
   },
   "outputs": [],
   "source": [
    "    hidden_size= 35,\n",
    "    num_layers= 2,\n",
    "    dropout_prob=0.1245094994892297,\n",
    "    lr=0.0004611478696350405"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m2cVdby6F2jm"
   },
   "outputs": [],
   "source": [
    "# Load the best hyperparameters\n",
    "#with open('best_multioutput_lstm_params.json', 'r') as f:\n",
    "#    best_params = json.load(f)\n",
    "best_mo_lstm_params={\n",
    "    'hidden_size': 64,\n",
    "    'num_layers': 2,\n",
    "    'dropout_prob': 0.2975636226379064,\n",
    "    'learning_rate': 0.000544701286921017,\n",
    "}\n",
    "input_size=1\n",
    "output_size_classification= 7 # Number of classes (0 to 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Combine training and validation data\n",
    "X_combined = torch.cat((X_train_combined_multi, X_val_combined_multi), dim=0)\n",
    "y_combined_regression = torch.cat((y_train_seq_energy, y_val_seq_energy), dim=0)\n",
    "y_combined_classification = torch.cat((y_train_seq_ports, y_val_seq_ports), dim=0)\n",
    "\n",
    "# Create combined dataset and dataloader\n",
    "combined_dataset = MultiOutputTimeSeriesDataset(X_combined, y_combined_regression, y_combined_classification)\n",
    "combined_loader = DataLoader(combined_dataset, batch_size=32, shuffle=False)\n",
    "\n",
    "# Create the model with the best hyperparameters\n",
    "model = MultiOutputLSTMModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size= best_mo_lstm_params['hidden_size'],\n",
    "    num_layers= best_mo_lstm_params['num_layers'],\n",
    "    dropout_prob= best_mo_lstm_params['dropout_prob'],\n",
    "    output_size_classification= output_size_classification\n",
    ").to(device)\n",
    "\n",
    "# Define loss functions and optimizer\n",
    "criterion_regression = nn.MSELoss().to(device)\n",
    "criterion_classification = nn.CrossEntropyLoss().to(device)\n",
    "optimizer = optim.Adam(model.parameters(), lr=best_mo_lstm_params['learning_rate'])\n",
    "\n",
    "# Train the model on the combined dataset\n",
    "num_epochs = 400\n",
    "training_log = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    running_loss = 0.0  # Initialize running loss\n",
    "    progress_bar = tqdm(enumerate(combined_loader), total=len(combined_loader), desc=f'Epoch {epoch+1}/{num_epochs}', ncols=100)\n",
    "\n",
    "    for i, (inputs, targets_regression, targets_classification) in progress_bar:\n",
    "        inputs, targets_regression, targets_classification = inputs.to(device), targets_regression.to(device), targets_classification.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs_regression, outputs_classification = model(inputs)\n",
    "        #loss_regression = criterion_regression(outputs_regression, targets_regression.unsqueeze(-1))\n",
    "        loss_regression = criterion_regression(outputs_regression, targets_regression)\n",
    "        loss_classification = criterion_classification(outputs_classification, targets_classification)\n",
    "        loss = loss_regression + loss_classification\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()  # Update running loss\n",
    "        progress_bar.set_postfix(loss=running_loss/(i+1))  # Update progress bar with the average loss\n",
    "\n",
    "    average_loss = running_loss / len(combined_loader)\n",
    "    training_log.append({\n",
    "        'epoch': epoch + 1,\n",
    "        'loss': average_loss\n",
    "    })\n",
    "\n",
    "    print(f'Epoch {epoch+1}, Loss: {running_loss/len(combined_loader):.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MLihU9I1GKnc"
   },
   "outputs": [],
   "source": [
    "# Save the final model's state dictionary\n",
    "save_path = '/content/drive/MyDrive/EV_pred/final_multioutput_lstm_model.pth'\n",
    "torch.save(model.state_dict(), save_path)\n",
    "print(f\"Final model training complete and saved as '{save_path}'\")\n",
    "\n",
    "# Save the training log\n",
    "csv_path = '/content/drive/MyDrive/EV_pred/training_log_multioutput_lstm.csv'\n",
    "pd.DataFrame(training_log).to_csv(csv_path, index=False)\n",
    "print(\"Training log saved to 'training_log_multioutput_gru.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jYQbUqHGKnd"
   },
   "outputs": [],
   "source": [
    "# Define the test dataset\n",
    "test_dataset = MultiOutputTimeSeriesDataset(X_test_combined_multi, y_test_seq_energy, y_test_seq_ports)\n",
    "# Define the test DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dcWI4adQGKnd"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, accuracy_score\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Define the path where the model is saved on Google Drive\n",
    "model_path = '/content/drive/MyDrive/EV_pred/final_multioutput_lstm_model.pth'\n",
    "# Load the saved model's state dictionary\n",
    "model.load_state_dict(torch.load(model_path))\n",
    "\n",
    "model.eval()\n",
    "\n",
    "# Define lists to hold predictions and true labels\n",
    "test_predictions_regression = []\n",
    "test_predictions_classification = []\n",
    "test_true_labels_classification = []\n",
    "\n",
    "# Evaluate the model\n",
    "with torch.no_grad():\n",
    "    for test_inputs, test_labels_regression, test_labels_classification in test_loader:\n",
    "        test_inputs = test_inputs.to(device)\n",
    "        test_labels_regression = test_labels_regression.to(device)\n",
    "        test_labels_classification = test_labels_classification.to(device)\n",
    "\n",
    "        test_outputs_regression, test_outputs_classification = model(test_inputs)\n",
    "\n",
    "        test_predictions_regression.append(test_outputs_regression.cpu().numpy())\n",
    "        test_predictions_classification.append(torch.argmax(test_outputs_classification, dim=1).cpu().numpy())\n",
    "        test_true_labels_classification.append(test_labels_classification.cpu().numpy())\n",
    "\n",
    "# Concatenate predictions and true labels\n",
    "test_predictions_regression = np.concatenate(test_predictions_regression)\n",
    "test_predictions_classification = np.concatenate(test_predictions_classification)\n",
    "test_true_labels_classification = np.concatenate(test_true_labels_classification)\n",
    "\n",
    "# Calculate metrics\n",
    "test_mse = mean_squared_error(y_test_seq_energy.cpu().numpy(), test_predictions_regression)\n",
    "test_mae = mean_absolute_error(y_test_seq_energy.cpu().numpy(), test_predictions_regression)\n",
    "#test_mse = mean_squared_error(y_test_seq_energy.cpu().numpy().ravel(), test_predictions_regression.ravel())\n",
    "#test_mae = mean_absolute_error(y_test_seq_energy.cpu().numpy().ravel(), test_predictions_regression.ravel())\n",
    "test_f1 = f1_score(test_true_labels_classification, test_predictions_classification, average='weighted')\n",
    "test_accuracy = accuracy_score(test_true_labels_classification, test_predictions_classification)\n",
    "\n",
    "# Print the results\n",
    "print(f\"Test MSE for MO_LSTM: {test_mse:.4f}\")\n",
    "print(f\"Test MAE for MO_LSTM: {test_mae:.4f}\")\n",
    "print(f\"Test F1 Score for MO_LSTM: {test_f1:.4f}\")\n",
    "print(f\"Test Accuracy for MO_LSTM: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ThdI5C2uueyc"
   },
   "source": [
    "## **5.5. Model Evaluation (Best Model: Multi-Output GRU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Vx3owfwg9bs"
   },
   "source": [
    "### **5.5.1. Model Evaluation on Test Dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i3FrLDNrnRXa"
   },
   "outputs": [],
   "source": [
    "\"# Load the best hyperparameters\n",
    "#with open('best_multioutput_lstm_params.json', 'r') as f:\n",
    "#    best_params = json.load(f)\n",
    "best_mo_gru_params={\n",
    "    'hidden_size': 56,\n",
    "    'num_layers': 2,\n",
    "    'dropout_prob': 0.31034939184612553,\n",
    "    'learning_rate': 0.002000716164292076,\n",
    "}\n",
    "input_size=1\n",
    "output_size_classification= 7 # Number of classes (0 to 6)\n",
    "\n",
    "\n",
    "# Create the model with the best hyperparameters\n",
    "best_model = MultiOutputGRUModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size= best_mo_gru_params['hidden_size'],\n",
    "    num_layers= best_mo_gru_params['num_layers'],\n",
    "    dropout_prob= best_mo_gru_params['dropout_prob'],\n",
    "    output_size_classification= output_size_classification\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UKAVqLLnqKi7"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, accuracy_score\n",
    "import torch\n",
    "\n",
    "# Define the device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load the model state dictionary with appropriate map location\n",
    "if torch.cuda.is_available():\n",
    "    best_model.load_state_dict(torch.load('/content/drive/MyDrive/EV_pred/final_multioutput_gru_model.pth'))\n",
    "else:\n",
    "    best_model.load_state_dict(torch.load('/content/drive/MyDrive/EV_pred/final_multioutput_gru_model.pth', map_location=torch.device('cpu')))\n",
    "\n",
    "best_model.eval()\n",
    "\n",
    "# Function to make multi-step predictions\n",
    "def make_multistep_predictions(model, initial_input, prediction_horizon):\n",
    "    model.eval()\n",
    "    inputs = initial_input\n",
    "    predictions = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for _ in range(prediction_horizon):\n",
    "            inputs = inputs.to(device)\n",
    "            output_regression, output_classification = model(inputs)\n",
    "            predictions.append(output_regression.cpu().numpy())\n",
    "\n",
    "            # Use the predicted value as the next input\n",
    "            inputs = torch.cat((inputs[:, 1:, :], output_regression.unsqueeze(1)), dim=1)\n",
    "\n",
    "    return np.concatenate(predictions, axis=1)\n",
    "\n",
    "# Function to evaluate the model on multi-step predictions\n",
    "def evaluate_multistep_model(model, test_loader, prediction_horizon):\n",
    "    all_predictions_regression = []\n",
    "    all_true_labels_regression = []\n",
    "    all_predictions_classification = []\n",
    "    all_true_labels_classification = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels_regression, test_labels_classification in test_loader:\n",
    "            test_inputs = test_inputs.to(device)\n",
    "            test_labels_regression = test_labels_regression.to(device)\n",
    "            test_labels_classification = test_labels_classification.to(device)\n",
    "\n",
    "            # Ensure that test_labels_regression is two-dimensional\n",
    "            if test_labels_regression.dim() == 1:\n",
    "                test_labels_regression = test_labels_regression.unsqueeze(1)\n",
    "\n",
    "            predictions_regression = make_multistep_predictions(model, test_inputs, prediction_horizon)\n",
    "            all_predictions_regression.append(predictions_regression)\n",
    "\n",
    "            # Adjust true labels to match the prediction horizon\n",
    "            true_labels_regression = test_labels_regression[:, :prediction_horizon].cpu().numpy()\n",
    "            if true_labels_regression.shape[1] != prediction_horizon:\n",
    "                true_labels_regression = np.tile(true_labels_regression, (1, prediction_horizon // true_labels_regression.shape[1]))\n",
    "\n",
    "            all_true_labels_regression.append(true_labels_regression)\n",
    "\n",
    "            test_outputs_classification = torch.argmax(model(test_inputs)[1], dim=1)\n",
    "            all_predictions_classification.append(test_outputs_classification.cpu().numpy())\n",
    "            all_true_labels_classification.append(test_labels_classification.cpu().numpy())\n",
    "\n",
    "    all_predictions_regression = np.concatenate(all_predictions_regression, axis=0)\n",
    "    all_true_labels_regression = np.concatenate(all_true_labels_regression, axis=0)\n",
    "    all_predictions_classification = np.concatenate(all_predictions_classification, axis=0)\n",
    "    all_true_labels_classification = np.concatenate(all_true_labels_classification, axis=0)\n",
    "\n",
    "    test_mse = mean_squared_error(all_true_labels_regression, all_predictions_regression)\n",
    "    test_mae = mean_absolute_error(all_true_labels_regression, all_predictions_regression)\n",
    "    test_f1 = f1_score(all_true_labels_classification, all_predictions_classification, average='weighted')\n",
    "    test_accuracy = accuracy_score(all_true_labels_classification, all_predictions_classification)\n",
    "\n",
    "    print(f\"Test MSE: {test_mse:.4f}\")\n",
    "    print(f\"Test MAE: {test_mae:.4f}\")\n",
    "    print(f\"Test F1 Score: {test_f1:.4f}\")\n",
    "    print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XmiCuM7VqNea"
   },
   "outputs": [],
   "source": [
    "# Evaluate the day-ahead model (24 hours ahead)\n",
    "print(\"Day-ahead Forecast Evaluation:\")\n",
    "evaluate_multistep_model(best_model, test_loader, prediction_horizon=24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKHBY6kmqPCY"
   },
   "outputs": [],
   "source": [
    "# Evaluate the week-ahead model (168 hours ahead)\n",
    "print(\"Week-ahead Forecast Evaluation:\")\n",
    "evaluate_multistep_model(best_model, test_loader, prediction_horizon=168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gz20jmkSl3GN"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming y_test_dates is a pandas DataFrame with 'Date' as the index\n",
    "y_test_dates.index = pd.to_datetime(y_test_dates.index)  # Convert the index to datetime if it's not already\n",
    "\n",
    "# Calculate error metrics\n",
    "mse = mean_squared_error(y_test_seq_energy.cpu().numpy(), test_predictions_regression)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Set Seaborn style for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set font to DejaVu Serif, similar to Times New Roman\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"DejaVu Serif\"]\n",
    "plt.rcParams[\"font.size\"] = 10  # Default font size\n",
    "\n",
    "# Time Series Plot for Regression Predictions\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(y_test_dates.index, y_test_seq_energy.cpu().numpy(), label='Actual Energy Consumption', color='blue', linewidth=1)\n",
    "plt.plot(y_test_dates.index, test_predictions_regression, label='Forecasted Energy Consumption', color='orange', linestyle='--', linewidth=1), #alpha=0.7)\n",
    "\n",
    "# Title and labels\n",
    "plt.xlabel('Date', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Energy Consumption (kWh)', fontsize=12, fontweight='bold')\n",
    "\n",
    "# Add error metrics as text on the plot\n",
    "plt.text(0.02, 0.95, f'RMSE: {rmse:.2f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Legend\n",
    "plt.legend(fontsize=12, loc='upper right')\n",
    "\n",
    "# Improve grid lines\n",
    "plt.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# High-resolution output\n",
    "#plt.savefig('energy_consumption_comparison.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2ElCc9_bJ2rt"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "# Assuming y_test_dates is a pandas DataFrame with 'Date' as the index\n",
    "y_test_dates_ports.index = pd.to_datetime(y_test_dates_ports.index)  # Convert the index to datetime if it's not already\n",
    "\n",
    "# Time Series Plot for Classification Predictions\n",
    "plt.figure(figsize=(15, 5))  # Adjusted to match the first plot size\n",
    "plt.plot(y_test_dates_ports.index, y_test_seq_ports.cpu().numpy(), label='Actual Available Ports', color='blue', linewidth=1)  # Adjusted linewidth\n",
    "plt.plot(y_test_dates_ports.index, test_predictions_classification, label='Predicted Available Ports', color='orange', linestyle='--', linewidth=1)#, alpha=0.7)  # Adjusted linestyle and linewidth\n",
    "\n",
    "# Title and labels\n",
    "plt.xlabel('Date', fontsize=12, fontweight='bold')  # Match x-axis label style\n",
    "plt.ylabel('Available Ports', fontsize=12, fontweight='bold')  # Match y-axis label style\n",
    "\n",
    "# Add error metrics as text on the plot\n",
    "plt.text(0.02, 0.95, f'F1 Score: {test_f1:.2f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Legend\n",
    "plt.legend(fontsize=12, loc='upper right')  # Match legend style\n",
    "\n",
    "# Improve grid lines\n",
    "plt.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# High-resolution output\n",
    "plt.savefig('available_ports_comparison.png', dpi=300)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J-9saUBsGg06"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming y_test_dates is a pandas DataFrame with 'Date' as the index\n",
    "# Assuming y_test_seq_energy and test_predictions_regression are your test data arrays\n",
    "\n",
    "# Set the random seed\n",
    "random.seed(713)\n",
    "\n",
    "# Set Seaborn style for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set font to DejaVu Serif, similar to Times New Roman\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"DejaVu Serif\"]\n",
    "plt.rcParams[\"font.size\"] = 10  # Default font size\n",
    "\n",
    "# Convert the index to datetime if it's not already\n",
    "y_test_dates.index = pd.to_datetime(y_test_dates.index)\n",
    "\n",
    "# Randomly select a week from the test data\n",
    "selected_week_start = random.choice(y_test_dates.index.to_period('W')).start_time\n",
    "\n",
    "# Filter the test data to include only the selected week\n",
    "selected_week_data = y_test_dates[(y_test_dates.index >= selected_week_start) &\n",
    "                                  (y_test_dates.index < selected_week_start + pd.Timedelta(days=7))]\n",
    "\n",
    "selected_week_predictions = test_predictions_regression[(y_test_dates.index >= selected_week_start) &\n",
    "                                                         (y_test_dates.index < selected_week_start + pd.Timedelta(days=7))]\n",
    "\n",
    "# Calculate RMSE for the selected week\n",
    "#mse = mean_squared_error(selected_week_data['Energy Consumption'].cpu().numpy(), selected_week_predictions)\n",
    "#rmse = mse ** 0.5\n",
    "\n",
    "# Plot the data for the selected week as a line plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(selected_week_data.index, selected_week_data['Energy Consumption'], label='Actual Energy Consumption', color='blue')\n",
    "plt.plot(selected_week_data.index, selected_week_predictions, label='Predicted Energy Consumption', color='orange', linestyle='--')#, alpha=0.7)\n",
    "#plt.title('Energy Consumption: Actual vs Predicted for the Selected Week', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Energy Consumption (kWh)', fontsize=12, fontweight='bold')\n",
    "#plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.legend(fontsize=12, loc='upper right')\n",
    "\n",
    "# Add RMSE as text on the plot\n",
    "#plt.text(0.02, 0.95, f'RMSE: {rmse:.2f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Improve grid lines\n",
    "plt.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# High-resolution output\n",
    "plt.savefig('energy_consumption_selected_week.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqtZhDMC0ydJ"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assuming y_test_dates is a pandas DataFrame with 'Date' as the index\n",
    "# Assuming y_test_seq_energy and test_predictions_regression are your test data arrays\n",
    "\n",
    "# Set the random seed\n",
    "random.seed(1000)\n",
    "\n",
    "# Set Seaborn style for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set font to DejaVu Serif, similar to Times New Roman\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"DejaVu Serif\"]\n",
    "plt.rcParams[\"font.size\"] = 10  # Default font size\n",
    "\n",
    "# Convert the index to datetime if it's not already\n",
    "y_test_dates.index = pd.to_datetime(y_test_dates.index)\n",
    "\n",
    "# Randomly select a week from the test data\n",
    "selected_week_start = random.choice(y_test_dates.index.to_period('W')).start_time\n",
    "\n",
    "# Filter the test data to include only the selected week\n",
    "selected_week_data = y_test_dates[(y_test_dates.index >= selected_week_start) &\n",
    "                                  (y_test_dates.index < selected_week_start + pd.Timedelta(days=1))]\n",
    "\n",
    "selected_week_predictions = test_predictions_regression[(y_test_dates.index >= selected_week_start) &\n",
    "                                                         (y_test_dates.index < selected_week_start + pd.Timedelta(days=1))]\n",
    "\n",
    "# Calculate RMSE for the selected week\n",
    "#mse = mean_squared_error(selected_week_data['Energy Consumption'].cpu().numpy(), selected_week_predictions)\n",
    "#rmse = mse ** 0.5\n",
    "\n",
    "# Plot the data for the selected week as a line plot\n",
    "plt.figure(figsize=(15, 5))\n",
    "plt.plot(selected_week_data.index, selected_week_data['Energy Consumption'], label='Actual Energy Consumption', color='blue')\n",
    "plt.plot(selected_week_data.index, selected_week_predictions, label='Predicted Energy Consumption', color='orange', linestyle='--')#, alpha=0.7)\n",
    "#plt.title('Energy Consumption: Actual vs Predicted for the Selected Week', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Energy Consumption (kWh)', fontsize=12, fontweight='bold')\n",
    "#plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.legend(fontsize=12, loc='upper right')\n",
    "\n",
    "# Add RMSE as text on the plot\n",
    "#plt.text(0.02, 0.95, f'RMSE: {rmse:.2f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Improve grid lines\n",
    "plt.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# High-resolution output\n",
    "#plt.savefig('energy_consumption_selected_week.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r91VJoJy2Iid"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_test_dates is a pandas DataFrame with 'Date' as the index and has a column 'Energy Consumption'\n",
    "# Assuming test_predictions_regression is a NumPy array with predictions\n",
    "\n",
    "# Convert the index to datetime if it's not already\n",
    "y_test_dates.index = pd.to_datetime(y_test_dates.index)\n",
    "\n",
    "# Specify the date and hours for which you want to extract data\n",
    "specific_date = '2020-02-28'\n",
    "specific_hours = ['14:00', '15:00']\n",
    "\n",
    "# Create a datetime range for the specific date and hours\n",
    "specific_datetimes = [pd.to_datetime(f\"{specific_date} {hour}\") for hour in specific_hours]\n",
    "\n",
    "# Extract the actual energy consumption values\n",
    "actual_values = y_test_dates.loc[specific_datetimes, 'Energy Consumption']\n",
    "\n",
    "# Extract the predicted energy consumption values (matching by index position)\n",
    "index_positions = [y_test_dates.index.get_loc(dt) for dt in specific_datetimes]\n",
    "predicted_values = test_predictions_regression[index_positions]\n",
    "\n",
    "# Print the actual and predicted values\n",
    "for dt, actual, predicted in zip(specific_datetimes, actual_values, predicted_values):\n",
    "    print(f\"Date and Time: {dt}\")\n",
    "    print(f\"Actual Energy Consumption: {actual} kWh\")\n",
    "    print(f\"Predicted Energy Consumption: {predicted} kWh\")\n",
    "    print()  # Add a blank line for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5Q_coaRp228B"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Assuming y_test_dates_ports is a pandas DataFrame with 'Date' as the index and has a column 'Available Ports'\n",
    "# Assuming test_predictions_classification is a NumPy array with predictions\n",
    "\n",
    "# Convert the index to datetime if it's not already\n",
    "y_test_dates_ports.index = pd.to_datetime(y_test_dates_ports.index)\n",
    "\n",
    "# Specify the date and hours for which you want to extract data\n",
    "specific_date = '2020-02-28'\n",
    "specific_hours = ['14:00', '15:00']\n",
    "\n",
    "# Create a datetime range for the specific date and hours\n",
    "specific_datetimes = [pd.to_datetime(f\"{specific_date} {hour}\") for hour in specific_hours]\n",
    "\n",
    "# Extract the actual available ports values\n",
    "actual_values = y_test_dates_ports.loc[specific_datetimes, 'Available Ports']\n",
    "\n",
    "# Extract the predicted available ports values (matching by index position)\n",
    "index_positions = [y_test_dates_ports.index.get_loc(dt) for dt in specific_datetimes]\n",
    "predicted_values = test_predictions_classification[index_positions]\n",
    "\n",
    "# Print the actual and predicted values\n",
    "for dt, actual, predicted in zip(specific_datetimes, actual_values, predicted_values):\n",
    "    print(f\"Date and Time: {dt}\")\n",
    "    print(f\"Actual Available Ports: {actual}\")\n",
    "    print(f\"Predicted Available Ports: {predicted}\")\n",
    "    print()  # Add a blank line for better readability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Uj4Lfm8SK7xn"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Assuming y_test_dates_ports is a pandas DataFrame with 'Date' as the index\n",
    "# Assuming y_test_seq_energy and test_predictions_regression are your test data arrays\n",
    "\n",
    "# Set the random seed\n",
    "random.seed(713)\n",
    "\n",
    "# Set Seaborn style for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set font to DejaVu Serif, similar to Times New Roman\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"DejaVu Serif\"]\n",
    "plt.rcParams[\"font.size\"] = 10  # Default font size\n",
    "\n",
    "# Convert the index to datetime if it's not already\n",
    "y_test_dates_ports.index = pd.to_datetime(y_test_dates_ports.index)\n",
    "\n",
    "# Randomly select a week from the test data\n",
    "selected_week_start = random.choice(y_test_dates_ports.index.to_period('W')).start_time\n",
    "\n",
    "# Filter the test data to include only the selected week\n",
    "selected_week_data = y_test_dates_ports[(y_test_dates_ports.index >= selected_week_start) &\n",
    "                                       (y_test_dates_ports.index < selected_week_start + pd.Timedelta(days=7))]\n",
    "\n",
    "selected_week_predictions = test_predictions_classification[(y_test_dates_ports.index >= selected_week_start) &\n",
    "                                                            (y_test_dates_ports.index < selected_week_start + pd.Timedelta(days=7))]\n",
    "\n",
    "# Calculate accuracy score for the selected week\n",
    "accuracy = accuracy_score(selected_week_data['Available Ports'], selected_week_predictions)\n",
    "\n",
    "# Plot the data for the selected week as a line plot\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(selected_week_data.index, selected_week_data['Available Ports'], label='Actual Available Ports', color='blue')\n",
    "plt.plot(selected_week_data.index, selected_week_predictions, label='Predicted Available Ports', color='orange', linestyle='--')#, alpha=0.7)\n",
    "#plt.title('Available Ports: Actual vs Predicted for the Selected Week', fontsize=14, fontweight='bold')\n",
    "plt.xlabel('Date', fontsize=12, fontweight='bold')\n",
    "plt.ylabel('Available Ports', fontsize=12, fontweight='bold')\n",
    "#plt.xticks(rotation=45, ha='right')  # Rotate x-axis labels for better readability\n",
    "plt.legend(fontsize=12, loc='upper right')\n",
    "\n",
    "# Add accuracy score as text on the plot\n",
    "#plt.text(0.02, 0.95, f'Accuracy: {accuracy:.2f}', transform=plt.gca().transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Improve grid lines\n",
    "plt.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# High-resolution output\n",
    "plt.savefig('available_ports_selected_week.png', dpi=300)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0hgM8N-8YA7I"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "\n",
    "# Assuming y_test_dates is a pandas DataFrame with 'Date' as the index\n",
    "# Assuming y_test_seq_energy and test_predictions_regression are your test data arrays\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Set Seaborn style for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set font to DejaVu Serif, similar to Times New Roman\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"DejaVu Serif\"]\n",
    "plt.rcParams[\"font.size\"] = 12  # Adjusted font size for better readability in academic articles\n",
    "\n",
    "# Convert the index to datetime if it's not already\n",
    "y_test_dates.index = pd.to_datetime(y_test_dates.index)\n",
    "\n",
    "# Calculate overall RMSE for the entire test set\n",
    "mse = mean_squared_error(y_test_seq_energy.cpu().numpy(), test_predictions_regression)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Plotting the figure\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=False)\n",
    "\n",
    "# Plot 1: Overall Time Series Plot\n",
    "ax1.plot(y_test_dates.index, y_test_seq_energy.cpu().numpy(), label='Actual Energy Consumption', color='blue', linewidth=1)\n",
    "ax1.plot(y_test_dates.index, test_predictions_regression, label='Predicted Energy Consumption', color='orange', linestyle='--', linewidth=1)\n",
    "ax1.set_ylabel('Energy Consumption (kWh)', fontsize=12)\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_title('Energy Consumption: Actual vs Predicted', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='upper right')\n",
    "ax1.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add RMSE as text on the plot\n",
    "ax1.text(0.02, 0.95, f'Overall RMSE: {rmse:.2f}', transform=ax1.transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Plot 2: Selected Week Plot\n",
    "# Randomly select a week from the test data\n",
    "selected_week_start = random.choice(y_test_dates.index.to_period('W')).start_time\n",
    "selected_week_data = y_test_dates[(y_test_dates.index >= selected_week_start) &\n",
    "                                  (y_test_dates.index < selected_week_start + pd.Timedelta(days=7))]\n",
    "selected_week_predictions = test_predictions_regression[(y_test_dates.index >= selected_week_start) &\n",
    "                                                         (y_test_dates.index < selected_week_start + pd.Timedelta(days=7))]\n",
    "\n",
    "# Calculate RMSE for the selected week\n",
    "mse_selected = mean_squared_error(selected_week_data['Energy Consumption'], selected_week_predictions)\n",
    "rmse_selected = mse_selected ** 0.5\n",
    "\n",
    "ax2.plot(selected_week_data.index, selected_week_data['Energy Consumption'], label='Actual Energy Consumption', color='blue')\n",
    "ax2.plot(selected_week_data.index, selected_week_predictions, label='Predicted Energy Consumption', color='orange', linestyle='--')\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Energy Consumption (kWh)', fontsize=12)\n",
    "ax2.set_title('Energy Consumption: Actual vs Predicted for a Randomly Selected Week', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10, loc='upper right')\n",
    "\n",
    "# Add RMSE for the selected week as text on the plot\n",
    "ax2.text(0.02, 0.95, f'RMSE for a Randomly Selected Week: {rmse_selected:.2f}', transform=ax2.transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save or display the plot\n",
    "plt.savefig('combined_energy_consumption_plots.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NsRwBSwgtrkj"
   },
   "outputs": [],
   "source": [
    "df_bry_sliced_from_20200317_to_20200630_dates= df_bry_sliced_from_20200317_to_20200630[[\"Date\", \"Energy Consumption\"]]\n",
    "df_bry_sliced_from_20200317_to_20200630_dates= df_bry_sliced_from_20200317_to_20200630_dates.set_index(\"Date\")\n",
    "\n",
    "df_bry_covid_partial_reopenings_range_dates= df_bry_covid_partial_reopenings_range[[\"Date\", \"Energy Consumption\"]]\n",
    "df_bry_covid_partial_reopenings_range_dates= df_bry_covid_partial_reopenings_range_dates.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_E5R1W6u5uKs"
   },
   "outputs": [],
   "source": [
    "# Drop the first 168 rows from both dataframes\n",
    "df_bry_sliced_from_20200317_to_20200630_dates = df_bry_sliced_from_20200317_to_20200630_dates.iloc[168:]\n",
    "df_bry_covid_partial_reopenings_range_dates = df_bry_covid_partial_reopenings_range_dates.iloc[168:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "62KyBrnL_0lU"
   },
   "outputs": [],
   "source": [
    "pred_cov= pd.DataFrame(test_predictions_regression_Bry_Covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9_AutjrEATcL"
   },
   "outputs": [],
   "source": [
    "# Assuming df_actual is the actual data and df_pred is the predicted data\n",
    "# Convert actual data's index to sequential index (if needed)\n",
    "df_actual = df_bry_sliced_from_20200317_to_20200630_dates.reset_index()\n",
    "\n",
    "# Add the predicted values as a column\n",
    "df_actual['Predicted Values'] = pred_cov.values\n",
    "\n",
    "# (Optional) Set the index back to the DateTime index if needed\n",
    "df_actual.set_index('Date', inplace=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(df_actual)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PN-fBj2xDiua"
   },
   "outputs": [],
   "source": [
    "df_lockdown= df_actual.loc[\"2020-04-10 00:00:00\": \"2020-04-18 00:00:00\"]\n",
    "df_reopenning= df_actual.loc[\"2020-06-10 00:00:00\": \"2020-06-18 00:00:00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cOWZgHUjAEkh"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "\n",
    "# Assuming y_test_dates is a pandas DataFrame with 'Date' as the index\n",
    "# Assuming y_test_seq_energy and test_predictions_regression are your test data arrays\n",
    "\n",
    "# Set Seaborn style for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set font to DejaVu Serif, similar to Times New Roman\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"DejaVu Serif\"]\n",
    "plt.rcParams[\"font.size\"] = 12  # Adjusted font size for better readability in academic articles\n",
    "\n",
    "# Convert the index to datetime if it's not already\n",
    "df_lockdown.index = pd.to_datetime(df_lockdown.index)\n",
    "\n",
    "# Calculate overall RMSE for the entire test set\n",
    "mse = mean_squared_error(df_lockdown[\"Energy Consumption\"], df_lockdown[\"Predicted Values\"])\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Plotting the figure\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=False)\n",
    "\n",
    "# Plot 1: Overall Time Series Plot\n",
    "ax1.plot(df_lockdown.index, df_lockdown[\"Energy Consumption\"], label='Actual Energy Consumption', color='blue', linewidth=1)\n",
    "ax1.plot(df_lockdown.index, df_lockdown[\"Predicted Values\"], label='Predicted Energy Consumption', color='orange', linestyle='--', linewidth=1)\n",
    "ax1.set_ylabel('Energy Consumption (kWh)', fontsize=12)\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_title('Energy Consumption: Actual vs Predicted - Random Week of Lockdown Periods', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='upper right')\n",
    "ax1.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add RMSE as text on the plot\n",
    "ax1.text(0.02, 0.95, f'RMSE for a Random Week of Lockdown Periods: {rmse:.2f}', transform=ax1.transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Plot 2: Selected Week Plot\n",
    "# Calculate RMSE for the selected week\n",
    "mse = mean_squared_error(df_reopenning[\"Energy Consumption\"], df_reopenning[\"Predicted Values\"])\n",
    "rmse_selected = mse_selected ** 0.5\n",
    "\n",
    "ax2.plot(df_reopenning.index, df_reopenning[\"Energy Consumption\"], label='Actual Energy Consumption', color='blue')\n",
    "ax2.plot(df_reopenning.index, df_reopenning[\"Predicted Values\"], label='Predicted Energy Consumption', color='orange', linestyle='--')\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Energy Consumption (kWh)', fontsize=12)\n",
    "ax2.set_title('Energy Consumption: Actual vs Predicted - Random Week of Partial Reopening Periods', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10, loc='upper right')\n",
    "\n",
    "# Add RMSE for the selected week as text on the plot\n",
    "ax2.text(0.02, 0.95, f'RMSE for a Random Week of Partial Reopening Periods: {rmse_selected:.2f}', transform=ax2.transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save or display the plot\n",
    "plt.savefig('covid_consumption.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Lt877gTHF0cV"
   },
   "outputs": [],
   "source": [
    "df_bry_sliced_from_20200317_to_20200630_dates_ports= df_bry_sliced_from_20200317_to_20200630[[\"Date\", \"Available Ports\"]]\n",
    "df_bry_sliced_from_20200317_to_20200630_dates_ports= df_bry_sliced_from_20200317_to_20200630_dates_ports.set_index(\"Date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Jvi_2_u0F9Zm"
   },
   "outputs": [],
   "source": [
    "# Drop the first 168 rows from both dataframes\n",
    "df_bry_sliced_from_20200317_to_20200630_dates_ports = df_bry_sliced_from_20200317_to_20200630_dates_ports.iloc[168:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ieJvuJCeF9Zm"
   },
   "outputs": [],
   "source": [
    "pred_cov_port= pd.DataFrame(test_predictions_classification_Bry_Covid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Pe0Ww2JF9Zn"
   },
   "outputs": [],
   "source": [
    "# Assuming df_actual is the actual data and df_pred is the predicted data\n",
    "# Convert actual data's index to sequential index (if needed)\n",
    "df_actual_ports = df_bry_sliced_from_20200317_to_20200630_dates_ports.reset_index()\n",
    "\n",
    "# Add the predicted values as a column\n",
    "df_actual_ports['Predicted Values'] = pred_cov_port.values\n",
    "\n",
    "# (Optional) Set the index back to the DateTime index if needed\n",
    "df_actual_ports.set_index('Date', inplace=True)\n",
    "\n",
    "# Display the combined DataFrame\n",
    "print(df_actual_ports)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UweUpHmDF9Zn"
   },
   "outputs": [],
   "source": [
    "df_lockdown_ports= df_actual_ports.loc[\"2020-04-10 00:00:00\": \"2020-04-18 00:00:00\"]\n",
    "df_reopenning_ports= df_actual_ports.loc[\"2020-06-10 00:00:00\": \"2020-06-18 00:00:00\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UJOsNHwoHeMp"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Set Seaborn style for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set font to DejaVu Serif, similar to Times New Roman\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"DejaVu Serif\"]\n",
    "plt.rcParams[\"font.size\"] = 12  # Adjusted font size for better readability\n",
    "\n",
    "# Calculate accuracy score and F1 score for the selected week\n",
    "selected_week_data = df_lockdown_ports[\"Available Ports\"]\n",
    "selected_week_predictions = df_lockdown_ports[\"Predicted Values\"]\n",
    "accuracy = test_f1\n",
    "f1 = f1_score(selected_week_data, selected_week_predictions, average='weighted')  # Change 'binary' if multiclass\n",
    "\n",
    "# Plotting the figure with subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=False)\n",
    "\n",
    "# Plot 1: Time Series Plot for Classification Predictions\n",
    "ax1.plot(df_lockdown_ports.index, df_lockdown_ports[\"Available Ports\"], label='Actual Available Ports', color='blue', linewidth=1)\n",
    "ax1.plot(df_lockdown_ports.index, df_lockdown_ports[\"Predicted Values\"], label='Predicted Available Ports', color='orange', linestyle='--', linewidth=1)\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_ylabel('Available Ports', fontsize=12)\n",
    "ax1.set_title('Available Ports: Actual vs Predicted - Random Week of Lockdown Periods', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='lower right')\n",
    "ax1.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add accuracy score as text on the plot\n",
    "ax1.text(\n",
    "    0.02, 0.05,  # Coordinates for the lower-left position in Axes-relative coordinates\n",
    "    f'F1 Score for a Random Week of Lockdown Periods: {accuracy:.2f}',\n",
    "    transform=ax1.transAxes,  # Ensure coordinates are relative to the Axes\n",
    "    fontsize=12,\n",
    "    verticalalignment='bottom',  # Align text vertically at the bottom\n",
    "    bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white')  # Add a box around the text\n",
    ")\n",
    "\n",
    "# Plot 2: Selected Week Plot\n",
    "ax2.plot(df_reopenning_ports.index, df_reopenning_ports['Available Ports'], label='Actual Available Ports', color='blue')\n",
    "ax2.plot(df_reopenning_ports.index, df_reopenning_ports[\"Predicted Values\"], label='Predicted Available Ports', color='orange', linestyle='--')\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Available Ports', fontsize=12)\n",
    "ax2.set_title('Available Ports: Actual vs Predicted - Random Week of Partial Reopening Periods', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10, loc='lower right')\n",
    "ax2.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Set y-axis range and ticks\n",
    "ax2.set_ylim(0, 6)  # Set limits from 0 to 6\n",
    "ax2.set_yticks(range(0, 7))  # Create integer ticks from 0 to 6\n",
    "\n",
    "# Add F1 score as text on the plot for the selected week\n",
    "ax2.text(\n",
    "    0.02, 0.05,  # Coordinates for the lower-left position in Axes-relative coordinates\n",
    "    f'F1 Score for a Random Week of Partial Reopening Periods: {f1:.2f}',\n",
    "    transform=ax2.transAxes,  # Ensure coordinates are relative to the Axes\n",
    "    fontsize=12,\n",
    "    verticalalignment='bottom',  # Align text vertically at the bottom\n",
    "    bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white')  # Add a box around the text\n",
    ")\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save or display the plot\n",
    "plt.savefig('covid_ports.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IV8_rMpJFmo8"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Assuming y_test_dates_ports and y_test_seq_ports are pandas DataFrame and tensor/array respectively\n",
    "# Assuming test_predictions_classification is your classification predictions array\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Set Seaborn style for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set font to DejaVu Serif, similar to Times New Roman\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"DejaVu Serif\"]\n",
    "plt.rcParams[\"font.size\"] = 12  # Adjusted font size for better readability\n",
    "\n",
    "# Convert the index to datetime if it's not already\n",
    "y_test_dates_ports.index = pd.to_datetime(y_test_dates_ports.index)\n",
    "\n",
    "# Calculate accuracy score and F1 score for the selected week\n",
    "selected_week_start = random.choice(y_test_dates_ports.index.to_period('W')).start_time\n",
    "selected_week_data = y_test_dates_ports[(y_test_dates_ports.index >= selected_week_start) &\n",
    "                                        (y_test_dates_ports.index < selected_week_start + pd.Timedelta(days=7))]\n",
    "selected_week_predictions = test_predictions_classification[(y_test_dates_ports.index >= selected_week_start) &\n",
    "                                                            (y_test_dates_ports.index < selected_week_start + pd.Timedelta(days=7))]\n",
    "accuracy = test_f1\n",
    "f1 = f1_score(selected_week_data['Available Ports'], selected_week_predictions, average='weighted')  # Change 'binary' if multiclass\n",
    "\n",
    "# Plotting the figure with subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=False)\n",
    "\n",
    "# Plot 1: Time Series Plot for Classification Predictions\n",
    "ax1.plot(y_test_dates_ports.index, y_test_seq_ports.cpu().numpy(), label='Actual Available Ports', color='blue', linewidth=1)\n",
    "ax1.plot(y_test_dates_ports.index, test_predictions_classification, label='Predicted Available Ports', color='orange', linestyle='--', linewidth=1)\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_ylabel('Available Ports', fontsize=12)\n",
    "ax1.set_title('Available Ports: Actual vs Predicted', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='upper right')\n",
    "ax1.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add accuracy score as text on the plot\n",
    "ax1.text(0.02, 0.95, f'Overall F1 Score: {accuracy:.2f}', transform=ax1.transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Plot 2: Selected Week Plot\n",
    "ax2.plot(selected_week_data.index, selected_week_data['Available Ports'], label='Actual Available Ports', color='blue')\n",
    "ax2.plot(selected_week_data.index, selected_week_predictions, label='Predicted Available Ports', color='orange', linestyle='--')\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Available Ports', fontsize=12)\n",
    "ax2.set_title('Available Ports: Actual vs Predicted for a Randomly Selected Week', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10, loc='upper right')\n",
    "ax2.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add F1 score as text on the plot for the selected week\n",
    "ax2.text(0.02, 0.95, f'F1 Score for a Randomly Selected Week: {f1:.2f}', transform=ax2.transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save or display the plot\n",
    "plt.savefig('combined_available_ports_plots.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "w4b9_qMjtsLn"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import random\n",
    "\n",
    "# Assuming y_test_dates is a pandas DataFrame with 'Date' as the index\n",
    "# Assuming y_test_seq_energy and test_predictions_regression are your test data arrays\n",
    "\n",
    "# Set Seaborn style for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set font to DejaVu Serif, similar to Times New Roman\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"DejaVu Serif\"]\n",
    "plt.rcParams[\"font.size\"] = 12  # Adjusted font size for better readability in academic articles\n",
    "\n",
    "# Convert the index to datetime if it's not already\n",
    "df_bry_sliced_from_20200317_to_20200630_dates.index = pd.to_datetime(df_bry_sliced_from_20200317_to_20200630_dates.index)\n",
    "\n",
    "# Calculate overall RMSE for the entire test set\n",
    "mse = mean_squared_error(y_test_energy_df_bry_sliced_from_20200317_to_20200630.cpu().numpy(), test_predictions_regression_Bry_Covid)\n",
    "rmse = mse ** 0.5\n",
    "\n",
    "# Plotting the figure\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=False)\n",
    "\n",
    "# Plot 1: Overall Time Series Plot\n",
    "ax1.plot(df_bry_sliced_from_20200317_to_20200630_dates.index, y_test_energy_df_bry_sliced_from_20200317_to_20200630.cpu().numpy(), label='Actual Energy Consumption', color='blue', linewidth=1)\n",
    "ax1.plot(df_bry_sliced_from_20200317_to_20200630_dates.index, test_predictions_regression_Bry_Covid, label='Predicted Energy Consumption', color='orange', linestyle='--', linewidth=1)\n",
    "ax1.set_ylabel('Energy Consumption (kWh)', fontsize=12)\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_title('Energy Consumption: Actual vs Predicted', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='upper right')\n",
    "ax1.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add RMSE as text on the plot\n",
    "ax1.text(0.02, 0.95, f'Overall RMSE: {rmse:.2f}', transform=ax1.transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Plot 2: Selected Week Plot\n",
    "# Calculate RMSE for the selected week\n",
    "mse_selected = mean_squared_error(y_test_energy_bry_covid_partial_reopenings_range, test_predictions_regression_Bry_Covid_Partial_Reopenings_Range)\n",
    "rmse_selected = mse_selected ** 0.5\n",
    "\n",
    "ax2.plot(df_bry_covid_partial_reopenings_range_dates.index, y_test_energy_bry_covid_partial_reopenings_range.cpu().numpy(), label='Actual Energy Consumption', color='blue')\n",
    "ax2.plot(df_bry_covid_partial_reopenings_range_dates.index, test_predictions_regression_Bry_Covid_Partial_Reopenings_Range, label='Predicted Energy Consumption', color='orange', linestyle='--')\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Energy Consumption (kWh)', fontsize=12)\n",
    "ax2.set_title('Energy Consumption: Actual vs Predicted for a Randomly Selected Week', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10, loc='upper right')\n",
    "\n",
    "# Add RMSE for the selected week as text on the plot\n",
    "ax2.text(0.02, 0.95, f'RMSE for a Randomly Selected Week: {rmse_selected:.2f}', transform=ax2.transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save or display the plot\n",
    "#plt.savefig('combined_energy_consumption_plots.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Rr7OinK1cB9U"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Assuming y_test_dates_ports and y_test_seq_ports are pandas DataFrame and tensor/array respectively\n",
    "# Assuming test_predictions_classification is your classification predictions array\n",
    "\n",
    "# Set the random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Set Seaborn style for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set font to DejaVu Serif, similar to Times New Roman\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"DejaVu Serif\"]\n",
    "plt.rcParams[\"font.size\"] = 12  # Adjusted font size for better readability\n",
    "\n",
    "# Convert the index to datetime if it's not already\n",
    "y_test_dates_ports.index = pd.to_datetime(y_test_dates_ports.index)\n",
    "\n",
    "# Calculate accuracy score and F1 score for the selected week\n",
    "selected_week_start = random.choice(y_test_dates_ports.index.to_period('W')).start_time\n",
    "selected_week_data = y_test_dates_ports[(y_test_dates_ports.index >= selected_week_start) &\n",
    "                                        (y_test_dates_ports.index < selected_week_start + pd.Timedelta(days=7))]\n",
    "selected_week_predictions = test_predictions_classification[(y_test_dates_ports.index >= selected_week_start) &\n",
    "                                                            (y_test_dates_ports.index < selected_week_start + pd.Timedelta(days=7))]\n",
    "accuracy = test_f1\n",
    "f1 = f1_score(selected_week_data['Available Ports'], selected_week_predictions, average='weighted')  # Change 'binary' if multiclass\n",
    "\n",
    "# Plotting the figure with subplots\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(15, 10), sharex=False)\n",
    "\n",
    "# Plot 1: Time Series Plot for Classification Predictions\n",
    "ax1.plot(y_test_dates_ports.index, y_test_seq_ports.cpu().numpy(), label='Actual Available Ports', color='blue', linewidth=1)\n",
    "ax1.plot(y_test_dates_ports.index, test_predictions_classification, label='Predicted Available Ports', color='orange', linestyle='--', linewidth=1)\n",
    "ax1.set_xlabel('Date', fontsize=12)\n",
    "ax1.set_ylabel('Available Ports', fontsize=12)\n",
    "ax1.set_title('Available Ports: Actual vs Predicted', fontsize=12, fontweight='bold')\n",
    "ax1.legend(fontsize=10, loc='upper right')\n",
    "ax1.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add accuracy score as text on the plot\n",
    "ax1.text(0.02, 0.95, f'Overall F1 Score: {accuracy:.2f}', transform=ax1.transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Plot 2: Selected Week Plot\n",
    "ax2.plot(selected_week_data.index, selected_week_data['Available Ports'], label='Actual Available Ports', color='blue')\n",
    "ax2.plot(selected_week_data.index, selected_week_predictions, label='Predicted Available Ports', color='orange', linestyle='--')\n",
    "ax2.set_xlabel('Date', fontsize=12)\n",
    "ax2.set_ylabel('Available Ports', fontsize=12)\n",
    "ax2.set_title('Available Ports: Actual vs Predicted for a Randomly Selected Week', fontsize=12, fontweight='bold')\n",
    "ax2.legend(fontsize=10, loc='upper right')\n",
    "ax2.grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "# Add F1 score as text on the plot for the selected week\n",
    "ax2.text(0.02, 0.95, f'F1 Score for a Randomly Selected Week: {f1:.2f}', transform=ax2.transAxes, fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save or display the plot\n",
    "plt.savefig('combined_available_ports_plots.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CAKDr0FulDFU"
   },
   "source": [
    "### **5.5.2. Evaluating Model's Performance on Test Stations**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fzKc1nPhnASa"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, accuracy_score\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def evaluate_model_on_station(model, X_test, y_test_energy, y_test_ports, batch_size=16, device='cpu'):\n",
    "    # Create the test dataset and DataLoader\n",
    "    test_dataset = MultiOutputTimeSeriesDataset(X_test, y_test_energy, y_test_ports)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize lists to hold predictions and true labels\n",
    "    test_predictions_regression = []\n",
    "    test_predictions_classification = []\n",
    "    test_true_labels_classification = []\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels_regression, test_labels_classification in test_loader:\n",
    "            # Move inputs and labels to the specified device\n",
    "            test_inputs = test_inputs.to(device)\n",
    "            test_labels_regression = test_labels_regression.to(device)\n",
    "            test_labels_classification = test_labels_classification.to(device)\n",
    "\n",
    "            # Perform predictions\n",
    "            test_outputs_regression, test_outputs_classification = model(test_inputs)\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            test_predictions_regression.append(test_outputs_regression.cpu().numpy())\n",
    "            test_predictions_classification.append(torch.argmax(test_outputs_classification, dim=1).cpu().numpy())\n",
    "            test_true_labels_classification.append(test_labels_classification.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    test_predictions_regression = np.concatenate(test_predictions_regression)\n",
    "    test_predictions_classification = np.concatenate(test_predictions_classification)\n",
    "    test_true_labels_classification = np.concatenate(test_true_labels_classification)\n",
    "\n",
    "    # Calculate metrics\n",
    "    test_mse = mean_squared_error(y_test_energy.cpu().numpy(), test_predictions_regression)\n",
    "    test_mae = mean_absolute_error(y_test_energy.cpu().numpy(), test_predictions_regression)\n",
    "    test_f1 = f1_score(test_true_labels_classification, test_predictions_classification, average='weighted')\n",
    "    test_accuracy = accuracy_score(test_true_labels_classification, test_predictions_classification)\n",
    "\n",
    "    # Return the evaluation metrics as a dictionary\n",
    "    return {\n",
    "        'mse': test_mse,\n",
    "        'mae': test_mae,\n",
    "        'f1_score': test_f1,\n",
    "        'accuracy': test_accuracy\n",
    "    }\n",
    "\n",
    "# Load your model (if not already loaded)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "input_size = 1\n",
    "hidden_size = 35\n",
    "num_layers = 2\n",
    "dropout_prob = 0.1245094994892297\n",
    "output_size_classification = 7\n",
    "\n",
    "best_model = MultiOutputLSTMModel(input_size, hidden_size, num_layers, dropout_prob, output_size_classification).to(device)\n",
    "best_model.load_state_dict(torch.load('/content/drive/MyDrive/final_multioutput_lstm_model.pth', map_location=device))\n",
    "\n",
    "# Evaluate on each station and print results\n",
    "for station, (X_test, y_test_energy, y_test_ports) in {\n",
    "    'Ham_Sliced_to_20200229': (X_test_ham_sliced_to_20200229, y_test_energy_ham_sliced_to_20200229, y_test_ports_ham_sliced_to_20200229),\n",
    "    'Cam_Sliced_to_20200229': (X_test_cam_sliced_to_20200229, y_test_energy_cam_sliced_to_20200229, y_test_ports_cam_sliced_to_20200229),\n",
    "    'Rin_Sliced_to_20200229': (X_test_rin_sliced_to_20200229, y_test_energy_rin_sliced_to_20200229, y_test_ports_rin_sliced_to_20200229),\n",
    "    'Bry_Sliced_to_20200229': (X_test_bry_sliced_to_20200229, y_test_energy_bry_sliced_to_20200229, y_test_ports_bry_sliced_to_20200229),\n",
    "\n",
    "    'Ham_Covid_Early_Lockdown_Range': (X_test_ham_covid_early_lockdown_range, y_test_energy_ham_covid_early_lockdown_range, y_test_ports_ham_covid_early_lockdown_range),\n",
    "    'Cam_Covid_Early_Lockdown_Range': (X_test_cam_covid_early_lockdown_range, y_test_energy_cam_covid_early_lockdown_range, y_test_ports_cam_covid_early_lockdown_range),\n",
    "    'Rin_Covid_Early_Lockdown_Range': (X_test_rin_covid_early_lockdown_range, y_test_energy_rin_covid_early_lockdown_range, y_test_ports_rin_covid_early_lockdown_range),\n",
    "    'Bry_Covid_Early_Lockdown_Range': (X_test_bry_covid_early_lockdown_range, y_test_energy_bry_covid_early_lockdown_range, y_test_ports_bry_covid_early_lockdown_range),\n",
    "\n",
    "    'Ham_Covid_Partial_Reopenings_Range': (X_test_ham_covid_partial_reopenings_range, y_test_energy_ham_covid_partial_reopenings_range, y_test_ports_ham_covid_partial_reopenings_range),\n",
    "    'Cam_Covid_Partial_Reopenings_Range': (X_test_cam_covid_partial_reopenings_range, y_test_energy_cam_covid_partial_reopenings_range, y_test_ports_cam_covid_partial_reopenings_range),\n",
    "    'Rin_Covid_Partial_Reopenings_Range': (X_test_rin_covid_partial_reopenings_range, y_test_energy_rin_covid_partial_reopenings_range, y_test_ports_rin_covid_partial_reopenings_range),\n",
    "    'Bry_Covid_Partial_Reopenings_Range': (X_test_bry_covid_partial_reopenings_range, y_test_energy_bry_covid_partial_reopenings_range, y_test_ports_bry_covid_partial_reopenings_range),\n",
    "    'Bry_Covid':(X_test_df_bry_sliced_from_20200317_to_20200630, y_test_energy_df_bry_sliced_from_20200317_to_20200630, y_test_ports_df_bry_sliced_from_20200317_to_20200630)\n",
    "}.items():\n",
    "    metrics = evaluate_model_on_station(best_model, X_test, y_test_energy, y_test_ports, batch_size=16, device=device)\n",
    "    print(f\"Results for {station} Station:\")\n",
    "    print(f\"  Test MSE: {metrics['mse']:.4f}\")\n",
    "    print(f\"  Test MAE: {metrics['mae']:.4f}\")\n",
    "    print(f\"  Test F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Test Accuracy: {metrics['accuracy']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tZvHpdmerPUW"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_on_station(model, X_test, y_test_energy, y_test_ports, batch_size=32, device='cpu'):\n",
    "    # Create the test dataset and DataLoader\n",
    "    test_dataset = MultiOutputTimeSeriesDataset(X_test, y_test_energy, y_test_ports)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize lists to hold predictions and true labels\n",
    "    test_predictions_regression = []\n",
    "    test_predictions_classification = []\n",
    "    test_true_labels_classification = []\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels_regression, test_labels_classification in test_loader:\n",
    "            # Move inputs and labels to the specified device\n",
    "            test_inputs = test_inputs.to(device)\n",
    "            test_labels_regression = test_labels_regression.to(device)\n",
    "            test_labels_classification = test_labels_classification.to(device)\n",
    "\n",
    "            # Perform predictions\n",
    "            test_outputs_regression, test_outputs_classification = model(test_inputs)\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            test_predictions_regression.append(test_outputs_regression.cpu().numpy())\n",
    "            test_predictions_classification.append(torch.argmax(test_outputs_classification, dim=1).cpu().numpy())\n",
    "            test_true_labels_classification.append(test_labels_classification.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    test_predictions_regression = np.concatenate(test_predictions_regression)\n",
    "    test_predictions_classification = np.concatenate(test_predictions_classification)\n",
    "    test_true_labels_classification = np.concatenate(test_true_labels_classification)\n",
    "\n",
    "    # Calculate metrics\n",
    "    test_mse = mean_squared_error(y_test_energy.cpu().numpy(), test_predictions_regression)\n",
    "    test_mae = mean_absolute_error(y_test_energy.cpu().numpy(), test_predictions_regression)\n",
    "    test_f1 = f1_score(test_true_labels_classification, test_predictions_classification, average='weighted')\n",
    "    test_accuracy = accuracy_score(test_true_labels_classification, test_predictions_classification)\n",
    "\n",
    "    # Return the evaluation metrics as a dictionary\n",
    "    return {\n",
    "        'mse': test_mse,\n",
    "        'mae': test_mae,\n",
    "        'f1_score': test_f1,\n",
    "        'accuracy': test_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "best_mo_gru_params={\n",
    "    'hidden_size': 56,\n",
    "    'num_layers': 2,\n",
    "    'dropout_prob': 0.31034939184612553,\n",
    "    'learning_rate': 0.002000716164292076,\n",
    "}\n",
    "input_size=1\n",
    "output_size_classification= 7 # Number of classes (0 to 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create the model with the best hyperparameters\n",
    "model = MultiOutputGRUModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size= best_mo_gru_params['hidden_size'],\n",
    "    num_layers= best_mo_gru_params['num_layers'],\n",
    "    dropout_prob= best_mo_gru_params['dropout_prob'],\n",
    "    output_size_classification= output_size_classification\n",
    ").to(device)\n",
    "\n",
    "\n",
    "model_path = '/content/drive/MyDrive/EV_pred/final_multioutput_gru_model.pth'\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/EV_pred/final_multioutput_gru_model.pth', map_location=device))\n",
    "\n",
    "# Evaluate on each station and print results\n",
    "for station, (X_test, y_test_energy, y_test_ports) in {\n",
    "    'Ham_Sliced_to_20200229': (X_test_ham_sliced_to_20200229, y_test_energy_ham_sliced_to_20200229, y_test_ports_ham_sliced_to_20200229),\n",
    "    'Cam_Sliced_to_20200229': (X_test_cam_sliced_to_20200229, y_test_energy_cam_sliced_to_20200229, y_test_ports_cam_sliced_to_20200229),\n",
    "    'Rin_Sliced_to_20200229': (X_test_rin_sliced_to_20200229, y_test_energy_rin_sliced_to_20200229, y_test_ports_rin_sliced_to_20200229),\n",
    "    'Bry_Sliced_to_20200229': (X_test_bry_sliced_to_20200229, y_test_energy_bry_sliced_to_20200229, y_test_ports_bry_sliced_to_20200229),\n",
    "\n",
    "    'Ham_Covid_Early_Lockdown_Range': (X_test_ham_covid_early_lockdown_range, y_test_energy_ham_covid_early_lockdown_range, y_test_ports_ham_covid_early_lockdown_range),\n",
    "    'Cam_Covid_Early_Lockdown_Range': (X_test_cam_covid_early_lockdown_range, y_test_energy_cam_covid_early_lockdown_range, y_test_ports_cam_covid_early_lockdown_range),\n",
    "    'Rin_Covid_Early_Lockdown_Range': (X_test_rin_covid_early_lockdown_range, y_test_energy_rin_covid_early_lockdown_range, y_test_ports_rin_covid_early_lockdown_range),\n",
    "    'Bry_Covid_Early_Lockdown_Range': (X_test_bry_covid_early_lockdown_range, y_test_energy_bry_covid_early_lockdown_range, y_test_ports_bry_covid_early_lockdown_range),\n",
    "\n",
    "    'Ham_Covid_Partial_Reopenings_Range': (X_test_ham_covid_partial_reopenings_range, y_test_energy_ham_covid_partial_reopenings_range, y_test_ports_ham_covid_partial_reopenings_range),\n",
    "    'Cam_Covid_Partial_Reopenings_Range': (X_test_cam_covid_partial_reopenings_range, y_test_energy_cam_covid_partial_reopenings_range, y_test_ports_cam_covid_partial_reopenings_range),\n",
    "    'Rin_Covid_Partial_Reopenings_Range': (X_test_rin_covid_partial_reopenings_range, y_test_energy_rin_covid_partial_reopenings_range, y_test_ports_rin_covid_partial_reopenings_range),\n",
    "    'Bry_Covid_Partial_Reopenings_Range': (X_test_bry_covid_partial_reopenings_range, y_test_energy_bry_covid_partial_reopenings_range, y_test_ports_bry_covid_partial_reopenings_range),\n",
    "    'Bry_Covid':(X_test_df_bry_sliced_from_20200317_to_20200630, y_test_energy_df_bry_sliced_from_20200317_to_20200630, y_test_ports_df_bry_sliced_from_20200317_to_20200630)\n",
    "}.items():\n",
    "    metrics = evaluate_model_on_station(model, X_test, y_test_energy, y_test_ports, batch_size=32, device=device)\n",
    "    print(f\"Results for {station} Station:\")\n",
    "    print(f\"  Test MSE: {metrics['mse']:.4f}\")\n",
    "    print(f\"  Test MAE: {metrics['mae']:.4f}\")\n",
    "    print(f\"  Test F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Test Accuracy: {metrics['accuracy']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZuN0gBJkyppO"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, f1_score, accuracy_score\n",
    "\n",
    "def evaluate_model_on_station(model, X_test, y_test_energy, y_test_ports, batch_size=32, device='cpu'):\n",
    "    # Create the test dataset and DataLoader\n",
    "    test_dataset = MultiOutputTimeSeriesDataset(X_test, y_test_energy, y_test_ports)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize lists to hold predictions and true labels\n",
    "    test_predictions_regression = []\n",
    "    test_predictions_classification = []\n",
    "    test_true_labels_classification = []\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels_regression, test_labels_classification in test_loader:\n",
    "            # Move inputs and labels to the specified device\n",
    "            test_inputs = test_inputs.to(device)\n",
    "            test_labels_regression = test_labels_regression.to(device)\n",
    "            test_labels_classification = test_labels_classification.to(device)\n",
    "\n",
    "            # Perform predictions\n",
    "            test_outputs_regression, test_outputs_classification = model(test_inputs)\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            test_predictions_regression.append(test_outputs_regression.cpu().numpy())\n",
    "            test_predictions_classification.append(torch.argmax(test_outputs_classification, dim=1).cpu().numpy())\n",
    "            test_true_labels_classification.append(test_labels_classification.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    test_predictions_regression = np.concatenate(test_predictions_regression)\n",
    "    test_predictions_classification = np.concatenate(test_predictions_classification)\n",
    "    test_true_labels_classification = np.concatenate(test_true_labels_classification)\n",
    "\n",
    "    # Calculate metrics\n",
    "    test_mse = mean_squared_error(y_test_energy.cpu().numpy(), test_predictions_regression)\n",
    "    test_mae = mean_absolute_error(y_test_energy.cpu().numpy(), test_predictions_regression)\n",
    "    test_f1 = f1_score(test_true_labels_classification, test_predictions_classification, average='weighted')\n",
    "    test_accuracy = accuracy_score(test_true_labels_classification, test_predictions_classification)\n",
    "\n",
    "    # Return the evaluation metrics and predictions as a dictionary\n",
    "    return {\n",
    "        'mse': test_mse,\n",
    "        'mae': test_mae,\n",
    "        'f1_score': test_f1,\n",
    "        'accuracy': test_accuracy,\n",
    "        'predictions_regression': test_predictions_regression,\n",
    "        'predictions_classification': test_predictions_classification,\n",
    "        'true_labels_classification': test_true_labels_classification\n",
    "    }\n",
    "\n",
    "\n",
    "best_mo_gru_params = {\n",
    "    'hidden_size': 56,\n",
    "    'num_layers': 2,\n",
    "    'dropout_prob': 0.31034939184612553,\n",
    "    'learning_rate': 0.002000716164292076,\n",
    "}\n",
    "input_size = 1\n",
    "output_size_classification = 7  # Number of classes (0 to 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Create the model with the best hyperparameters\n",
    "model = MultiOutputGRUModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size=best_mo_gru_params['hidden_size'],\n",
    "    num_layers=best_mo_gru_params['num_layers'],\n",
    "    dropout_prob=best_mo_gru_params['dropout_prob'],\n",
    "    output_size_classification=output_size_classification\n",
    ").to(device)\n",
    "\n",
    "# Load the pre-trained model\n",
    "model_path = '/content/drive/MyDrive/EV_pred/final_multioutput_gru_model.pth'\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "# Initialize lists to store predictions for both datasets\n",
    "test_predictions_regression_Bry_Covid = []\n",
    "test_predictions_classification_Bry_Covid = []\n",
    "test_true_labels_classification_Bry_Covid = []\n",
    "\n",
    "test_predictions_regression_Bry_Covid_Partial_Reopenings_Range = []\n",
    "test_predictions_classification_Bry_Covid_Partial_Reopenings_Range = []\n",
    "test_true_labels_classification_Bry_Covid_Partial_Reopenings_Range = []\n",
    "\n",
    "# Evaluate on each station and store predictions for Bry_Covid and Bry_Covid_Partial_Reopenings_Range\n",
    "for station, (X_test, y_test_energy, y_test_ports) in {\n",
    "    'Bry_Covid_Partial_Reopenings_Range': (X_test_bry_covid_partial_reopenings_range, y_test_energy_bry_covid_partial_reopenings_range, y_test_ports_bry_covid_partial_reopenings_range),\n",
    "    'Bry_Covid': (X_test_df_bry_sliced_from_20200317_to_20200630, y_test_energy_df_bry_sliced_from_20200317_to_20200630, y_test_ports_df_bry_sliced_from_20200317_to_20200630)\n",
    "}.items():\n",
    "    metrics = evaluate_model_on_station(model, X_test, y_test_energy, y_test_ports, batch_size=32, device=device)\n",
    "\n",
    "    print(f\"Results for {station} Station:\")\n",
    "    print(f\"  Test MSE: {metrics['mse']:.4f}\")\n",
    "    print(f\"  Test MAE: {metrics['mae']:.4f}\")\n",
    "    print(f\"  Test F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Test Accuracy: {metrics['accuracy']:.4f}\\n\")\n",
    "\n",
    "    # Store predictions for Bry_Covid\n",
    "    if station == 'Bry_Covid':\n",
    "        test_predictions_regression_Bry_Covid.append(metrics['predictions_regression'])\n",
    "        test_predictions_classification_Bry_Covid.append(metrics['predictions_classification'])\n",
    "        test_true_labels_classification_Bry_Covid.append(metrics['true_labels_classification'])\n",
    "\n",
    "    # Store predictions for Bry_Covid_Partial_Reopenings_Range\n",
    "    elif station == 'Bry_Covid_Partial_Reopenings_Range':\n",
    "        test_predictions_regression_Bry_Covid_Partial_Reopenings_Range.append(metrics['predictions_regression'])\n",
    "        test_predictions_classification_Bry_Covid_Partial_Reopenings_Range.append(metrics['predictions_classification'])\n",
    "        test_true_labels_classification_Bry_Covid_Partial_Reopenings_Range.append(metrics['true_labels_classification'])\n",
    "\n",
    "# Concatenate predictions for Bry_Covid\n",
    "test_predictions_regression_Bry_Covid = np.concatenate(test_predictions_regression_Bry_Covid)\n",
    "test_predictions_classification_Bry_Covid = np.concatenate(test_predictions_classification_Bry_Covid)\n",
    "test_true_labels_classification_Bry_Covid = np.concatenate(test_true_labels_classification_Bry_Covid)\n",
    "\n",
    "# Concatenate predictions for Bry_Covid_Partial_Reopenings_Range\n",
    "test_predictions_regression_Bry_Covid_Partial_Reopenings_Range = np.concatenate(test_predictions_regression_Bry_Covid_Partial_Reopenings_Range)\n",
    "test_predictions_classification_Bry_Covid_Partial_Reopenings_Range = np.concatenate(test_predictions_classification_Bry_Covid_Partial_Reopenings_Range)\n",
    "test_true_labels_classification_Bry_Covid_Partial_Reopenings_Range = np.concatenate(test_true_labels_classification_Bry_Covid_Partial_Reopenings_Range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fQi53j_Qr4hE"
   },
   "outputs": [],
   "source": [
    "def evaluate_model_on_station(model, X_test, y_test_energy, y_test_ports, batch_size=32, device='cpu'):\n",
    "    # Create the test dataset and DataLoader\n",
    "    test_dataset = MultiOutputTimeSeriesDataset(X_test, y_test_energy, y_test_ports)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "    # Initialize lists to hold predictions and true labels\n",
    "    test_predictions_regression = []\n",
    "    test_predictions_classification = []\n",
    "    test_true_labels_classification = []\n",
    "\n",
    "    # Set model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation\n",
    "    with torch.no_grad():\n",
    "        for test_inputs, test_labels_regression, test_labels_classification in test_loader:\n",
    "            # Move inputs and labels to the specified device\n",
    "            test_inputs = test_inputs.to(device)\n",
    "            test_labels_regression = test_labels_regression.to(device)\n",
    "            test_labels_classification = test_labels_classification.to(device)\n",
    "\n",
    "            # Perform predictions\n",
    "            test_outputs_regression, test_outputs_classification = model(test_inputs)\n",
    "\n",
    "            # Store predictions and true labels\n",
    "            test_predictions_regression.append(test_outputs_regression.cpu().numpy())\n",
    "            test_predictions_classification.append(torch.argmax(test_outputs_classification, dim=1).cpu().numpy())\n",
    "            test_true_labels_classification.append(test_labels_classification.cpu().numpy())\n",
    "\n",
    "    # Concatenate all batches\n",
    "    test_predictions_regression = np.concatenate(test_predictions_regression)\n",
    "    test_predictions_classification = np.concatenate(test_predictions_classification)\n",
    "    test_true_labels_classification = np.concatenate(test_true_labels_classification)\n",
    "\n",
    "    # Calculate metrics\n",
    "    test_mse = mean_squared_error(y_test_energy.cpu().numpy(), test_predictions_regression)\n",
    "    test_mae = mean_absolute_error(y_test_energy.cpu().numpy(), test_predictions_regression)\n",
    "    test_f1 = f1_score(test_true_labels_classification, test_predictions_classification, average='weighted')\n",
    "    test_accuracy = accuracy_score(test_true_labels_classification, test_predictions_classification)\n",
    "\n",
    "    # Return the evaluation metrics as a dictionary\n",
    "    return {\n",
    "        'mse': test_mse,\n",
    "        'mae': test_mae,\n",
    "        'f1_score': test_f1,\n",
    "        'accuracy': test_accuracy\n",
    "    }\n",
    "\n",
    "\n",
    "input_size=1\n",
    "output_size_classification= 7 # Number of classes (0 to 6)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Model with the best parameters\n",
    "model = MultiOutputLSTM_GRU_Model(\n",
    "    input_size=input_size,\n",
    "    hidden_size_lstm=57,\n",
    "    num_layers_lstm=2,\n",
    "    dropout_prob_lstm=0.2809202101658443,\n",
    "    hidden_size_gru=59,\n",
    "    num_layers_gru=2,\n",
    "    dropout_prob_gru=0.25834085665265316,\n",
    "    output_size_classification=output_size_classification\n",
    ").to(device)\n",
    "\n",
    "\n",
    "model_path = '/content/drive/MyDrive/EV_pred/final_multioutput_lstm_gru_model.pth'\n",
    "model.load_state_dict(torch.load('/content/drive/MyDrive/EV_pred/final_multioutput_lstm_gru_model.pth', map_location=device))\n",
    "\n",
    "# Evaluate on each station and print results\n",
    "for station, (X_test, y_test_energy, y_test_ports) in {\n",
    "    'Ham_Sliced_to_20200229': (X_test_ham_sliced_to_20200229, y_test_energy_ham_sliced_to_20200229, y_test_ports_ham_sliced_to_20200229),\n",
    "    'Cam_Sliced_to_20200229': (X_test_cam_sliced_to_20200229, y_test_energy_cam_sliced_to_20200229, y_test_ports_cam_sliced_to_20200229),\n",
    "    'Rin_Sliced_to_20200229': (X_test_rin_sliced_to_20200229, y_test_energy_rin_sliced_to_20200229, y_test_ports_rin_sliced_to_20200229),\n",
    "    'Bry_Sliced_to_20200229': (X_test_bry_sliced_to_20200229, y_test_energy_bry_sliced_to_20200229, y_test_ports_bry_sliced_to_20200229),\n",
    "\n",
    "    'Ham_Covid_Early_Lockdown_Range': (X_test_ham_covid_early_lockdown_range, y_test_energy_ham_covid_early_lockdown_range, y_test_ports_ham_covid_early_lockdown_range),\n",
    "    'Cam_Covid_Early_Lockdown_Range': (X_test_cam_covid_early_lockdown_range, y_test_energy_cam_covid_early_lockdown_range, y_test_ports_cam_covid_early_lockdown_range),\n",
    "    'Rin_Covid_Early_Lockdown_Range': (X_test_rin_covid_early_lockdown_range, y_test_energy_rin_covid_early_lockdown_range, y_test_ports_rin_covid_early_lockdown_range),\n",
    "    'Bry_Covid_Early_Lockdown_Range': (X_test_bry_covid_early_lockdown_range, y_test_energy_bry_covid_early_lockdown_range, y_test_ports_bry_covid_early_lockdown_range),\n",
    "\n",
    "    'Ham_Covid_Partial_Reopenings_Range': (X_test_ham_covid_partial_reopenings_range, y_test_energy_ham_covid_partial_reopenings_range, y_test_ports_ham_covid_partial_reopenings_range),\n",
    "    'Cam_Covid_Partial_Reopenings_Range': (X_test_cam_covid_partial_reopenings_range, y_test_energy_cam_covid_partial_reopenings_range, y_test_ports_cam_covid_partial_reopenings_range),\n",
    "    'Rin_Covid_Partial_Reopenings_Range': (X_test_rin_covid_partial_reopenings_range, y_test_energy_rin_covid_partial_reopenings_range, y_test_ports_rin_covid_partial_reopenings_range),\n",
    "    'Bry_Covid_Partial_Reopenings_Range': (X_test_bry_covid_partial_reopenings_range, y_test_energy_bry_covid_partial_reopenings_range, y_test_ports_bry_covid_partial_reopenings_range),\n",
    "    'Bry_Covid':(X_test_df_bry_sliced_from_20200317_to_20200630, y_test_energy_df_bry_sliced_from_20200317_to_20200630, y_test_ports_df_bry_sliced_from_20200317_to_20200630)\n",
    "}.items():\n",
    "    metrics = evaluate_model_on_station(model, X_test, y_test_energy, y_test_ports, batch_size=32, device=device)\n",
    "    print(f\"Results for {station} Station:\")\n",
    "    print(f\"  Test MSE: {metrics['mse']:.4f}\")\n",
    "    print(f\"  Test MAE: {metrics['mae']:.4f}\")\n",
    "    print(f\"  Test F1 Score: {metrics['f1_score']:.4f}\")\n",
    "    print(f\"  Test Accuracy: {metrics['accuracy']:.4f}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fd_C6cJWbx8D"
   },
   "source": [
    "### **5.5.3. SHAP Analysis on Best Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lNRYRIDxeVy1"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Load the trained GRU model\n",
    "best_model_gru = MultiOutputGRUModel(\n",
    "    input_size=input_size,\n",
    "    hidden_size=best_mo_gru_params['hidden_size'],\n",
    "    num_layers=best_mo_gru_params['num_layers'],\n",
    "    dropout_prob=best_mo_gru_params['dropout_prob'],\n",
    "    output_size_classification=output_size_classification\n",
    ").to(device)\n",
    "\n",
    "# Load the model state dictionary\n",
    "model_path_gru = '/content/drive/MyDrive/EV_pred/final_multioutput_gru_model.pth'\n",
    "if torch.cuda.is_available():\n",
    "    best_model_gru.load_state_dict(torch.load(model_path_gru))\n",
    "else:\n",
    "    best_model_gru.load_state_dict(torch.load(model_path_gru, map_location=torch.device('cpu')))\n",
    "\n",
    "best_model_gru.eval()\n",
    "\n",
    "# Print the shape of the test data\n",
    "print(\"Shape of X_test_combined_multi:\", X_test_combined_multi.shape)\n",
    "\n",
    "# Define placeholders for sequence features\n",
    "sequence_placeholder = np.zeros((X_test_subset_static.shape[0], 168, 1))  # Adjust sequence length if different\n",
    "\n",
    "# Combine static features with sequence placeholders\n",
    "reintegrated_data_gru = np.concatenate(\n",
    "    [sequence_placeholder, X_test_subset_static[:, :, np.newaxis]],  # Add new axis for consistency\n",
    "    axis=1  # Combine along the sequence dimension\n",
    ")  # Final shape: (batch_size, 191, 1)\n",
    "\n",
    "# Define the model wrapper for regression SHAP explanation\n",
    "def model_static_wrapper_regression(input_data):\n",
    "    # Create placeholder for sequence features\n",
    "    sequence_placeholder = np.zeros((input_data.shape[0], 168, 1))\n",
    "    reintegrated_data = np.concatenate(\n",
    "        [sequence_placeholder, input_data[:, :, np.newaxis]], axis=1\n",
    "    )\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    reintegrated_data = torch.tensor(reintegrated_data, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Forward through the model\n",
    "    with torch.no_grad():\n",
    "        regression_output, _ = best_model_gru(reintegrated_data)\n",
    "    return regression_output.cpu().numpy()\n",
    "\n",
    "# Initialize SHAP explainer for regression\n",
    "explainer_static_regression = shap.Explainer(model_static_wrapper_regression, masker)\n",
    "\n",
    "# Calculate SHAP values for regression\n",
    "shap_values_static_regression = explainer_static_regression(X_test_subset_static)\n",
    "\n",
    "# Save SHAP values and feature names for regression\n",
    "np.save('/content/drive/My Drive/SHAP_Results/shap_values_static_gru_regression.npy', shap_values_static_regression.values)\n",
    "np.save('/content/drive/My Drive/SHAP_Results/feature_names_gru_regression.npy', np.array(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vdQU6mYxhf9h"
   },
   "outputs": [],
   "source": [
    "# Load SHAP values and feature names\n",
    "shap_values_static_regression = np.load('/content/drive/My Drive/SHAP_Results/shap_values_static_gru_regression.npy')\n",
    "feature_names = np.load('/content/drive/My Drive/SHAP_Results/feature_names_gru_regression.npy', allow_pickle=True)\n",
    "\n",
    "# Convert test data subset to a DataFrame\n",
    "X_test_subset_static_df = pd.DataFrame(X_test_subset_static, columns=feature_names)\n",
    "\n",
    "# Plot summary for regression\n",
    "shap.summary_plot(shap_values_static_regression, X_test_subset_static_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j57MJ8hgY3ks"
   },
   "outputs": [],
   "source": [
    "# Generate feature importance bar plot for regression\n",
    "shap.summary_plot(shap_values_static_regression, X_test_subset_static_df, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kd0I3PYrQTqW"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Adjust DPI for high-resolution output\n",
    "plt.figure(dpi=300)\n",
    "\n",
    "# Create the SHAP summary bar plot\n",
    "shap.summary_plot(\n",
    "    shap_values_static_regression,\n",
    "    X_test_subset_static_df,\n",
    "    plot_type=\"bar\",\n",
    "    show=False  # Prevent auto display for additional customization\n",
    ")\n",
    "\n",
    "# Customize plot aesthetics\n",
    "plt.gcf().set_size_inches(8, 6)  # Set figure size\n",
    "plt.title(\"Feature Importance (SHAP Values)\", fontsize=12, fontweight='bold')  # Add title\n",
    "plt.xlabel(\"Mean Absolute SHAP Value\", fontsize=10)  # X-axis label\n",
    "plt.ylabel(\"Features\", fontsize=10)  # Y-axis label\n",
    "plt.xticks(fontsize=10)  # Adjust tick size\n",
    "plt.yticks(fontsize=10)  # Adjust tick size\n",
    "\n",
    "# Customize axis lines\n",
    "ax = plt.gca()\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_alpha(0.5)  # Reduce opacity\n",
    "    spine.set_linewidth(0.8)  # Make thinner\n",
    "\n",
    "# Remove top and right spines for a cleaner look\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Adjust grid for clarity\n",
    "plt.grid(visible=True, which='both', axis='x', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "plt.tight_layout()  # Ensure layout fits well\n",
    "\n",
    "# Save the plot\n",
    "plt.savefig(\"shap_feature_importance.png\", dpi=300, bbox_inches='tight')  # Save as PNG\n",
    "#plt.savefig(\"shap_feature_importance.pdf\", bbox_inches='tight')  # Save as PDF for journal submission\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9t82GaSNYwwD"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Adjust DPI for high-resolution output\n",
    "plt.figure(dpi=300)\n",
    "\n",
    "# Create the SHAP summary bar plot\n",
    "shap.summary_plot(\n",
    "    shap_values_static_regression,\n",
    "    X_test_subset_static_df,\n",
    "    plot_type=\"bar\",\n",
    "    show=False  # Prevent auto display for additional customization\n",
    ")\n",
    "\n",
    "# Customize plot aesthetics\n",
    "plt.gcf().set_size_inches(8, 6)  # Set figure size\n",
    "plt.title(\"Feature Importance (SHAP Values)\", fontsize=12, fontweight='bold')  # Add title\n",
    "plt.xlabel(\"Mean Absolute SHAP Value\", fontsize=10)  # X-axis label\n",
    "plt.ylabel(\"Features\", fontsize=10)  # Y-axis label\n",
    "plt.xticks(fontsize=10)  # Adjust tick size\n",
    "plt.yticks(fontsize=10)  # Adjust tick size\n",
    "\n",
    "# Customize axis lines\n",
    "ax = plt.gca()\n",
    "for spine in ax.spines.values():\n",
    "    spine.set_alpha(0.5)  # Reduce opacity\n",
    "    spine.set_linewidth(0.8)  # Make thinner\n",
    "\n",
    "# Remove top and right spines for a cleaner look\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "\n",
    "# Adjust grid for clarity, remove y gridlines\n",
    "plt.grid(visible=False, which='both', axis='x', linestyle='--', linewidth=0.5, alpha=0.7)\n",
    "\n",
    "# Tighten layout to ensure plot fits well\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the plot\n",
    "#plt.savefig(\"shap_feature_importance.png\", dpi=300, bbox_inches='tight')  # Save as PNG\n",
    "#plt.savefig(\"shap_feature_importance.pdf\", bbox_inches='tight')  # Save as PDF for journal submission\n",
    "\n",
    "# Show the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JxKihNl5cy3-"
   },
   "outputs": [],
   "source": [
    "# Define the model wrapper for classification SHAP explanation\n",
    "def model_static_wrapper_classification(input_data):\n",
    "    # Create placeholder for sequence features\n",
    "    sequence_placeholder = np.zeros((input_data.shape[0], 168, 1))\n",
    "    reintegrated_data = np.concatenate(\n",
    "        [sequence_placeholder, input_data[:, :, np.newaxis]], axis=1\n",
    "    )\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    reintegrated_data = torch.tensor(reintegrated_data, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Pass through the model\n",
    "    with torch.no_grad():\n",
    "        _, classification_output = best_model_gru(reintegrated_data)\n",
    "    return classification_output.cpu().numpy()  # Return logits or probabilities for all classes\n",
    "\n",
    "# Initialize SHAP explainer for classification\n",
    "explainer_static_classification = shap.Explainer(model_static_wrapper_classification, masker)\n",
    "\n",
    "# Calculate SHAP values for classification\n",
    "shap_values_static_classification = explainer_static_classification(X_test_subset_static)\n",
    "\n",
    "# Save SHAP values and feature names for classification\n",
    "np.save('/content/drive/My Drive/SHAP_Results/shap_values_static_gru_classification.npy', shap_values_static_classification.values)\n",
    "np.save('/content/drive/My Drive/SHAP_Results/feature_names_gru_classification.npy', np.array(feature_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8bfPaJwhjlj"
   },
   "outputs": [],
   "source": [
    "# Load SHAP values and feature names for classification\n",
    "shap_values_static_classification = np.load('/content/drive/My Drive/SHAP_Results/shap_values_static_gru_classification.npy')\n",
    "feature_names = np.load('/content/drive/My Drive/SHAP_Results/feature_names_gru_classification.npy', allow_pickle=True)\n",
    "\n",
    "# Visualization for multi-class SHAP values\n",
    "X_test_subset_static_df = pd.DataFrame(X_test_subset_static, columns=feature_names)\n",
    "\n",
    "# Check if binary or multi-class classification\n",
    "if shap_values_static_classification.ndim == 3 and shap_values_static_classification.shape[2] == 2:  # Binary classification\n",
    "    shap_values_positive_class = shap_values_static_classification[..., 1]  # SHAP values for positive class\n",
    "    shap.summary_plot(shap_values_positive_class, X_test_subset_static_df)\n",
    "elif shap_values_static_classification.ndim == 3:  # Multi-class classification\n",
    "    for class_idx in range(shap_values_static_classification.shape[2]):  # Iterate over classes\n",
    "        print(f\"Class {class_idx} SHAP values:\")\n",
    "        shap_values_class = shap_values_static_classification[..., class_idx]\n",
    "        shap.summary_plot(shap_values_class, X_test_subset_static_df)\n",
    "else:  # SHAP values have only two dimensions\n",
    "    print(\"SHAP values are missing class-specific outputs. Visualizing overall contributions.\")\n",
    "    shap.summary_plot(shap_values_static_classification, X_test_subset_static_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rJwc3XH0VEFr"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load SHAP values and feature names for classification\n",
    "shap_values_static_classification = np.load('/content/drive/My Drive/SHAP_Results/shap_values_static_gru_classification.npy')\n",
    "feature_names = np.load('/content/drive/My Drive/SHAP_Results/feature_names_gru_classification.npy', allow_pickle=True)\n",
    "\n",
    "# Visualization for multi-class SHAP values\n",
    "X_test_subset_static_df = pd.DataFrame(X_test_subset_static, columns=feature_names)\n",
    "\n",
    "# Check if binary or multi-class classification\n",
    "if shap_values_static_classification.ndim == 3 and shap_values_static_classification.shape[2] == 2:  # Binary classification\n",
    "    shap_values_positive_class = shap_values_static_classification[..., 1]  # SHAP values for positive class\n",
    "    # Set the resolution to 300 DPI and plot\n",
    "    plt.figure(dpi=300)\n",
    "    shap.summary_plot(shap_values_positive_class, X_test_subset_static_df)\n",
    "elif shap_values_static_classification.ndim == 3:  # Multi-class classification\n",
    "    for class_idx in range(shap_values_static_classification.shape[2]):  # Iterate over classes\n",
    "        print(f\"Class {class_idx} SHAP values:\")\n",
    "        shap_values_class = shap_values_static_classification[..., class_idx]\n",
    "        # Set the resolution to 300 DPI and plot\n",
    "        plt.figure(dpi=300)\n",
    "        shap.summary_plot(shap_values_class, X_test_subset_static_df)\n",
    "else:  # SHAP values have only two dimensions\n",
    "    print(\"SHAP values are missing class-specific outputs. Visualizing overall contributions.\")\n",
    "    # Set the resolution to 300 DPI and plot\n",
    "    plt.figure(dpi=300)\n",
    "    shap.summary_plot(shap_values_static_classification, X_test_subset_static_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5yOGEdAjnavu"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# Load SHAP values and feature names for classification\n",
    "shap_values_static_classification = np.load('/content/drive/My Drive/SHAP_Results/shap_values_static_gru_classification.npy')\n",
    "feature_names = np.load('/content/drive/My Drive/SHAP_Results/feature_names_gru_classification.npy', allow_pickle=True)\n",
    "\n",
    "# Visualization for multi-class SHAP values\n",
    "X_test_subset_static_df = pd.DataFrame(X_test_subset_static, columns=feature_names)\n",
    "\n",
    "# Create a list to hold the SHAP plot images\n",
    "plot_images = []\n",
    "\n",
    "# Check if binary or multi-class classification\n",
    "if shap_values_static_classification.ndim == 3 and shap_values_static_classification.shape[2] == 2:  # Binary classification\n",
    "    shap_values_positive_class = shap_values_static_classification[..., 1]  # SHAP values for positive class\n",
    "    # Create a plot and save it as an image in a buffer\n",
    "    plt.figure(dpi=300)\n",
    "    shap.summary_plot(shap_values_positive_class, X_test_subset_static_df, show=False, plot_size=(5, 5))\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plot_images.append(buf)  # Append the image buffer to the list\n",
    "    plt.close()  # Close the plot to prevent it from showing\n",
    "elif shap_values_static_classification.ndim == 3:  # Multi-class classification\n",
    "    num_classes = shap_values_static_classification.shape[2]\n",
    "    for class_idx in range(num_classes):  # Iterate over classes\n",
    "        print(f\"Class {class_idx} SHAP values:\")\n",
    "        shap_values_class = shap_values_static_classification[..., class_idx]\n",
    "        # Create a plot and save it as an image in a buffer\n",
    "        plt.figure(dpi=300)\n",
    "        shap.summary_plot(shap_values_class, X_test_subset_static_df, show=False, plot_size=(5, 5))\n",
    "        buf = BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        plot_images.append(buf)  # Append the image buffer to the list\n",
    "        plt.close()  # Close the plot to prevent it from showing\n",
    "else:  # SHAP values have only two dimensions\n",
    "    print(\"SHAP values are missing class-specific outputs. Visualizing overall contributions.\")\n",
    "    plt.figure(dpi=300)\n",
    "    shap.summary_plot(shap_values_static_classification, X_test_subset_static_df, show=False, plot_size=(5, 5))\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plot_images.append(buf)  # Append the image buffer to the list\n",
    "    plt.close()  # Close the plot to prevent it from showing\n",
    "\n",
    "# Create a grid of subplots to display the images\n",
    "num_plots = len(plot_images)\n",
    "fig, axes = plt.subplots(1, num_plots, figsize=(6 * num_plots, 6), dpi=300)\n",
    "\n",
    "# If there's only one plot, axes will be a single object, so we wrap it in a list\n",
    "if num_plots == 1:\n",
    "    axes = [axes]\n",
    "\n",
    "# Iterate through the collected plot images and add them to the subplots\n",
    "for i, buf in enumerate(plot_images):\n",
    "    # Read the image buffer into an image array\n",
    "    img = plt.imread(buf)\n",
    "\n",
    "    # Display the image on the corresponding subplot\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')  # Turn off axis for clean display\n",
    "    axes[i].set_title(f'Class {i}' if num_plots > 1 else 'SHAP Summary Plot')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "a2ka55azpYPz"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# Load SHAP values and feature names for classification\n",
    "shap_values_static_classification = np.load('/content/drive/My Drive/SHAP_Results/shap_values_static_gru_classification.npy')\n",
    "feature_names = np.load('/content/drive/My Drive/SHAP_Results/feature_names_gru_classification.npy', allow_pickle=True)\n",
    "\n",
    "# Visualization for multi-class SHAP values\n",
    "X_test_subset_static_df = pd.DataFrame(X_test_subset_static, columns=feature_names)\n",
    "\n",
    "# Create a list to hold the SHAP plot images\n",
    "plot_images = []\n",
    "\n",
    "# Check if binary or multi-class classification\n",
    "if shap_values_static_classification.ndim == 3 and shap_values_static_classification.shape[2] == 2:  # Binary classification\n",
    "    shap_values_positive_class = shap_values_static_classification[..., 1]  # SHAP values for positive class\n",
    "    # Create a plot and save it as an image in a buffer\n",
    "    plt.figure(dpi=300)\n",
    "    shap.summary_plot(shap_values_positive_class, X_test_subset_static_df, show=False, plot_size=(5, 5))\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plot_images.append(buf)  # Append the image buffer to the list\n",
    "    plt.close()  # Close the plot to prevent it from showing\n",
    "elif shap_values_static_classification.ndim == 3:  # Multi-class classification\n",
    "    num_classes = shap_values_static_classification.shape[2]\n",
    "    for class_idx in range(num_classes):  # Iterate over classes\n",
    "        print(f\"Class {class_idx} SHAP values:\")\n",
    "        shap_values_class = shap_values_static_classification[..., class_idx]\n",
    "        # Create a plot and save it as an image in a buffer\n",
    "        plt.figure(dpi=300)\n",
    "        shap.summary_plot(shap_values_class, X_test_subset_static_df, show=False, plot_size=(5, 5))\n",
    "        buf = BytesIO()\n",
    "        plt.savefig(buf, format='png')\n",
    "        buf.seek(0)\n",
    "        plot_images.append(buf)  # Append the image buffer to the list\n",
    "        plt.close()  # Close the plot to prevent it from showing\n",
    "else:  # SHAP values have only two dimensions\n",
    "    print(\"SHAP values are missing class-specific outputs. Visualizing overall contributions.\")\n",
    "    plt.figure(dpi=300)\n",
    "    shap.summary_plot(shap_values_static_classification, X_test_subset_static_df, show=False, plot_size=(5, 5))\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png')\n",
    "    buf.seek(0)\n",
    "    plot_images.append(buf)  # Append the image buffer to the list\n",
    "    plt.close()  # Close the plot to prevent it from showing\n",
    "\n",
    "# Create a grid of subplots to display the images in two rows\n",
    "num_plots = len(plot_images)\n",
    "rows = 2  # Set the number of rows\n",
    "cols = (num_plots + 1) // 2  # Calculate the number of columns, rounding up if necessary\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(6 * cols, 6 * rows), dpi=300)\n",
    "\n",
    "# If there's only one plot, axes will be a single object, so we wrap it in a list\n",
    "if num_plots == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()  # Flatten axes if it's a 2D array\n",
    "\n",
    "# Iterate through the collected plot images and add them to the subplots\n",
    "for i, buf in enumerate(plot_images):\n",
    "    # Read the image buffer into an image array\n",
    "    img = plt.imread(buf)\n",
    "\n",
    "    # Display the image on the corresponding subplot\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')  # Turn off axis for clean display\n",
    "    axes[i].set_title(f'Class {i}' if num_plots > 1 else 'SHAP Summary Plot')\n",
    "\n",
    "# Adjust layout for better spacing\n",
    "plt.tight_layout()\n",
    "\n",
    "# Save the final plot with dpi=300\n",
    "plt.savefig('shap_summary_plots.png', dpi=300)\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B-XVpcvqp_HS"
   },
   "outputs": [],
   "source": [
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from io import BytesIO\n",
    "from matplotlib import gridspec\n",
    "\n",
    "# Load SHAP values and feature names for classification\n",
    "shap_values_static_classification = np.load('/content/drive/My Drive/SHAP_Results/shap_values_static_gru_classification.npy')\n",
    "feature_names = np.load('/content/drive/My Drive/SHAP_Results/feature_names_gru_classification.npy', allow_pickle=True)\n",
    "\n",
    "# Visualization for multi-class SHAP values\n",
    "X_test_subset_static_df = pd.DataFrame(X_test_subset_static, columns=feature_names)\n",
    "\n",
    "# Create a list to hold the SHAP plot images\n",
    "plot_images = []\n",
    "\n",
    "# Check if binary or multi-class classification\n",
    "if shap_values_static_classification.ndim == 3 and shap_values_static_classification.shape[2] == 2:  # Binary classification\n",
    "    shap_values_positive_class = shap_values_static_classification[..., 1]  # SHAP values for positive class\n",
    "    # Create a plot and save it as an image in a buffer\n",
    "    plt.figure(dpi=300)\n",
    "    shap.summary_plot(shap_values_positive_class, X_test_subset_static_df, show=False, plot_size=(5, 5))\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png', dpi=300)\n",
    "    buf.seek(0)\n",
    "    plot_images.append(buf)  # Append the image buffer to the list\n",
    "    plt.close()  # Close the plot to prevent it from showing\n",
    "elif shap_values_static_classification.ndim == 3:  # Multi-class classification\n",
    "    num_classes = shap_values_static_classification.shape[2]\n",
    "    for class_idx in range(num_classes):  # Iterate over classes\n",
    "        print(f\"Class {class_idx} SHAP values:\")\n",
    "        shap_values_class = shap_values_static_classification[..., class_idx]\n",
    "        # Create a plot and save it as an image in a buffer\n",
    "        plt.figure(dpi=300)\n",
    "        shap.summary_plot(shap_values_class, X_test_subset_static_df, show=False, plot_size=(5, 5))\n",
    "        buf = BytesIO()\n",
    "        plt.savefig(buf, format='png', dpi=300)\n",
    "        buf.seek(0)\n",
    "        plot_images.append(buf)  # Append the image buffer to the list\n",
    "        plt.close()  # Close the plot to prevent it from showing\n",
    "else:  # SHAP values have only two dimensions\n",
    "    print(\"SHAP values are missing class-specific outputs. Visualizing overall contributions.\")\n",
    "    plt.figure(dpi=300)\n",
    "    shap.summary_plot(shap_values_static_classification, X_test_subset_static_df, show=False, plot_size=(5, 5))\n",
    "    buf = BytesIO()\n",
    "    plt.savefig(buf, format='png', dpi=300)\n",
    "    buf.seek(0)\n",
    "    plot_images.append(buf)  # Append the image buffer to the list\n",
    "    plt.close()  # Close the plot to prevent it from showing\n",
    "\n",
    "# Create a grid of subplots to display the images in two rows\n",
    "num_plots = len(plot_images)\n",
    "rows = 2  # Set the number of rows\n",
    "cols = (num_plots + 1) // 2  # Calculate the number of columns, rounding up if necessary\n",
    "\n",
    "# Create the subplots\n",
    "fig, axes = plt.subplots(rows, cols, figsize=(6 * cols, 6 * rows), dpi=300)\n",
    "\n",
    "# If there's only one plot, axes will be a single object, so we wrap it in a list\n",
    "if num_plots == 1:\n",
    "    axes = [axes]\n",
    "else:\n",
    "    axes = axes.flatten()  # Flatten axes if it's a 2D array\n",
    "\n",
    "# Iterate through the collected plot images and add them to the subplots\n",
    "for i, buf in enumerate(plot_images):\n",
    "    # Read the image buffer into an image array\n",
    "    img = plt.imread(buf)\n",
    "\n",
    "    # Display the image on the corresponding subplot\n",
    "    axes[i].imshow(img)\n",
    "    axes[i].axis('off')  # Turn off axis for clean display\n",
    "    axes[i].set_title(f'Class {i}' if num_plots > 1 else 'SHAP Summary Plot', fontsize=12)\n",
    "\n",
    "# Adjust layout for better spacing and to avoid overlaps\n",
    "plt.tight_layout(pad=2.0)  # Increase the padding for clarity\n",
    "\n",
    "# Save the final plot with dpi=300 (publication-quality)\n",
    "plt.savefig('shap_summary_plots_publication.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the combined plot (optional in notebook)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_Jsw7wQNbF5G"
   },
   "outputs": [],
   "source": [
    "# Feature importance bar plot for classification\n",
    "if shap_values_static_classification.ndim == 3:  # Multi-class\n",
    "    for class_idx in range(shap_values_static_classification.shape[2]):  # Iterate over classes\n",
    "        print(f\"Feature importance for Class {class_idx}:\")\n",
    "        shap.summary_plot(shap_values_static_classification[..., class_idx], X_test_subset_static_df, plot_type=\"bar\")\n",
    "else:  # Binary or single-class case\n",
    "    print(\"Feature importance for binary classification:\")\n",
    "    shap.summary_plot(shap_values_static_classification, X_test_subset_static_df, plot_type=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "85-iUYTSojMs"
   },
   "source": [
    "### **5.5.4. Demo of an Informative Dashboard Based on Model Predictions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OLfpKTPjo7jC"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import random\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(713)\n",
    "\n",
    "# Set Seaborn style for better aesthetics\n",
    "sns.set(style=\"whitegrid\")\n",
    "\n",
    "# Set font to DejaVu Serif, similar to Times New Roman\n",
    "plt.rcParams[\"font.family\"] = \"serif\"\n",
    "plt.rcParams[\"font.serif\"] = [\"DejaVu Serif\"]\n",
    "plt.rcParams[\"font.size\"] = 12  # Adjusted font size for better readability\n",
    "\n",
    "# Placeholder for data - make sure you have the date, actual values, and predictions for each station\n",
    "stations = {\n",
    "    'Ham': (y_test_dates_ports_ham, y_test_ham_ports.cpu().numpy(), test_predictions_classification_ham),\n",
    "    'Cam': (y_test_dates_ports_cam, y_test_cam_ports.cpu().numpy(), test_predictions_classification_cam),\n",
    "    'Rin': (y_test_dates_ports_rin, y_test_rin_ports.cpu().numpy(), test_predictions_classification_rin),\n",
    "    'Bry': (y_test_dates_ports_bry, y_test_bry_ports.cpu().numpy(), test_predictions_classification_bry)\n",
    "}\n",
    "\n",
    "# Plotting the figure\n",
    "fig, axes = plt.subplots(len(stations), 2, figsize=(15, len(stations) * 5), sharex=False)\n",
    "fig.suptitle('Available Ports: Actual vs Predicted for Each Station', fontsize=14, fontweight='bold')\n",
    "\n",
    "for i, (station_name, (dates, actual, predicted)) in enumerate(stations.items()):\n",
    "    # Convert index to datetime if not already done\n",
    "    dates.index = pd.to_datetime(dates.index)\n",
    "\n",
    "    # Calculate overall accuracy and F1 score for the station's test set\n",
    "    accuracy = accuracy_score(actual, predicted)\n",
    "    f1 = f1_score(actual, predicted, average='weighted')\n",
    "\n",
    "    # Plot 1: Overall Time Series Plot for the station\n",
    "    axes[i, 0].plot(dates.index, actual, label='Actual Available Ports', color='blue', linewidth=1)\n",
    "    axes[i, 0].plot(dates.index, predicted, label='Predicted Available Ports', color='orange', linestyle='--', linewidth=1)\n",
    "    axes[i, 0].set_xlabel('Date', fontsize=12)\n",
    "    axes[i, 0].set_ylabel('Available Ports', fontsize=12)\n",
    "    axes[i, 0].set_title(f'{station_name} Station: Actual vs Predicted (Overall)', fontsize=12, fontweight='bold')\n",
    "    axes[i, 0].legend(fontsize=10, loc='upper right')\n",
    "    axes[i, 0].grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Add accuracy and F1 score as text on the plot\n",
    "    axes[i, 0].text(0.02, 0.95, f'Accuracy: {accuracy:.2f}\\nF1 Score: {f1:.2f}', transform=axes[i, 0].transAxes, fontsize=12,\n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "    # Plot 2: Selected Week Plot for the station\n",
    "    selected_week_start = random.choice(dates.index.to_period('W')).start_time\n",
    "    selected_week_data = dates[(dates.index >= selected_week_start) & (dates.index < selected_week_start + pd.Timedelta(days=7))]\n",
    "    selected_week_actual = actual[(dates.index >= selected_week_start) & (dates.index < selected_week_start + pd.Timedelta(days=7))]\n",
    "    selected_week_predicted = predicted[(dates.index >= selected_week_start) & (dates.index < selected_week_start + pd.Timedelta(days=7))]\n",
    "\n",
    "    # Calculate F1 score for the selected week\n",
    "    f1_selected_week = f1_score(selected_week_actual, selected_week_predicted, average='weighted')\n",
    "\n",
    "    axes[i, 1].plot(selected_week_data.index, selected_week_actual, label='Actual Available Ports', color='blue')\n",
    "    axes[i, 1].plot(selected_week_data.index, selected_week_predicted, label='Predicted Available Ports', color='orange', linestyle='--')\n",
    "    axes[i, 1].set_xlabel('Date', fontsize=12)\n",
    "    axes[i, 1].set_ylabel('Available Ports', fontsize=12)\n",
    "    axes[i, 1].set_title(f'{station_name} Station: Actual vs Predicted (Selected Week)', fontsize=12, fontweight='bold')\n",
    "    axes[i, 1].legend(fontsize=10, loc='upper right')\n",
    "    axes[i, 1].grid(which='both', linestyle='--', linewidth=0.5)\n",
    "\n",
    "    # Add F1 score for the selected week as text on the plot\n",
    "    axes[i, 1].text(0.02, 0.95, f'F1 Score for Selected Week: {f1_selected_week:.2f}', transform=axes[i, 1].transAxes, fontsize=12,\n",
    "                    verticalalignment='top', bbox=dict(boxstyle='round,pad=0.3', edgecolor='gray', facecolor='white'))\n",
    "\n",
    "# Tight layout for better spacing\n",
    "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "# Save or display the plot\n",
    "plt.savefig('station_available_ports_plots.png', dpi=300)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "j_tyt_bGQOmn"
   },
   "outputs": [],
   "source": [
    "!pip install dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xLWL_VNNQ67-"
   },
   "outputs": [],
   "source": [
    "!pip install pyngrok"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B345AVoa3PT3"
   },
   "source": [
    "Date and Time: 2020-02-28 14:00:00\n",
    "Actual Energy Consumption: 17.93303198857396 kWh\n",
    "Predicted Energy Consumption: [18.562878] kWh\n",
    "\n",
    "Date and Time: 2020-02-28 15:00:00\n",
    "Actual Energy Consumption: 10.56491969091045 kWh\n",
    "Predicted Energy Consumption: [10.994608] kWh\n",
    "\n",
    "Date and Time: 2020-02-28 14:00:00\n",
    "Actual Available Ports: 0\n",
    "Predicted Available Ports: 0\n",
    "\n",
    "Date and Time: 2020-02-28 15:00:00\n",
    "Actual Available Ports: 3\n",
    "Predicted Available Ports: 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBVIFTCmVjxM"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from datetime import datetime\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "current_energy_consumption = 17.9\n",
    "next_hour_energy = 10.6\n",
    "current_ports = 0\n",
    "next_hour_ports = 3\n",
    "\n",
    "# Location of the Bryant EV station\n",
    "station_name = \"Bryant\"\n",
    "latitude = 37.446373\n",
    "longitude = -122.162331\n",
    "\n",
    "# Initialize a Folium map centered on the station location\n",
    "ev_map = folium.Map(location=[latitude, longitude], zoom_start=15)\n",
    "\n",
    "# Function to update map marker and popup based on selected date and time\n",
    "def update_map(date_time):\n",
    "    # Convert date_time to datetime object\n",
    "    date_time = datetime.strptime(date_time, '%Y-%m-%dT%H:%M')\n",
    "\n",
    "    # Clear previous markers\n",
    "    ev_map.get_root().html.add_child(folium.Element())\n",
    "\n",
    "    # Add marker with custom icon and popup\n",
    "    popup_content = f\"\"\"\n",
    "    <div style=\"font-family: Arial, sans-serif;\">\n",
    "        <h4 style=\"margin-bottom: 5px;\">{station_name}</h4>\n",
    "        <p><strong>Date and Time:</strong> {date_time.strftime('%Y/%m/%d %H:%M')}</p>\n",
    "        <p><strong>Current Energy Consumption:</strong> {current_energy_consumption} kWh</p>\n",
    "        <p><strong>Next Hour Energy Consumption:</strong> {next_hour_energy} kWh</p>\n",
    "        <p><strong>Current Available Ports:</strong> {current_ports}</p>\n",
    "        <p><strong>Next Hour Available Ports:</strong> {next_hour_ports}</p>\n",
    "    </div>\n",
    "    \"\"\"\n",
    "\n",
    "    icon = folium.DivIcon(\n",
    "        html=f'<i class=\"fas fa-charging-station fa-2x\" style=\"color: #1e00ff;\"></i>')  # Custom FontAwesome icon\n",
    "\n",
    "    folium.Marker(\n",
    "        location=[latitude, longitude],\n",
    "        popup=folium.Popup(folium.Html(popup_content, script=True), max_width=2650),\n",
    "        icon=icon\n",
    "    ).add_to(ev_map)\n",
    "\n",
    "# Fixed date and time for the map update\n",
    "fixed_date_time = \"2020-02-28T14:00\"\n",
    "\n",
    "# Create an interactive widget\n",
    "interact(update_map, date_time=fixed(fixed_date_time))\n",
    "\n",
    "# Display the map\n",
    "ev_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FVNeWURG4yQO"
   },
   "outputs": [],
   "source": [
    "import folium\n",
    "from datetime import datetime\n",
    "\n",
    "# Sample data (replace with your actual data)\n",
    "current_energy_consumption = 17.9  # kWh\n",
    "next_hour_energy = 10.6  # kWh\n",
    "current_ports = 0  # Number of available ports\n",
    "next_hour_ports = 3  # Number of available ports\n",
    "\n",
    "# Location of the Bryant EV station\n",
    "station_name = \"Bryant\"\n",
    "latitude = 37.446373\n",
    "longitude = -122.162331\n",
    "\n",
    "# Initialize a Folium map centered on the station location\n",
    "ev_map = folium.Map(location=[latitude, longitude], zoom_start=15)\n",
    "\n",
    "# Fixed date and time for the map update\n",
    "fixed_date_time = \"2020-02-28T14:00\"\n",
    "\n",
    "# Convert date_time to datetime object\n",
    "date_time = datetime.strptime(fixed_date_time, '%Y-%m-%dT%H:%M')\n",
    "\n",
    "# Add marker with custom icon and enhanced popup\n",
    "popup_content = f\"\"\"\n",
    "<div style=\"font-family: Arial, sans-serif; width: 250px;\">\n",
    "    <h4 style=\"margin-bottom: 5px; color: #1e00ff;\">{station_name} EV Charging Station</h4>\n",
    "    <p><strong>Date and Time:</strong><br>{date_time.strftime('%Y/%m/%d %H:%M')}</p>\n",
    "    <hr style=\"margin: 5px 0;\">\n",
    "    <p><strong>Current Energy Consumption:</strong><br>{current_energy_consumption:.1f} kWh</p>\n",
    "    <p><strong>Next Hour Energy Consumption:</strong><br>{next_hour_energy:.1f} kWh</p>\n",
    "    <hr style=\"margin: 5px 0;\">\n",
    "    <p><strong>Current Available Ports:</strong><br>{current_ports}</p>\n",
    "    <p><strong>Next Hour Available Ports:</strong><br>{next_hour_ports}</p>\n",
    "</div>\n",
    "\"\"\"\n",
    "\n",
    "icon = folium.DivIcon(\n",
    "    html=f'<i class=\"fas fa-charging-station fa-2x\" style=\"color: #1e00ff;\"></i>')  # Custom FontAwesome icon\n",
    "\n",
    "folium.Marker(\n",
    "    location=[latitude, longitude],\n",
    "    popup=folium.Popup(folium.Html(popup_content, script=True), max_width=265),\n",
    "    icon=icon\n",
    ").add_to(ev_map)\n",
    "\n",
    "# Display the map\n",
    "ev_map"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "iaXr8k9ja3BG",
    "VXSg9kwCbSN_",
    "q7Ap-3dwbSmE",
    "JEaj36iKSG_G",
    "T3opDOmibnwg",
    "7LH6MOSZAsv1",
    "wwzr63AHbD69",
    "EkwSSX_yA3di",
    "GnjVL4Gf6eKp",
    "78MENGL_yAoQ",
    "I5U1zxfh6tf6",
    "XTzGDAVC7OTb",
    "Er-cBu4TEK7_",
    "5zI2DJryMvo-",
    "3PuV7dNUToOb",
    "pwvkOS4PaR3n",
    "Zbz4Ir9lhmGg",
    "dWAi-Qf-Tfo6",
    "y4JKC8gx3s0S",
    "_jjJQjOH_Jiz",
    "lI56KFnzSjWf",
    "QkGjZK8e0ton",
    "B-3gUzR7bjfT",
    "wSklLnINbjfY",
    "l5fD8klQd1n9",
    "coLXwXbKQtWK",
    "99I7Kv3muvAk",
    "7371h_4ZEdC_",
    "U2nL6FlTExr-",
    "cGCxzxT2GfG7",
    "haPjMsSLG3dc",
    "LEZ2d_YfHHB2",
    "wBI7sOQRHSYm",
    "GgRI4TRFa4mw",
    "WQ7AtHTUa_s8",
    "YBCDlf9J-2_-",
    "ue0rtcK1U1WK",
    "n1tOBEh6o_kb",
    "aj_3dz6B8ntj",
    "46YYCa1vcrxF",
    "eKSWIF_d6lPf",
    "Mp5GcvYZmCY-",
    "2jQKf0fxsNK3",
    "iRILTEbZ7wEx",
    "hlchBGc84VNZ",
    "GcdDrsWw_beb",
    "8t0eR3n6AB6g",
    "8MzLAFJFK-bx",
    "TZTpzxLXSHzl",
    "h2qcVDdYzzfQ",
    "iEg5RDblYuI7",
    "IBXHvoRgMcic"
   ],
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
